{
 "cells": [
  {
   "source": [
    "# 0. Standard imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/external\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9002. The TBB threading layer is disabled.\u001b[0m\n  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from models.vae import VAE\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from inference.inference import UnsupervisedTrainer\n",
    "from external.inference import posterior\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.plots_util import plot_histograms, plot_density\n",
    "from external.utils.plots_util import plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import scvi_baseline_z, cascvi_baseline_z, avg_baseline_z, construct_latent\n",
    "from external.utils.metrics import knn_purity_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import ete3 Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "import ete3\n",
    "\n",
    "#tree_name = \"/data/yosef2/users/mattjones/projects/metastasis/JQ19/5k/trees/lg4lg4_tree_hybrid_priors.alleleThresh.processed.txt\"\n",
    "#tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/Cassiopeia_trees/lg7_branch_length.txt\"\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/Cassiopeia_trees/lg7_tree_hybrid_priors.alleleThresh.processed.ultrametric.tree\"\n",
    "\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "N = len([n for n in tree.traverse()])\n",
    "\n",
    "#leaves = [n.name for n in tree.traverse('levelorder')]\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]"
   ]
  },
  {
   "source": [
    "### 1. Gene expression data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                Barcodes\n",
       "0  LL.AAACCTGCAAGAAAGG-1\n",
       "1  LL.AAACCTGCATGAAGTA-1\n",
       "2  LL.AAACCTGGTACATGTC-1\n",
       "3  LL.AAACCTGTCAAGGTAA-1\n",
       "4  LL.AAACGGGGTCTTGATG-1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Barcodes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LL.AAACCTGCAAGAAAGG-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LL.AAACCTGCATGAAGTA-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LL.AAACCTGGTACATGTC-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LL.AAACCTGTCAAGGTAA-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LL.AAACGGGGTCTTGATG-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "barcodes = pd.read_csv(\"/data/yosef2/users/mattjones/projects/metastasis/JQ19/5k/RNA/ALL_Samples/GRCh38/barcodes.tsv\", names=['Barcodes'])\n",
    "\n",
    "barcodes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                id          Gene\n",
       "0  ENSG00000243485  RP11-34P13.3\n",
       "1  ENSG00000237613       FAM138A\n",
       "2  ENSG00000186092         OR4F5\n",
       "3  ENSG00000238009  RP11-34P13.7\n",
       "4  ENSG00000239945  RP11-34P13.8"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Gene</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENSG00000243485</td>\n      <td>RP11-34P13.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENSG00000237613</td>\n      <td>FAM138A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENSG00000186092</td>\n      <td>OR4F5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENSG00000238009</td>\n      <td>RP11-34P13.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENSG00000239945</td>\n      <td>RP11-34P13.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "genes = pd.read_csv(\"/data/yosef2/users/mattjones/projects/metastasis/JQ19/5k/RNA/ALL_Samples/GRCh38/genes.tsv\", sep='\\t', names=['id', 'Gene'])\n",
    "\n",
    "genes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Gene         C          Z           Pval            FDR\n",
       "0     TFF3  0.174633  42.425806   0.000000e+00   0.000000e+00\n",
       "1     TFF1  0.227374  41.599879   0.000000e+00   0.000000e+00\n",
       "2   MUC5AC  0.157811  37.940520   0.000000e+00   0.000000e+00\n",
       "3  CEACAM6  0.245428  32.255498  1.472918e-228  2.218583e-225\n",
       "4    MUC5B  0.084600  24.601674  6.060690e-134  7.303131e-131"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gene</th>\n      <th>C</th>\n      <th>Z</th>\n      <th>Pval</th>\n      <th>FDR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TFF3</td>\n      <td>0.174633</td>\n      <td>42.425806</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TFF1</td>\n      <td>0.227374</td>\n      <td>41.599879</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MUC5AC</td>\n      <td>0.157811</td>\n      <td>37.940520</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CEACAM6</td>\n      <td>0.245428</td>\n      <td>32.255498</td>\n      <td>1.472918e-228</td>\n      <td>2.218583e-225</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MUC5B</td>\n      <td>0.084600</td>\n      <td>24.601674</td>\n      <td>6.060690e-134</td>\n      <td>7.303131e-131</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "hotspot_genes = pd.read_csv(\"/data/yosef2/users/mattjones/projects/metastasis/hotspot_gene_sets/lg7_hotspot_genes.tsv\", sep='\\t')\n",
    "hotspot_genes.head(5)"
   ]
  },
  {
   "source": [
    "Gene expression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.io import mmread\n",
    "\n",
    "#X = mmread('/data/yosef2/users/mattjones/projects/metastasis/JQ19/5k/RNA/ALL_Samples/GRCh38/matrix.mtx')\n",
    "#Y = X.todense()\n",
    "#print(Y.shape)"
   ]
  },
  {
   "source": [
    "# 1. Data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### a. Filter out cells not present in gene expression matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 0\n",
    "#leaves_to_delete = []\n",
    "#leaves_X = {}\n",
    "#for barcode in leaves:\n",
    "#    foo = barcodes.index[barcodes['Barcodes'] == barcode].tolist()\n",
    "#    if foo == []:\n",
    "#        N += 1\n",
    "#        leaves_to_delete.append(barcode)\n",
    "#        continue\n",
    "#    else:\n",
    "#        idx = foo[0]\n",
    "    #x = np.squeeze(np.array(Y[:, idx]))\n",
    "    #leaves_X[barcode] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Kept {} leaves\".format(len(leaves_X)))\n",
    "#print(\"{} Cells were not present in the gene expression dataset\".format(N))"
   ]
  },
  {
   "source": [
    "### b. Prune cells from the tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_leaves = [n for n in leaves if n not in leaves_to_delete]\n",
    "#tree.prune(keep_leaves) "
   ]
  },
  {
   "source": [
    "Resolve unifurcations in tree"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_unifurcations(tree: ete3.Tree) -> ete3.Tree:\n",
    "    \"\"\"Collapse unifurcations.\n",
    "    Collapse all unifurcations in the tree, namely any node with only one child\n",
    "    should be removed and all children should be connected to the parent node.\n",
    "    Args:\n",
    "        tree: tree to be collapsed\n",
    "    Returns:\n",
    "        A collapsed tree.\n",
    "    \"\"\"\n",
    "    collapse_fn = lambda x: (len(x.children) == 1)\n",
    "    collapsed_tree = tree.copy()\n",
    "    to_collapse = [n for n in collapsed_tree.traverse() if collapse_fn(n)]\n",
    "    for n in to_collapse:\n",
    "        n.delete()\n",
    "    return collapsed_tree\n",
    "\n",
    "import copy\n",
    "collapsed_tree = copy.deepcopy(tree) #collapse_unifurcations(tree)"
   ]
  },
  {
   "source": [
    "### c. Filter out genes not present in hot spot gene set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***--> genes to keep***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_genes = []\n",
    "max = 100\n",
    "for i, g in enumerate(hotspot_genes['Gene'].values):\n",
    "    if i > max:\n",
    "        break\n",
    "    idx_g = genes.index[genes['Gene'] == g].tolist()[0]\n",
    "    keep_genes.append(idx_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset has 101 genes and 663 cells\n"
     ]
    }
   ],
   "source": [
    "n_genes = len(keep_genes)\n",
    "n_cells = len(keep_leaves)\n",
    "\n",
    "print(\"Dataset has {} genes and {} cells\".format(n_genes, n_cells))"
   ]
  },
  {
   "source": [
    "***--> Rearrange cells in array - in level order***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.zeros((n_cells, genes.shape[0]))\n",
    "#idx = 0\n",
    "#for n in tree.traverse('levelorder'):\n",
    "#    if n.is_leaf():\n",
    "#        barcode = n.name\n",
    "#        X[idx] = leaves_X[barcode]\n",
    "#        idx += 1\n",
    "#print(\"Dropout rate in gene expression matrix: {}\".format(np.mean(X == 0)))"
   ]
  },
  {
   "source": [
    "***--> Filtering genes***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X[:, keep_genes]\n",
    "#print(\"shape; {}\".format(X.shape))\n",
    "#print(\"Dropout rate in gene expression matrix: {}\".format(np.mean(X == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(603, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "#\n",
    "X = np.load('/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/metastasis_data/Metastasis_lg7_100g.npy')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('Metastasis_lg7_100g.npy', X)"
   ]
  },
  {
   "source": [
    "***--> Historgram***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_histograms(X, 'Metastasis', 'histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(collapsed_tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)\n",
    "    if not n.is_leaf():\n",
    "        n.name = str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "diameter = collapsed_tree.get_leaves()[0].get_distance(collapsed_tree)\n",
    "#diameter = 200\n",
    "diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_length = {}\n",
    "for node in collapsed_tree.traverse('levelorder'):\n",
    "    if node.is_root():\n",
    "        branch_length[node.name] = 0.0\n",
    "    else:\n",
    "        branch_length[node.name] = node.dist / diameter\n",
    "branch_length['prior_root'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f121767d3d0>"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# anndata + gene and celle filtering\n",
    "adata = AnnData(X)\n",
    "leaves = [n for n in collapsed_tree.traverse('levelorder') if n.is_leaf()]\n",
    "adata.obs_names = [n.name for n in leaves]\n",
    "sc.pp.filter_genes(adata, min_counts=3)\n",
    "sc.pp.filter_cells(adata, min_counts=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "n_counts is a protected attribute or already exists as a cell attribute and cannot be set with this name in initialize_gene_attribute, changing name to n_counts_gene and setting\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "n_cells, n_genes = X.shape\n",
    "tree_bis = copy.deepcopy(collapsed_tree)\n",
    "scvi_dataset = AnnDatasetFromAnnData(adata, filtering=False)\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', adata.obs_names)\n",
    "cas_dataset = TreeDataset(scvi_dataset, tree=tree_bis, filtering=False)\n",
    "cas_dataset\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = True\n",
    "use_MP = True\n",
    "ldvae = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=10,\n",
    "              n_hidden=64,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=use_MP\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 700\n",
    "lr = 1e-3\n",
    "lambda_ = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_leaves:  [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526], [527], [528], [529], [530], [531], [532], [533], [534], [535], [536], [537], [538], [539], [540], [541], [542], [543], [544], [545], [546], [547], [548], [549], [550], [551], [552], [553], [554], [555], [556], [557], [558], [559], [560], [561], [562], [563], [564], [565], [566], [567], [568], [569], [570], [571], [572], [573], [574], [575], [576], [577], [578], [579], [580], [581], [582], [583], [584], [585], [586], [587], [588], [589], [590], [591], [592], [593], [594], [595], [596], [597], [598], [599], [600], [601], [602]]\ntest_leaves:  []\nvalidation leaves:  []\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "944880375277\n",
      "training:  78%|███████▊  | 548/700 [04:51<01:30,  1.67it/s]Encodings MP Likelihood: 9.147760027106251\n",
      "ELBO Loss: 267.9500091650155\n",
      "training:  78%|███████▊  | 549/700 [04:51<01:19,  1.90it/s]Encodings MP Likelihood: 9.235904625509509\n",
      "ELBO Loss: 267.6481670220532\n",
      "training:  79%|███████▊  | 550/700 [04:52<01:11,  2.10it/s]Encodings MP Likelihood: 9.277956094638759\n",
      "ELBO Loss: 269.29523804088024\n",
      "training:  79%|███████▊  | 551/700 [04:53<01:25,  1.75it/s]Encodings MP Likelihood: 9.132675539279857\n",
      "ELBO Loss: 267.29917594452377\n",
      "training:  79%|███████▉  | 552/700 [04:53<01:14,  1.98it/s]Encodings MP Likelihood: 9.108848282465495\n",
      "ELBO Loss: 268.82251500296917\n",
      "training:  79%|███████▉  | 553/700 [04:53<01:07,  2.17it/s]Encodings MP Likelihood: 9.207169932175836\n",
      "ELBO Loss: 268.83989428057936\n",
      "training:  79%|███████▉  | 554/700 [04:54<01:22,  1.76it/s]Encodings MP Likelihood: 9.216706872623845\n",
      "ELBO Loss: 267.89750792742205\n",
      "training:  79%|███████▉  | 555/700 [04:55<01:13,  1.98it/s]Encodings MP Likelihood: 8.981147347072753\n",
      "ELBO Loss: 267.9138127405425\n",
      "training:  79%|███████▉  | 556/700 [04:55<01:05,  2.19it/s]Encodings MP Likelihood: 9.214994402632431\n",
      "ELBO Loss: 267.8712457019112\n",
      "training:  80%|███████▉  | 557/700 [04:56<01:25,  1.67it/s]Encodings MP Likelihood: 9.344330502337684\n",
      "ELBO Loss: 267.2938774452421\n",
      "training:  80%|███████▉  | 558/700 [04:56<01:14,  1.90it/s]Encodings MP Likelihood: 9.190462990762656\n",
      "ELBO Loss: 267.5312984402141\n",
      "training:  80%|███████▉  | 559/700 [04:56<01:06,  2.11it/s]Encodings MP Likelihood: 9.204968817395493\n",
      "ELBO Loss: 267.238451625763\n",
      "training:  80%|████████  | 560/700 [04:57<01:24,  1.66it/s]Encodings MP Likelihood: 9.137555008006235\n",
      "ELBO Loss: 267.7385804965553\n",
      "training:  80%|████████  | 561/700 [04:58<01:13,  1.88it/s]Encodings MP Likelihood: 9.21699875239748\n",
      "ELBO Loss: 267.0466582291234\n",
      "training:  80%|████████  | 562/700 [04:58<01:05,  2.10it/s]Encodings MP Likelihood: 9.16614005654736\n",
      "ELBO Loss: 267.10945334732946\n",
      "training:  80%|████████  | 563/700 [04:59<01:23,  1.64it/s]Encodings MP Likelihood: 9.184922037895545\n",
      "ELBO Loss: 266.97774770624375\n",
      "training:  81%|████████  | 564/700 [04:59<01:12,  1.87it/s]Encodings MP Likelihood: 9.214593936242082\n",
      "ELBO Loss: 267.13604299231\n",
      "training:  81%|████████  | 565/700 [05:00<01:04,  2.08it/s]Encodings MP Likelihood: 9.100256642916971\n",
      "ELBO Loss: 267.6579360212254\n",
      "training:  81%|████████  | 566/700 [05:00<00:59,  2.26it/s]Encodings MP Likelihood: 9.164183227315785\n",
      "ELBO Loss: 266.9084474042972\n",
      "training:  81%|████████  | 567/700 [05:00<00:55,  2.41it/s]Encodings MP Likelihood: 9.067625620668624\n",
      "ELBO Loss: 266.8903171752546\n",
      "training:  81%|████████  | 568/700 [05:01<00:51,  2.54it/s]Encodings MP Likelihood: 9.117896104194093\n",
      "ELBO Loss: 266.6314507974388\n",
      "training:  81%|████████▏ | 569/700 [05:01<00:50,  2.61it/s]Encodings MP Likelihood: 9.104929583437599\n",
      "ELBO Loss: 267.3189324890217\n",
      "training:  81%|████████▏ | 570/700 [05:02<00:48,  2.68it/s]Encodings MP Likelihood: 9.11832995058861\n",
      "ELBO Loss: 267.71409212239956\n",
      "training:  82%|████████▏ | 571/700 [05:02<00:47,  2.73it/s]Encodings MP Likelihood: 9.041652501407725\n",
      "ELBO Loss: 266.8524439275735\n",
      "training:  82%|████████▏ | 572/700 [05:02<00:48,  2.62it/s]Encodings MP Likelihood: 9.26564948815496\n",
      "ELBO Loss: 267.2640294574127\n",
      "training:  82%|████████▏ | 573/700 [05:03<00:47,  2.67it/s]Encodings MP Likelihood: 8.99964727653612\n",
      "ELBO Loss: 266.60556339493905\n",
      "training:  82%|████████▏ | 574/700 [05:03<00:46,  2.72it/s]Encodings MP Likelihood: 9.073978298340405\n",
      "ELBO Loss: 266.6023544243412\n",
      "training:  82%|████████▏ | 575/700 [05:03<00:48,  2.58it/s]Encodings MP Likelihood: 9.04967368828006\n",
      "ELBO Loss: 265.95941258054256\n",
      "training:  82%|████████▏ | 576/700 [05:04<00:47,  2.63it/s]Encodings MP Likelihood: 9.186635500516921\n",
      "ELBO Loss: 267.7018322555674\n",
      "training:  82%|████████▏ | 577/700 [05:04<00:46,  2.66it/s]Encodings MP Likelihood: 8.958609685552462\n",
      "ELBO Loss: 267.3897292360756\n",
      "training:  83%|████████▎ | 578/700 [05:05<00:45,  2.68it/s]Encodings MP Likelihood: 9.076249811876401\n",
      "ELBO Loss: 266.22130228676207\n",
      "training:  83%|████████▎ | 579/700 [05:05<00:44,  2.71it/s]Encodings MP Likelihood: 9.055303707056995\n",
      "ELBO Loss: 266.72951787663703\n",
      "training:  83%|████████▎ | 580/700 [05:05<00:46,  2.59it/s]Encodings MP Likelihood: 8.946455569814004\n",
      "ELBO Loss: 266.38034426247197\n",
      "training:  83%|████████▎ | 581/700 [05:06<00:44,  2.66it/s]Encodings MP Likelihood: 8.918408109174534\n",
      "ELBO Loss: 266.8818746280396\n",
      "training:  83%|████████▎ | 582/700 [05:06<00:43,  2.71it/s]Encodings MP Likelihood: 8.89614045525153\n",
      "ELBO Loss: 266.973363368508\n",
      "training:  83%|████████▎ | 583/700 [05:06<00:44,  2.66it/s]Encodings MP Likelihood: 8.915972470878213\n",
      "ELBO Loss: 265.5347206463338\n",
      "training:  83%|████████▎ | 584/700 [05:07<00:42,  2.71it/s]Encodings MP Likelihood: 8.976702934249154\n",
      "ELBO Loss: 266.1433900998443\n",
      "training:  84%|████████▎ | 585/700 [05:07<00:42,  2.72it/s]Encodings MP Likelihood: 8.994872141723215\n",
      "ELBO Loss: 266.14315824426575\n",
      "training:  84%|████████▎ | 586/700 [05:07<00:41,  2.73it/s]Encodings MP Likelihood: 8.839827632319224\n",
      "ELBO Loss: 265.57586398387485\n",
      "training:  84%|████████▍ | 587/700 [05:08<00:41,  2.75it/s]Encodings MP Likelihood: 8.923172595964138\n",
      "ELBO Loss: 266.4889505259525\n",
      "training:  84%|████████▍ | 588/700 [05:08<00:40,  2.77it/s]Encodings MP Likelihood: 8.928351009909749\n",
      "ELBO Loss: 266.6247699236151\n",
      "training:  84%|████████▍ | 589/700 [05:09<00:39,  2.83it/s]Encodings MP Likelihood: 8.92934566895774\n",
      "ELBO Loss: 265.11886814615804\n",
      "training:  84%|████████▍ | 590/700 [05:09<00:39,  2.81it/s]Encodings MP Likelihood: 8.81704343060058\n",
      "ELBO Loss: 266.38266376678666\n",
      "training:  84%|████████▍ | 591/700 [05:09<00:41,  2.65it/s]Encodings MP Likelihood: 8.77320715591022\n",
      "ELBO Loss: 266.4909495234272\n",
      "training:  85%|████████▍ | 592/700 [05:10<00:40,  2.68it/s]Encodings MP Likelihood: 8.955203815038704\n",
      "ELBO Loss: 265.9214769851583\n",
      "training:  85%|████████▍ | 593/700 [05:10<00:39,  2.72it/s]Encodings MP Likelihood: 8.925063061374617\n",
      "ELBO Loss: 265.9464313385265\n",
      "training:  85%|████████▍ | 594/700 [05:10<00:41,  2.57it/s]Encodings MP Likelihood: 8.93959043342382\n",
      "ELBO Loss: 265.28409669628917\n",
      "training:  85%|████████▌ | 595/700 [05:11<00:39,  2.63it/s]Encodings MP Likelihood: 8.756479982364803\n",
      "ELBO Loss: 265.8639418878985\n",
      "training:  85%|████████▌ | 596/700 [05:11<00:38,  2.69it/s]Encodings MP Likelihood: 8.947241398329478\n",
      "ELBO Loss: 266.016270895716\n",
      "training:  85%|████████▌ | 597/700 [05:12<00:39,  2.64it/s]Encodings MP Likelihood: 9.058420010534997\n",
      "ELBO Loss: 266.2423148496321\n",
      "training:  85%|████████▌ | 598/700 [05:12<00:38,  2.68it/s]Encodings MP Likelihood: 8.775063786424628\n",
      "ELBO Loss: 265.615571091447\n",
      "training:  86%|████████▌ | 599/700 [05:12<00:37,  2.72it/s]Encodings MP Likelihood: 8.928828017336437\n",
      "ELBO Loss: 264.45089407533055\n",
      "training:  86%|████████▌ | 600/700 [05:13<00:42,  2.33it/s]Encodings MP Likelihood: 8.838691084463692\n",
      "ELBO Loss: 265.88771893805045\n",
      "training:  86%|████████▌ | 601/700 [05:13<00:40,  2.46it/s]Encodings MP Likelihood: 8.707256664183527\n",
      "ELBO Loss: 266.1880302755676\n",
      "training:  86%|████████▌ | 602/700 [05:14<00:39,  2.48it/s]Encodings MP Likelihood: 8.808597179142685\n",
      "ELBO Loss: 265.36059783881603\n",
      "training:  86%|████████▌ | 603/700 [05:14<00:36,  2.63it/s]Encodings MP Likelihood: 8.796075853727153\n",
      "ELBO Loss: 265.70589739750005\n",
      "training:  86%|████████▋ | 604/700 [05:14<00:35,  2.69it/s]Encodings MP Likelihood: 8.70310043964293\n",
      "ELBO Loss: 265.4707516449987\n",
      "training:  86%|████████▋ | 605/700 [05:15<00:35,  2.64it/s]Encodings MP Likelihood: 8.595938147185906\n",
      "ELBO Loss: 265.9719792542973\n",
      "training:  87%|████████▋ | 606/700 [05:15<00:34,  2.69it/s]Encodings MP Likelihood: 8.882054753387646\n",
      "ELBO Loss: 266.01180866870567\n",
      "training:  87%|████████▋ | 607/700 [05:15<00:34,  2.71it/s]Encodings MP Likelihood: 8.772700218535025\n",
      "ELBO Loss: 264.6533560804414\n",
      "training:  87%|████████▋ | 608/700 [05:16<00:34,  2.64it/s]Encodings MP Likelihood: 8.61285810017809\n",
      "ELBO Loss: 265.9466552914595\n",
      "training:  87%|████████▋ | 609/700 [05:16<00:33,  2.69it/s]Encodings MP Likelihood: 8.593793847994391\n",
      "ELBO Loss: 264.99937605163177\n",
      "training:  87%|████████▋ | 610/700 [05:17<00:33,  2.67it/s]Encodings MP Likelihood: 8.828935080071595\n",
      "ELBO Loss: 264.7768909813415\n",
      "training:  87%|████████▋ | 611/700 [05:17<00:32,  2.71it/s]Encodings MP Likelihood: 8.677261275305463\n",
      "ELBO Loss: 264.9612709615428\n",
      "training:  87%|████████▋ | 612/700 [05:17<00:32,  2.74it/s]Encodings MP Likelihood: 8.610305185499112\n",
      "ELBO Loss: 265.0569173253241\n",
      "training:  88%|████████▊ | 613/700 [05:18<00:33,  2.63it/s]Encodings MP Likelihood: 8.850528530020187\n",
      "ELBO Loss: 265.2570987876415\n",
      "training:  88%|████████▊ | 614/700 [05:18<00:32,  2.65it/s]Encodings MP Likelihood: 8.56093765700569\n",
      "ELBO Loss: 265.68875696045114\n",
      "training:  88%|████████▊ | 615/700 [05:18<00:31,  2.66it/s]Encodings MP Likelihood: 8.69175859443541\n",
      "ELBO Loss: 265.47774780950687\n",
      "training:  88%|████████▊ | 616/700 [05:19<00:32,  2.61it/s]Encodings MP Likelihood: 8.791660750010564\n",
      "ELBO Loss: 265.1615913713245\n",
      "training:  88%|████████▊ | 617/700 [05:19<00:31,  2.63it/s]Encodings MP Likelihood: 8.783384812927693\n",
      "ELBO Loss: 265.0312361264541\n",
      "training:  88%|████████▊ | 618/700 [05:20<00:30,  2.69it/s]Encodings MP Likelihood: 8.71338183751403\n",
      "ELBO Loss: 265.41784612682795\n",
      "training:  88%|████████▊ | 619/700 [05:20<00:31,  2.60it/s]Encodings MP Likelihood: 8.666956800665366\n",
      "ELBO Loss: 265.5198000622245\n",
      "training:  89%|████████▊ | 620/700 [05:20<00:30,  2.65it/s]Encodings MP Likelihood: 8.502391245849386\n",
      "ELBO Loss: 264.3989611757255\n",
      "training:  89%|████████▊ | 621/700 [05:21<00:29,  2.66it/s]Encodings MP Likelihood: 8.623144357727087\n",
      "ELBO Loss: 264.2090308741615\n",
      "training:  89%|████████▉ | 622/700 [05:21<00:29,  2.67it/s]Encodings MP Likelihood: 8.67725321804638\n",
      "ELBO Loss: 264.3250534574024\n",
      "training:  89%|████████▉ | 623/700 [05:21<00:28,  2.72it/s]Encodings MP Likelihood: 8.620424343128471\n",
      "ELBO Loss: 264.7403043315374\n",
      "training:  89%|████████▉ | 624/700 [05:22<00:30,  2.53it/s]Encodings MP Likelihood: 8.644193728533685\n",
      "ELBO Loss: 264.2630473656176\n",
      "training:  89%|████████▉ | 625/700 [05:22<00:29,  2.56it/s]Encodings MP Likelihood: 8.804474766882086\n",
      "ELBO Loss: 263.9764786974237\n",
      "training:  89%|████████▉ | 626/700 [05:23<00:28,  2.62it/s]Encodings MP Likelihood: 8.56789203842564\n",
      "ELBO Loss: 264.64581512578565\n",
      "training:  90%|████████▉ | 627/700 [05:23<00:28,  2.59it/s]Encodings MP Likelihood: 8.614182057296258\n",
      "ELBO Loss: 264.162427725155\n",
      "training:  90%|████████▉ | 628/700 [05:23<00:27,  2.66it/s]Encodings MP Likelihood: 8.564022110727441\n",
      "ELBO Loss: 263.74256829802766\n",
      "training:  90%|████████▉ | 629/700 [05:24<00:26,  2.70it/s]Encodings MP Likelihood: 8.622923830208551\n",
      "ELBO Loss: 263.81629740581184\n",
      "training:  90%|█████████ | 630/700 [05:24<00:25,  2.73it/s]Encodings MP Likelihood: 8.586014588632402\n",
      "ELBO Loss: 264.5801582612705\n",
      "training:  90%|█████████ | 631/700 [05:24<00:25,  2.75it/s]Encodings MP Likelihood: 8.625168568807096\n",
      "ELBO Loss: 264.36336321864434\n",
      "training:  90%|█████████ | 632/700 [05:25<00:24,  2.72it/s]Encodings MP Likelihood: 8.573515924429113\n",
      "ELBO Loss: 263.6372125348353\n",
      "training:  90%|█████████ | 633/700 [05:25<00:24,  2.72it/s]Encodings MP Likelihood: 8.478925688788458\n",
      "ELBO Loss: 264.4364391616269\n",
      "training:  91%|█████████ | 634/700 [05:26<00:24,  2.74it/s]Encodings MP Likelihood: 8.564089363059065\n",
      "ELBO Loss: 265.1171917014875\n",
      "training:  91%|█████████ | 635/700 [05:26<00:24,  2.68it/s]Encodings MP Likelihood: 8.432449707387418\n",
      "ELBO Loss: 263.2614996890833\n",
      "training:  91%|█████████ | 636/700 [05:26<00:23,  2.73it/s]Encodings MP Likelihood: 8.545555438412427\n",
      "ELBO Loss: 264.5335602561675\n",
      "training:  91%|█████████ | 637/700 [05:27<00:22,  2.85it/s]Encodings MP Likelihood: 8.570259603371703\n",
      "ELBO Loss: 263.45047776342005\n",
      "training:  91%|█████████ | 638/700 [05:27<00:22,  2.81it/s]Encodings MP Likelihood: 8.52578301244247\n",
      "ELBO Loss: 263.64382620632193\n",
      "training:  91%|█████████▏| 639/700 [05:27<00:21,  2.79it/s]Encodings MP Likelihood: 8.600074443923301\n",
      "ELBO Loss: 264.02205728626313\n",
      "training:  91%|█████████▏| 640/700 [05:28<00:21,  2.74it/s]Encodings MP Likelihood: 8.505265290499311\n",
      "ELBO Loss: 263.936708227271\n",
      "training:  92%|█████████▏| 641/700 [05:28<00:22,  2.63it/s]Encodings MP Likelihood: 8.515326821884702\n",
      "ELBO Loss: 264.6905303340185\n",
      "training:  92%|█████████▏| 642/700 [05:28<00:21,  2.64it/s]Encodings MP Likelihood: 8.627253347619714\n",
      "ELBO Loss: 263.9497262897757\n",
      "training:  92%|█████████▏| 643/700 [05:29<00:21,  2.65it/s]Encodings MP Likelihood: 8.55343285113748\n",
      "ELBO Loss: 262.5926557634757\n",
      "training:  92%|█████████▏| 644/700 [05:29<00:22,  2.49it/s]Encodings MP Likelihood: 8.55358143776\n",
      "ELBO Loss: 264.15569991390913\n",
      "training:  92%|█████████▏| 645/700 [05:30<00:21,  2.56it/s]Encodings MP Likelihood: 8.451614303063288\n",
      "ELBO Loss: 263.4321496771071\n",
      "training:  92%|█████████▏| 646/700 [05:30<00:20,  2.63it/s]Encodings MP Likelihood: 8.545659268705908\n",
      "ELBO Loss: 263.96360471867905\n",
      "training:  92%|█████████▏| 647/700 [05:30<00:20,  2.62it/s]Encodings MP Likelihood: 8.338658331137692\n",
      "ELBO Loss: 265.07877905541017\n",
      "training:  93%|█████████▎| 648/700 [05:31<00:19,  2.67it/s]Encodings MP Likelihood: 8.455983953708676\n",
      "ELBO Loss: 263.67462043699953\n",
      "training:  93%|█████████▎| 649/700 [05:31<00:18,  2.70it/s]Encodings MP Likelihood: 8.577744829141361\n",
      "ELBO Loss: 264.0631543807457\n",
      "training:  93%|█████████▎| 650/700 [05:32<00:18,  2.73it/s]Encodings MP Likelihood: 8.297176076820254\n",
      "ELBO Loss: 263.1442725071857\n",
      "training:  93%|█████████▎| 651/700 [05:32<00:17,  2.76it/s]Encodings MP Likelihood: 8.350196699285473\n",
      "ELBO Loss: 263.6817669506477\n",
      "training:  93%|█████████▎| 652/700 [05:32<00:18,  2.65it/s]Encodings MP Likelihood: 8.372278156238526\n",
      "ELBO Loss: 263.28394747105466\n",
      "training:  93%|█████████▎| 653/700 [05:33<00:17,  2.68it/s]Encodings MP Likelihood: 8.436863503393127\n",
      "ELBO Loss: 262.8111254987466\n",
      "training:  93%|█████████▎| 654/700 [05:33<00:17,  2.71it/s]Encodings MP Likelihood: 8.476441903543428\n",
      "ELBO Loss: 263.5070664572319\n",
      "training:  94%|█████████▎| 655/700 [05:33<00:17,  2.60it/s]Encodings MP Likelihood: 8.382804997344081\n",
      "ELBO Loss: 263.1477447908522\n",
      "training:  94%|█████████▎| 656/700 [05:34<00:16,  2.64it/s]Encodings MP Likelihood: 8.376299212435852\n",
      "ELBO Loss: 263.9841566784427\n",
      "training:  94%|█████████▍| 657/700 [05:34<00:16,  2.66it/s]Encodings MP Likelihood: 8.40280939644556\n",
      "ELBO Loss: 263.5789788009625\n",
      "training:  94%|█████████▍| 658/700 [05:34<00:15,  2.75it/s]Encodings MP Likelihood: 8.31030519834204\n",
      "ELBO Loss: 263.92937046193464\n",
      "training:  94%|█████████▍| 659/700 [05:35<00:14,  2.79it/s]Encodings MP Likelihood: 8.397714611211589\n",
      "ELBO Loss: 264.03485996850304\n",
      "training:  94%|█████████▍| 660/700 [05:35<00:14,  2.73it/s]Encodings MP Likelihood: 8.463509473618918\n",
      "ELBO Loss: 263.5664253765562\n",
      "training:  94%|█████████▍| 661/700 [05:36<00:14,  2.71it/s]Encodings MP Likelihood: 8.474071900747866\n",
      "ELBO Loss: 262.9110124312908\n",
      "training:  95%|█████████▍| 662/700 [05:36<00:13,  2.77it/s]Encodings MP Likelihood: 8.358818013210307\n",
      "ELBO Loss: 264.2468297034151\n",
      "training:  95%|█████████▍| 663/700 [05:36<00:14,  2.64it/s]Encodings MP Likelihood: 8.483749097726808\n",
      "ELBO Loss: 262.54267699439004\n",
      "training:  95%|█████████▍| 664/700 [05:37<00:13,  2.71it/s]Encodings MP Likelihood: 8.366402794541163\n",
      "ELBO Loss: 263.9331920303709\n",
      "training:  95%|█████████▌| 665/700 [05:37<00:12,  2.75it/s]Encodings MP Likelihood: 8.410206196813137\n",
      "ELBO Loss: 263.661406205964\n",
      "training:  95%|█████████▌| 666/700 [05:37<00:12,  2.78it/s]Encodings MP Likelihood: 8.335379501486806\n",
      "ELBO Loss: 261.8691192045235\n",
      "training:  95%|█████████▌| 667/700 [05:38<00:12,  2.69it/s]Encodings MP Likelihood: 8.432785286076053\n",
      "ELBO Loss: 262.15398177721994\n",
      "training:  95%|█████████▌| 668/700 [05:38<00:11,  2.69it/s]Encodings MP Likelihood: 8.39320505122833\n",
      "ELBO Loss: 262.9203017489582\n",
      "training:  96%|█████████▌| 669/700 [05:39<00:11,  2.68it/s]Encodings MP Likelihood: 8.370447018326933\n",
      "ELBO Loss: 263.1764983843136\n",
      "training:  96%|█████████▌| 670/700 [05:39<00:10,  2.73it/s]Encodings MP Likelihood: 8.425542893193843\n",
      "ELBO Loss: 263.4323207626785\n",
      "training:  96%|█████████▌| 671/700 [05:39<00:10,  2.64it/s]Encodings MP Likelihood: 8.45128316080231\n",
      "ELBO Loss: 262.93389683649133\n",
      "training:  96%|█████████▌| 672/700 [05:40<00:10,  2.57it/s]Encodings MP Likelihood: 8.288280750641418\n",
      "ELBO Loss: 262.5847267248257\n",
      "training:  96%|█████████▌| 673/700 [05:40<00:10,  2.59it/s]Encodings MP Likelihood: 8.293043982080714\n",
      "ELBO Loss: 264.40427405275307\n",
      "training:  96%|█████████▋| 674/700 [05:40<00:09,  2.61it/s]Encodings MP Likelihood: 8.42930378522384\n",
      "ELBO Loss: 263.2538681959922\n",
      "training:  96%|█████████▋| 675/700 [05:41<00:09,  2.63it/s]Encodings MP Likelihood: 8.275509094008251\n",
      "ELBO Loss: 263.4963676071104\n",
      "training:  97%|█████████▋| 676/700 [05:41<00:10,  2.32it/s]Encodings MP Likelihood: 8.375809566576349\n",
      "ELBO Loss: 262.12894250429446\n",
      "training:  97%|█████████▋| 677/700 [05:42<00:09,  2.39it/s]Encodings MP Likelihood: 8.308693301409326\n",
      "ELBO Loss: 262.39831346480014\n",
      "training:  97%|█████████▋| 678/700 [05:42<00:08,  2.45it/s]Encodings MP Likelihood: 8.319570901133497\n",
      "ELBO Loss: 263.006714198151\n",
      "training:  97%|█████████▋| 679/700 [05:43<00:08,  2.52it/s]Encodings MP Likelihood: 8.332277955944395\n",
      "ELBO Loss: 262.831750552644\n",
      "training:  97%|█████████▋| 680/700 [05:43<00:07,  2.56it/s]Encodings MP Likelihood: 8.303727270392177\n",
      "ELBO Loss: 262.86884620365595\n",
      "training:  97%|█████████▋| 681/700 [05:43<00:07,  2.58it/s]Encodings MP Likelihood: 8.3993709525099\n",
      "ELBO Loss: 262.06311497736067\n",
      "training:  97%|█████████▋| 682/700 [05:44<00:07,  2.54it/s]Encodings MP Likelihood: 8.339611716349179\n",
      "ELBO Loss: 263.00793077527504\n",
      "training:  98%|█████████▊| 683/700 [05:44<00:06,  2.59it/s]Encodings MP Likelihood: 8.24929475163657\n",
      "ELBO Loss: 262.5540967147757\n",
      "training:  98%|█████████▊| 684/700 [05:44<00:06,  2.61it/s]Encodings MP Likelihood: 8.272485649276758\n",
      "ELBO Loss: 261.58676994429305\n",
      "training:  98%|█████████▊| 685/700 [05:45<00:05,  2.65it/s]Encodings MP Likelihood: 8.225818839670902\n",
      "ELBO Loss: 263.08189807725546\n",
      "training:  98%|█████████▊| 686/700 [05:45<00:05,  2.65it/s]Encodings MP Likelihood: 8.294984924577717\n",
      "ELBO Loss: 262.20157547971905\n",
      "training:  98%|█████████▊| 687/700 [05:46<00:05,  2.57it/s]Encodings MP Likelihood: 8.257310528392322\n",
      "ELBO Loss: 262.7209190893229\n",
      "training:  98%|█████████▊| 688/700 [05:46<00:04,  2.62it/s]Encodings MP Likelihood: 8.279832699575543\n",
      "ELBO Loss: 262.8223622870258\n",
      "training:  98%|█████████▊| 689/700 [05:46<00:04,  2.66it/s]Encodings MP Likelihood: 8.129143660758354\n",
      "ELBO Loss: 262.67098993808116\n",
      "training:  99%|█████████▊| 690/700 [05:47<00:03,  2.55it/s]Encodings MP Likelihood: 8.143495983965547\n",
      "ELBO Loss: 262.1530502268833\n",
      "training:  99%|█████████▊| 691/700 [05:47<00:03,  2.63it/s]Encodings MP Likelihood: 8.231365941120774\n",
      "ELBO Loss: 263.1277887617338\n",
      "training:  99%|█████████▉| 692/700 [05:48<00:03,  2.60it/s]Encodings MP Likelihood: 8.287700780667189\n",
      "ELBO Loss: 262.455843350916\n",
      "training:  99%|█████████▉| 693/700 [05:48<00:02,  2.40it/s]Encodings MP Likelihood: 8.314474954556886\n",
      "ELBO Loss: 262.2735579033681\n",
      "training:  99%|█████████▉| 694/700 [05:48<00:02,  2.40it/s]Encodings MP Likelihood: 8.226963025323021\n",
      "ELBO Loss: 261.89474028127336\n",
      "training:  99%|█████████▉| 695/700 [05:49<00:02,  2.40it/s]Encodings MP Likelihood: 8.253399943416998\n",
      "ELBO Loss: 262.47749599718463\n",
      "training:  99%|█████████▉| 696/700 [05:49<00:01,  2.51it/s]Encodings MP Likelihood: 8.245988132552505\n",
      "ELBO Loss: 261.88229009745515\n",
      "training: 100%|█████████▉| 697/700 [05:50<00:01,  2.24it/s]Encodings MP Likelihood: 8.192121257525125\n",
      "ELBO Loss: 261.56582264911583\n",
      "training: 100%|█████████▉| 698/700 [05:50<00:00,  2.34it/s]Encodings MP Likelihood: 8.308880162441557\n",
      "ELBO Loss: 262.98199352621725\n",
      "training: 100%|█████████▉| 699/700 [05:50<00:00,  2.47it/s]Encodings MP Likelihood: 8.246751467014649\n",
      "ELBO Loss: 261.8638095543181\n",
      "training: 100%|██████████| 700/700 [05:51<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "source": [
    "***Training ELBO***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CascVI train ELBO: 262.59528856120596\n"
     ]
    }
   ],
   "source": [
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "\n",
    "print(\"CascVI train ELBO: {}\".format(full_posterior.compute_elbo().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior and MV imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Missing Value imputation By Posterior Predictive sampling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_l = np.mean(np.sum(X, axis=1))\n",
    "\n",
    "# CascVI imputations\n",
    "imputed = {}\n",
    "imputed_z = {}\n",
    "\n",
    "imputed_mcmc_cov = {}\n",
    "\n",
    "for n in collapsed_tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        imputed[n.name], imputed_z[n.name]  = full_posterior.imputation_internal(n,\n",
    "                                                            give_mean=False,\n",
    "                                                            library_size=empirical_l\n",
    "                                                           )\n",
    "        _, imputed_mcmc_cov[n.name] = full_posterior.mcmc_estimate(query_node=n,\n",
    "                                                                    n_samples=20\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 313 into shape (100)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-cf7fd8fc383e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimputed_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimputed_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcas_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\", 'histogram.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 313 into shape (100)"
     ]
    }
   ],
   "source": [
    "imputed_X = [x[0] for x in imputed.values()]\n",
    "imputed_X = np.array(imputed_X).reshape(-1, cas_dataset.X.shape[1])\n",
    "#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\", 'histogram.png')"
   ]
  },
  {
   "source": [
    "# 3. Fitting scVI - full batch update"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "198866184\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 280.8707211289305\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.2635907919513\n",
      "training:  58%|█████▊    | 404/700 [00:06<00:05, 58.34it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.66221619907407\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.50475999816143\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.7267065054512\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.56555266147535\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.9251085059803\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.4613978120728\n",
      "training:  59%|█████▊    | 410/700 [00:06<00:04, 58.77it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 280.3260823515753\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.12120274606576\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.16508523158035\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.6762903453123\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 279.64854628952804\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.8934236446005\n",
      "training:  59%|█████▉    | 416/700 [00:06<00:05, 55.51it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.80677819391514\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.5182398123706\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.5678468157317\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.65245093891906\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.51759107167175\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.0650788696144\n",
      "training:  60%|██████    | 422/700 [00:06<00:05, 54.33it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 280.06839838161886\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.61532740945677\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.1042922427602\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.63377721080167\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.2240550958578\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.62088217774925\n",
      "training:  61%|██████    | 428/700 [00:07<00:05, 53.22it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.520613579066\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.87392675888583\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.63665480310516\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.4484443837196\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 278.06552241955825\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.40038513305217\n",
      "training:  62%|██████▏   | 434/700 [00:07<00:05, 51.51it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.97280842269123\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.5120817115068\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.7605729318175\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.46835521713604\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.98943948912466\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.69700172155734\n",
      "training:  63%|██████▎   | 440/700 [00:07<00:04, 53.65it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.0133219235888\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.1582792082445\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 277.1339201424359\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.9599419137186\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.2105051411138\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.31257813371406\n",
      "training:  64%|██████▎   | 446/700 [00:07<00:04, 54.75it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.5943572685488\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.96996513312405\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.50208608303006\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.2368018172141\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.5761841224746\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.8597600116827\n",
      "training:  65%|██████▍   | 452/700 [00:07<00:04, 55.37it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.7142545872226\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.158619640686\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.2557130202376\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.29557468963395\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.76331466814696\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.503089377389\n",
      "training:  65%|██████▌   | 458/700 [00:07<00:04, 51.98it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.9897253785002\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.2778983041963\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.8625568218651\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 276.3123021163276\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.01636280357843\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.0280554573444\n",
      "training:  66%|██████▋   | 464/700 [00:07<00:04, 50.55it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.26263272108105\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 275.14132002205537\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.3997814104846\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.63813203400747\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.5443510150636\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.4635090665897\n",
      "training:  67%|██████▋   | 470/700 [00:07<00:04, 50.47it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.8737781691488\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.6383466980219\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.9311046210303\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.5296637192072\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.39247301362246\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.6209231215386\n",
      "training:  68%|██████▊   | 476/700 [00:08<00:04, 49.44it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.88563849588104\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.02639189402373\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.32518633743877\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.6996558938842\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.232092543988\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.6228764741677\n",
      "training:  69%|██████▉   | 482/700 [00:08<00:04, 50.42it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.9195868036925\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.75554880476676\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.69434816717575\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.33935868682113\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.6944603433974\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.4875686561041\n",
      "training:  70%|██████▉   | 488/700 [00:08<00:04, 52.52it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 274.0829015680572\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.4474988442695\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.43272143668645\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.60541093110567\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.47702134633425\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.37958833418924\n",
      "training:  71%|███████   | 494/700 [00:08<00:03, 53.60it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.6729934766985\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.991134239897\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.64980349765636\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.86100261908484\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.7195316419631\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.6113627549548\n",
      "training:  71%|███████▏  | 500/700 [00:08<00:03, 51.80it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.21710820988295\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.81520983036114\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.8490352716563\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.0443820266939\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 273.04877911242073\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.6597632068226\n",
      "training:  72%|███████▏  | 506/700 [00:08<00:03, 53.19it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.80486327190886\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.1184306646354\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.87920501766894\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.0317791503557\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.7413101072842\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.86648117267697\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.6586572530989\n",
      "training:  73%|███████▎  | 513/700 [00:08<00:03, 56.00it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.76583540282803\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.420972628759\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.0612556418376\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.4614476659478\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.2058004839049\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 272.512360224826\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.4977146851873\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.4417370341447\n",
      "training:  74%|███████▍  | 521/700 [00:08<00:02, 60.69it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.7972435403882\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.3929209663443\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.5858970539569\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.5435822326811\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.2905446090208\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.55973207291123\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.0352500890391\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.099916943016\n",
      "training:  76%|███████▌  | 529/700 [00:08<00:02, 63.89it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.1087675595084\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.5663056703308\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.26638186028686\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.8026894650585\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.29481728071465\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.0610174011665\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.2500001256889\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.1971002458216\n",
      "training:  77%|███████▋  | 537/700 [00:09<00:02, 66.20it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.0817190098352\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.77242530309155\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 271.0621470278211\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.56144288781144\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.8011905211942\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.5801125137069\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.00510724825676\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.14704317556374\n",
      "training:  78%|███████▊  | 545/700 [00:09<00:02, 67.94it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 270.02730523890995\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.75413787744344\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.8267280172477\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.6982518849697\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.5270611743842\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.96992404425254\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.8610705681963\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.6517321515785\n",
      "training:  79%|███████▉  | 553/700 [00:09<00:02, 69.06it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.6574553796145\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.84349900622533\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 269.09520450827046\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.8914564572766\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.7297282486504\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.7582742267796\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.44696452197854\n",
      "training:  80%|████████  | 560/700 [00:09<00:02, 68.98it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.28981497880005\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.3242353490874\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.0204210081848\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.6787908862851\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.98363761545875\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.31054524783286\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.8445600121787\n",
      "training:  81%|████████  | 567/700 [00:09<00:01, 68.49it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.7642420666658\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.87835292308085\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.710212724865\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.5501671718655\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.6807369343416\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.97221748606137\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.44877268421914\n",
      "training:  82%|████████▏ | 574/700 [00:09<00:01, 67.21it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.19047470414546\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.5656042814198\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.7171448899154\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.12751358433576\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.6753274196496\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 268.04127613859043\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.8402010316477\n",
      "training:  83%|████████▎ | 581/700 [00:09<00:01, 67.63it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.6125286305369\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.4373882018342\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.36425569791425\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.5100876360516\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.07103072494965\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.28007073748915\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.36847270456997\n",
      "training:  84%|████████▍ | 588/700 [00:09<00:01, 68.19it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.1244759866753\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.30448283083484\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.3554801556422\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.5518896772805\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.7713460458443\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.2508208395493\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.2318719509242\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.9260622568025\n",
      "training:  85%|████████▌ | 596/700 [00:09<00:01, 69.29it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.5635658533659\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.340667845614\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.16170392101714\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.3453433562202\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.40926114618594\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.4539637479422\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.17188783824383\n",
      "training:  86%|████████▌ | 603/700 [00:09<00:01, 67.66it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.4874109025158\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.55615976619964\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.0114178375599\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.98056159256066\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.328406656392\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 267.0154609490919\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.16074625589096\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.00636005135397\n",
      "training:  87%|████████▋ | 611/700 [00:10<00:01, 68.72it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.93853307779904\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.6802021726311\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.14623159834565\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.8512525359352\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.8072264395214\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.1237936679564\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.5801078217417\n",
      "training:  88%|████████▊ | 618/700 [00:10<00:01, 68.52it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.65483240909026\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.90368458783007\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.53160058298846\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.2382740532174\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.65926514807904\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.7074489200914\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.0787431557507\n",
      "training:  89%|████████▉ | 625/700 [00:10<00:01, 66.23it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.5084659109106\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.3854657921014\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.9101041475522\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.2143537857887\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.76868224503875\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.7540842726917\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.89685696234324\n",
      "training:  90%|█████████ | 632/700 [00:10<00:01, 60.78it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.08900066429027\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.9256316861408\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 266.0801002236697\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.13182577591976\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.05033712421863\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.55939327002426\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.06483486551207\n",
      "training:  91%|█████████▏| 639/700 [00:10<00:01, 60.31it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.70568471223083\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.6937319056788\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.6226018847606\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.72293089925296\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.32537846989044\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.72294450772347\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.1716025028525\n",
      "training:  92%|█████████▏| 646/700 [00:10<00:00, 61.78it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.9780172785927\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 265.7749646385383\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.60016901385234\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.55349857637117\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.75749748132534\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.8830137627138\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.03909712497955\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.1535004359118\n",
      "training:  93%|█████████▎| 654/700 [00:10<00:00, 64.37it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.7158450117385\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.3846196344941\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.0176823277265\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.04226155175616\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.61563376780714\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.7314080179464\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.9735693942172\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.3190031433966\n",
      "training:  95%|█████████▍| 662/700 [00:10<00:00, 66.46it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.04051701124604\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.8621159979513\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.8333508958835\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.4699772659503\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.4656856604924\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.95529603181143\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.5799529710833\n",
      "training:  96%|█████████▌| 669/700 [00:10<00:00, 67.35it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.93466181802285\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.2110129159011\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.2739102391691\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.976297913441\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.77104671120424\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.0533838121345\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.60973071027195\n",
      "training:  97%|█████████▋| 676/700 [00:11<00:00, 64.95it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.859760674147\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.16587474541524\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.74034838803385\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.442483205726\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 264.0925193190478\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.1662720158391\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.6197092075182\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.330868101526\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.9569303139334\n",
      "training:  98%|█████████▊| 685/700 [00:11<00:00, 70.62it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.38059144031365\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.78970895505006\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.46519940236533\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.4552419089833\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.34590372355234\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.2661598921726\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.14461137088864\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.0222058373679\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.42791613175183\n",
      "training:  99%|█████████▉| 694/700 [00:11<00:00, 75.62it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.3759267249776\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.62400136041344\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.49867413309016\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.80953648226296\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 263.06581086069457\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 262.2441164458091\n",
      "training: 100%|██████████| 700/700 [00:11<00:00, 61.34it/s]\n"
     ]
    }
   ],
   "source": [
    "treevae_full = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=10,\n",
    "              n_hidden=64,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=False\n",
    "             )\n",
    "\n",
    "freq = 100\n",
    "trainer_full = TreeTrainer(\n",
    "    model = treevae_full,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")\n",
    "\n",
    "trainer_full.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scVI - full - batch train ELBO: 266.6711219475009\n"
     ]
    }
   ],
   "source": [
    "full_batch_posterior = trainer_full.create_posterior(trainer_full.model, cas_dataset, trainer_full.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "\n",
    "print(\"scVI - full - batch train ELBO: {}\".format(full_batch_posterior.compute_elbo().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elbo_treevae  = []\n",
    "elbo_vae = []\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    elbo_vae.append(full_batch_posterior.compute_elbo().item())\n",
    "    elbo_treevae.append(full_posterior.compute_elbo().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CascVI Elbo: mean = 262.9916118672789 | std = 0.3982202545057567\n"
     ]
    }
   ],
   "source": [
    "print(\"CascVI Elbo: mean = {} | std = {}\".format(np.mean(elbo_treevae), np.std(elbo_treevae)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scVI Elbo: mean = 266.3793400925319 | std = 0.4087974732964147\n"
     ]
    }
   ],
   "source": [
    "print(\"scVI Elbo: mean = {} | std = {}\".format(np.mean(elbo_vae), np.std(elbo_vae)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fitting scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anndata\n",
    "gene_dataset = GeneExpressionDataset()\n",
    "gene_dataset.populate_from_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs = 700\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=64,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=10,\n",
    "                  ldvae=ldvae\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 700/700 [01:05<00:00, 10.67it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=None\n",
    "                              )\n",
    "\n",
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3, use_cuda=use_cuda,) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "        \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")\n",
    "plt.show()\n",
    "plt.savefig('training_scvi.png')"
   ]
  },
  {
   "source": [
    "***Training elbo***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "635.9727080742123"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior(model=vae,\n",
    "                                               gene_dataset=gene_dataset \n",
    "                                                )\n",
    "\n",
    "scvi_posterior.elbo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 2 (Decoded Average Latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_size = np.mean(np.sum(X, axis=1))\n",
    "scvi_latent = np.array([scvi_posterior.get_latent(give_mean=False)[0] for i in range(10)])\n",
    "\n",
    "imputed_scvi_2, imputed_scvi_2_z = scvi_baseline_z(collapsed_tree,\n",
    "                                        posterior=scvi_posterior,\n",
    "                                        model=vae,\n",
    "                                        weighted=False,\n",
    "                                        n_samples_z=1,\n",
    "                                        library_size=library_size,\n",
    "                                        use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((603, 10), (603, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "cascvi_latent = full_posterior.get_latent()\n",
    "scvi_latent = scvi_posterior.get_latent()[0]\n",
    "scvi_full_latent = full_batch_posterior.get_latent()\n",
    "\n",
    "scvi_latent.shape, cascvi_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of scVI encodings:  -17542.425066081545\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(scvi_latent, cas_dataset.barcodes, scvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), scvi_latent.shape[1], False)\n",
    "mp_lik_scvi = treevae.aggregate_messages_into_leaves_likelihood(10, add_prior=True)\n",
    "print(\"Likelihood of scVI encodings: \", mp_lik_scvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  -4268.115672534094\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(cascvi_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(10, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  -16072.930023850036\n"
     ]
    }
   ],
   "source": [
    "treevae_full.initialize_visit()\n",
    "treevae_full.initialize_messages(scvi_full_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae_full.perform_message_passing((treevae_full.tree & treevae_full.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_full = treevae_full.aggregate_messages_into_leaves_likelihood(10, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_full.item())"
   ]
  },
  {
   "source": [
    "# 6. Output files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***Uncertainty***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz_v = full_posterior.empirical_qz_v(n_samples=100,\n",
    "                                    norm=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  node ID  variance  depth\n",
       "0                       0  0.205366    0.0\n",
       "1                       1  0.103199   12.0\n",
       "2                       2  0.205366    6.0\n",
       "3                       3  0.051055    6.0\n",
       "4                       4  0.187519    1.0\n",
       "5                       5  0.086763   13.0\n",
       "6   RE.CTAGAGTCAGTAAGAT-1  0.000012   14.0\n",
       "7                       7  0.201125   11.0\n",
       "8                       8  0.233221    7.0\n",
       "9                       9  0.064120   10.0\n",
       "10                     10  0.113294   12.0\n",
       "11                     11  0.196585   10.0\n",
       "12                     12  0.105640   13.0\n",
       "13                     13  0.129317   10.0\n",
       "14                     14  0.160974    9.0\n",
       "15                     15  0.161869   10.0\n",
       "16                     16  0.082325   12.0\n",
       "17                     17  0.118106    7.0\n",
       "18                     18  0.122030   10.0\n",
       "19                     19  0.120771    9.0\n",
       "20                     20  0.168625   10.0\n",
       "21                     21  0.087006   11.0\n",
       "22                     22  0.170207   12.0\n",
       "23  RE.ACATGGTAGAAGATTC-1  0.000005   14.0\n",
       "24  RE.CACAAACGTGCGATAG-1  0.000008   14.0\n",
       "25                     25  0.105638   13.0\n",
       "26                     26  0.248458    2.0\n",
       "27  M2.TCGTACCTCCAACCAA-1  0.000009   14.0\n",
       "28  RE.TTAACTCTCCCAGGTG-1  0.000012   14.0\n",
       "29  RE.CGGTTAATCTATCCCG-1  0.000015   14.0\n",
       "30  M1.CTCAGAAGTCTCCATC-1  0.000008   14.0\n",
       "31                     31  0.112988   12.0\n",
       "32                     32  0.220820    8.0\n",
       "33  M1.ATTGGACAGATAGGAG-1  0.000006   14.0\n",
       "34                     34  0.084296   11.0\n",
       "35                     35  0.072929   12.0\n",
       "36                     36  0.107667   12.0\n",
       "37  M2.TCGCGAGGTTACGGAG-1  0.000004   14.0\n",
       "38                     38  0.098114   13.0\n",
       "39  M2.GGGCACTCAATCTGCA-1  0.000007   14.0\n",
       "40  M1.ATCTGCCCACCGAAAG-1  0.000012   14.0\n",
       "41                     41  0.098114   13.0\n",
       "42  M2.AGGCCACTCCAAAGTC-1  0.000010   14.0\n",
       "43  M2.CTCGTCACATCATCCC-1  0.000006   14.0\n",
       "44                     44  0.087882   13.0\n",
       "45                     45  0.087882   13.0\n",
       "46  RE.TGCGTGGAGAGTACCG-1  0.000015   14.0\n",
       "47                     47  0.098460   11.0\n",
       "48  RW.CAGCGACCAGATCTGT-1  0.000003   14.0\n",
       "49  RE.GGATGTTCACAGACAG-1  0.000012   14.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>node ID</th>\n      <th>variance</th>\n      <th>depth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.205366</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.103199</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.205366</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.051055</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.187519</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.086763</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RE.CTAGAGTCAGTAAGAT-1</td>\n      <td>0.000012</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.201125</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.233221</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.064120</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.113294</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>0.196585</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0.105640</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>0.129317</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>0.160974</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>0.161869</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>0.082325</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>0.118106</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>0.122030</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>0.120771</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>0.168625</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>0.087006</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>0.170207</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>RE.ACATGGTAGAAGATTC-1</td>\n      <td>0.000005</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>RE.CACAAACGTGCGATAG-1</td>\n      <td>0.000008</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>0.105638</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>0.248458</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>M2.TCGTACCTCCAACCAA-1</td>\n      <td>0.000009</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>RE.TTAACTCTCCCAGGTG-1</td>\n      <td>0.000012</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>RE.CGGTTAATCTATCCCG-1</td>\n      <td>0.000015</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>M1.CTCAGAAGTCTCCATC-1</td>\n      <td>0.000008</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>0.112988</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>0.220820</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>M1.ATTGGACAGATAGGAG-1</td>\n      <td>0.000006</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>0.084296</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>0.072929</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0.107667</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>M2.TCGCGAGGTTACGGAG-1</td>\n      <td>0.000004</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>0.098114</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>M2.GGGCACTCAATCTGCA-1</td>\n      <td>0.000007</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>M1.ATCTGCCCACCGAAAG-1</td>\n      <td>0.000012</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>0.098114</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>M2.AGGCCACTCCAAAGTC-1</td>\n      <td>0.000010</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>M2.CTCGTCACATCATCCC-1</td>\n      <td>0.000006</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>0.087882</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>0.087882</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>RE.TGCGTGGAGAGTACCG-1</td>\n      <td>0.000015</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>0.098460</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>RW.CAGCGACCAGATCTGT-1</td>\n      <td>0.000003</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>RE.GGATGTTCACAGACAG-1</td>\n      <td>0.000012</td>\n      <td>14.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "output = []\n",
    "columns = ['node ID', 'variance', 'depth']\n",
    "idx = 0\n",
    "for n in collapsed_tree.traverse('levelorder'):\n",
    "    depth = n.get_distance(collapsed_tree)\n",
    "    if not n.is_leaf():\n",
    "        output.append([n.name, np.linalg.norm(imputed_mcmc_cov[n.name]), depth])\n",
    "    else:\n",
    "        output.append([n.name, qz_v[idx], depth])\n",
    "        idx += 1\n",
    "\n",
    "df_uncertainty = pd.DataFrame(data=output, columns=columns)\n",
    "df_uncertainty.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncertainty.to_csv('uncertainty_corrected.csv')"
   ]
  },
  {
   "source": [
    "***Imputations***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(916, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "n_nodes = len([n for n in collapsed_tree.traverse()])\n",
    "n_nodes, n_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X = np.zeros((n_nodes, n_genes))\n",
    "imputed_avg = np.zeros((n_nodes, n_genes))\n",
    "\n",
    "idx = 0\n",
    "for i, n in enumerate(collapsed_tree.traverse('levelorder')):\n",
    "    if not n.is_leaf():\n",
    "        imputed_X[i] = imputed[n.name].astype(int)\n",
    "        imputed_avg[i] = imputed_scvi_2[n.name].astype(int)\n",
    "    else:\n",
    "        imputed_X[i] = X[idx]\n",
    "        imputed_avg[i] = X[idx]     \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_genes = hotspot_genes['Gene'].values[:100]\n",
    "\n",
    "df_imputed = pd.DataFrame(data=imputed_X, \n",
    "                          index=[n.name for n in collapsed_tree.traverse('levelorder')],\n",
    "                         columns=top_100_genes\n",
    "                         )\n",
    "\n",
    "df_imputed_avg = pd.DataFrame(data=imputed_avg, \n",
    "                          index=[n.name for n in collapsed_tree.traverse('levelorder')],\n",
    "                         columns=top_100_genes\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.to_csv('imputations_metastasis_treeVAE.txt')\n",
    "df_imputed_avg.to_csv('imputations_metastasis_VAE.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7  ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}