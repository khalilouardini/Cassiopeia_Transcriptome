{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/external\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***import ete3 Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "#tree_name = \"/Users/khalilouardini/Desktop/projects/scVI/scvi/data/Cassiopeia_trees/3726_NT_T1_tree.processed.collapsed.tree\"\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/Cassiopeia_trees/tree_test.txt\"\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "#tree = Tree()\n",
    "#tree.populate(60)\n",
    "\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prune tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GCCTGAG-1\n443 L6.AACGTTGCATGAACCT-1\n444 L6.CGTCACTCATCGGTTA-1\n445 L6.TGGCCAGCAGTAACGG-1\n446 L6.GCTGGGTCATACGCTA-1\n447 L6.CGCTTCATCAAACAAG-1\n448 L6.CATGGCGGTTAAGATG-1\n449 L6.GTCTCGTAGTGAAGTT-1\n450 L6.GAAATGAGTTGGTGGA-1\n451 L6.TCATTACTCAGTACGT-1\n452 L6.TTCTACAAGGCATGTG-1\n453 4|2|2|2|2|9|2|7|2|2|2|5|0|6|5|-|-|-|5|6|5|2|11|-|-|-|-|-|-\n454 4|2|2|2|2|9|2|7|2|2|2|5|2|6|0|6|2|0|5|6|5|2|8|3|0|24|-|-|-\n455 5|2|2|2|2|5|2|7|2|2|2|5|8|8|9|4|2|2|9|6|5|2|8|3|12|10|-|-|-\n456 L6.TGGCCAGCAATCACAC-1\n457 L6.CACAGTACAGACACTT-1\n458 L6.TATCAGGTCGAGGTAG-1\n459 L6.TGGGAAGAGGGAACGG-1\n460 L6.CATCAAGAGTTCGCGC-1\n461 L6.TTGGAACGTTATGTGC-1\n462 L6.CACAGGCTCGATGAGG-1\n463 -|-|-|2|2|5|5|6|2|2|2|5|5|5|5|5|2|2|-|-|-|2|8|3|7|2|-|-|-\n464 L6.GGGACCTCAGACAAAT-1\n465 L6.GCAATCAGTCCAGTTA-1\n466 L6.ACACCGGGTCCGTTAA-1\n467 L6.GTGTTAGCAGCGATCC-1\n468 L6.CGATGGCAGGCGATAC-1\n469 L6.GCAGTTATCATTGCGA-1\n470 -|-|-|2|2|2|-|-|-|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n471 L6.TGACAACTCAGCTCGG-1\n472 L6.CCAGCGACACGCATCG-1\n473 L6.CGCTATCAGGGTCGAT-1\n474 L6.TTCTCCTAGCTGAAAT-1\n475 L6.TGGCTGGTCGGAAATA-1\n476 L6.GACTGCGAGTTGCAGG-1\n477 L6.CCCAATCGTGGACGAT-1\n478 L6.CAGGTGCAGTCCGGTC-1\n479 L6.ACTATCTAGGCACATG-1\n480 L6.CCTAAAGCAAGTAATG-1\n481 L6.GGAAAGCTCATGCTCC-1\n482 L6.CGATTGATCCAAACAC-1\n483 L6.TGTCCCAAGGTCGGAT-1\n484 L6.GGACAAGCAAGTCTGT-1\n485 L6.CTCGTCAGTGCCTGTG-1\n486 L6.AAAGTAGGTGCACCAC-1\n487 2|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|0|0|0|0|0|0\n488 2|2|2|2|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|0|0|0|0|0|0\n489 L6.ACCGTAAAGTCCGGTC-1\n490 2|2|2|2|2|0|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|5|16|14|0|0|0\n491 2|2|2|2|2|0|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n492 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|0|0|0|0|0|0\n493 2|2|2|2|2|5|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|0|0|0|-|-|-\n494 2|2|2|2|2|28|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n495 2|2|2|2|2|0|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|0|13|0|0|0|0\n496 2|2|2|2|2|0|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n497 2|2|2|2|2|0|2|2|2|2|2|2|0|2|0|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n498 2|2|2|2|2|0|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|3|5|0|0|0|0\n499 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|9|2|2|2|2|3|5|0|-|-|-\n500 L6.GTTACAGGTACACCGC-1\n501 2|2|2|2|2|0|2|2|2|2|2|2|-|-|-|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n502 2|2|2|2|2|0|2|2|2|2|2|2|9|2|0|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n503 2|2|2|2|2|0|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|0|0|0|0|0|0\n504 2|2|2|2|2|5|2|2|2|2|2|2|9|2|0|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n505 L6.ACGCCAGCACTAGTAC-1\n506 L6.AGCATACCACGGATAG-1\n507 L6.TCAGCTCTCATAGCAC-1\n508 L6.ATGCGATTCCTGCAGG-1\n509 L6.AACCGCGAGTCCATAC-1\n510 L6.GAGCAGAAGTGTACCT-1\n511 2|2|2|2|6|19|4|9|2|2|2|0|7|4|0|4|2|0|2|4|10|4|16|3|18|18|2|2|2\n512 L6.GTCATTTTCGCATGAT-1\n513 L6.TCAGCAAGTTACCGAT-1\n514 L6.TACTTACCAGTCCTTC-1\n515 L6.ACATACGTCCAACCAA-1\n516 L6.CGTGTAAGTTGTGGAG-1\n517 L6.CAGCAGCAGTTCCACA-1\n518 L6.CAAGTTGTCTACTATC-1\n519 3|2|2|2|4|5|4|5|2|2|2|5|2|4|5|4|2|0|0|5|4|0|0|3|6|5|2|2|2\n520 3|2|2|2|4|0|4|0|2|2|2|5|0|4|0|4|2|0|2|5|4|-|-|3|6|5|2|2|2\n521 3|2|2|2|4|8|4|0|2|2|2|5|2|4|5|4|2|6|2|5|4|2|2|3|6|5|2|2|2\n522 L6.CCTACCAAGCGCTCCA-1\n523 L6.TAGTTGGCAGCCACCA-1\n524 L6.AAGGTTCCAGGAATCG-1\n525 3|2|2|2|4|5|4|12|2|2|2|5|-|-|-|4|2|12|7|5|4|-|-|3|15|13|2|2|2\n526 L6.CGTCAGGTCGTACGGC-1\n527 2|2|2|2|7|0|7|10|2|2|2|3|3|4|0|8|2|2|7|5|0|0|0|3|13|11|0|2|2\n528 L6.ATCATGGCAGCTGGCT-1\n529 L6.CATGGCGTCACTGGGC-1\n530 2|2|2|2|10|23|9|9|2|2|2|0|14|4|22|8|2|21|10|5|4|-|-|3|20|25|2|2|2\n531 2|2|2|2|10|23|9|9|2|2|2|0|14|4|5|8|2|0|10|5|4|-|-|3|20|0|2|2|2\n532 2|2|2|2|10|23|9|9|2|2|2|5|14|4|9|8|2|0|10|5|4|3|8|3|20|30|2|2|2\n533 L6.GAGTCCGCACATTTCT-1\n534 L6.AGGGAGTTCAATCACG-1\n535 L6.TACGGTACATACTCTT-1\n536 L6.CAGGTGCTCCTACAGA-1\n537 L6.CTGCTGTAGTCACGCC-1\n538 L6.CGGGTCAAGATAGCAT-1\n539 L6.TATCAGGCAGGAACGT-1\n540 L6.CGGAGCTCACAGTCGC-1\n541 L6.CGTCAGGGTTATGTGC-1\n542 L6.TACTTGTAGCGGCTTC-1\n543 L6.TTAGTTCGTCACTGGC-1\n544 L6.GGACGTCGTTCAACCA-1\n545 2|2|2|2|6|11|4|9|2|2|2|7|-|-|-|4|2|8|6|4|4|4|17|3|10|0|2|2|2\n546 L6.GCTTGAAGTGTATGGG-1\n547 L6.CATCGAACACCTATCC-1\n548 L6.ACTTACTGTTGCTCCT-1\n549 L6.TGAGCATGTTGGACCC-1\n550 L6.GGCCGATCATTTCACT-1\n551 L6.CTCTGGTTCTTGCAAG-1\n552 L6.GGGAATGCAGTGGAGT-1\n553 L6.ACTTTCAAGCGGATCA-1\n554 L6.CTGTTTAGTAGCGTGA-1\n555 L6.ATGAGGGCATCCCATC-1\n556 L6.AGCTCCTGTCGAAAGC-1\n557 4|2|2|2|2|9|2|7|2|2|2|5|2|6|5|-|-|-|5|6|5|2|11|-|-|-|-|-|-\n558 4|2|2|2|2|9|2|7|2|2|2|5|2|6|0|6|2|0|5|6|5|2|8|-|-|-|-|-|-\n559 L6.CGATCGGTCACCACCT-1\n560 L6.CACATTTAGGCTAGAC-1\n561 L6.TTAACTCAGAGACGAA-1\n562 L6.GGCGTGTTCTATCCTA-1\n563 L6.AGCCTAACAACTGCTA-1\n564 L6.TCGAGGCAGCGATAGC-1\n565 L6.CCCATACGTCACTTCC-1\n566 L6.AAGGTTCTCGCTGATA-1\n567 L6.AAGGTTCAGGCATGGT-1\n568 2|2|2|2|2|2|2|2|2|-|-|-|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n569 L6.TTTGGTTTCTGAGTGT-1\n570 2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|0|0|0|-|-|-\n571 L6.GTCCTCAGTGGCAAAC-1\n572 L6.GTCATTTGTTGAGTTC-1\n573 L6.TGGGCGTGTGATGTCT-1\n574 2|2|2|2|2|0|2|2|2|2|2|2|10|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n575 L6.CTCGGAGGTAGCGTCC-1\n576 L6.CGATTGATCCTAAGTG-1\n577 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|0|0|0|-|-|-\n578 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|8|0|6|-|-|-\n579 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|0|6|5|-|-|-\n580 2|2|2|2|2|5|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n581 L6.CAGCAGCCATTGGTAC-1\n582 L6.TAGTTGGAGGCATGTG-1\n583 L6.GCTCCTAGTGTGACGA-1\n584 L6.TGAGCATGTTACGACT-1\n585 L6.CAGTAACGTGAGTATA-1\n586 2|2|2|2|2|5|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n587 L6.CTCGGAGCACTATCTT-1\n588 L6.TGCTACCAGGGCATGT-1\n589 L6.TCAGCTCGTATTCTCT-1\n590 L6.TACTCGCTCATGTAGC-1\n591 L6.TACGGGCAGCCGTCGT-1\n592 L6.GTGTTAGGTGATGATA-1\n593 L6.CGTTCTGGTTCGAATC-1\n594 L6.AAACCTGGTTTGCATG-1\n595 2|2|2|2|2|0|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n596 2|2|2|2|2|47|2|2|2|2|2|2|0|2|0|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n597 L6.GACGTGCAGTCTCCTC-1\n598 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|0|0|0\n599 L6.ACATACGCACGTGAGA-1\n600 L6.ACTGCTCGTCTTTCAT-1\n601 L6.GCATACAGTAAGTGTA-1\n602 L6.GGAAAGCGTATTCTCT-1\n603 L6.GCACTCTGTATATCCG-1\n604 L6.GCCAAATAGTGCCAGA-1\n605 L6.GTTAAGCTCGCGATCG-1\n606 L6.CTCGTACCACTTACGA-1\n607 L6.GGAGCAAGTCTAGCCG-1\n608 L6.CACTCCATCGGCGCAT-1\n609 L6.AAGACCTAGAATTCCC-1\n610 L6.TCGTAGACACCCATTC-1\n611 2|2|2|2|2|0|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|8|0|0|-|-|-\n612 2|2|2|2|2|6|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n613 2|2|2|2|2|0|2|2|2|2|2|2|9|2|15|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n614 L6.CACCACTGTTCCAACA-1\n615 L6.GGTGTTACATGGGACA-1\n616 L6.TGAGCCGCATAGGATA-1\n617 L6.ACACTGAAGTCCTCCT-1\n618 3|2|2|2|4|5|4|5|2|2|2|5|2|4|5|4|2|0|2|5|4|0|0|3|6|5|2|2|2\n619 L6.AGCTTGACACCCTATC-1\n620 L6.CTAGCCTAGTTAGGTA-1\n621 3|2|2|2|4|0|4|0|2|2|2|5|7|4|0|4|2|5|2|5|4|-|-|3|6|5|2|2|2\n622 3|2|2|2|4|0|4|9|2|2|2|5|0|4|25|4|2|0|2|5|4|-|-|3|6|5|2|2|2\n623 3|2|2|2|4|8|4|0|2|2|2|5|2|4|5|4|2|6|2|5|4|-|-|3|6|5|2|2|2\n624 L6.ACGCCGAAGGAATCGC-1\n625 3|2|2|2|4|5|4|12|2|2|2|5|-|-|-|4|2|12|7|5|4|-|-|3|15|13|-|-|-\n626 L6.AGTGAGGCAACGATCT-1\n627 L6.AGCGTCGCAGCCTGTG-1\n628 2|2|2|2|7|0|7|10|2|2|2|3|3|4|10|8|2|2|7|5|4|0|0|3|13|11|0|2|2\n629 L6.GGCGACTGTGTAACGG-1\n630 2|2|2|2|7|5|7|10|2|2|2|3|3|4|5|8|2|2|7|5|12|-|-|3|13|11|0|2|2\n631 2|2|2|2|10|23|9|9|2|2|2|0|14|4|22|8|2|21|10|5|4|-|-|3|20|25|-|-|-\n632 L6.ACATGGTTCACGACTA-1\n633 L6.TAGGCATCAGATGGGT-1\n634 L6.AACCGCGCAGACGCCT-1\n635 L6.GTACTTTGTGGTACAG-1\n636 2|2|2|2|10|23|9|9|2|2|2|0|14|4|5|8|2|25|10|5|4|-|-|3|20|0|2|2|2\n637 L6.CGAGCCACACATGACT-1\n638 L6.ACGGAGAGTAAGGGCT-1\n639 L6.GCAAACTTCGCAAACT-1\n640 L6.CCACGGACATCGATGT-1\n641 L6.ATCGAGTTCCATGAGT-1\n642 L6.TGTATTCCACTTCGAA-1\n643 L6.GACGTGCTCAGCAACT-1\n644 4|2|2|2|2|9|2|7|2|2|2|5|2|6|5|-|-|-|5|6|5|2|8|-|-|-|-|-|-\n645 L6.CACATTTGTATTCGTG-1\n646 L6.CGATCGGTCCCATTTA-1\n647 L6.GTACTCCGTGACTCAT-1\n648 2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n649 L6.CGTAGCGAGCCATCGC-1\n650 L6.TCTGAGACATATGAGA-1\n651 L6.CGACTTCGTACGCACC-1\n652 2|2|2|2|2|0|-|-|-|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n653 L6.GTCAAGTAGCGATTCT-1\n654 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n655 L6.AACTCAGGTGGGTCAA-1\n656 L6.GAAACTCCAAATACAG-1\n657 L6.AGAGTGGGTTGCCTCT-1\n658 2|2|2|2|2|7|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|6|5|-|-|-\n659 L6.TAGCCGGTCTGGCGTG-1\n660 L6.CGATGTAGTCGCCATG-1\n661 L6.CCCAATCAGAGACTTA-1\n662 L6.CTTTGCGTCCTTGCCA-1\n663 L6.TCAGCTCCATCTATGG-1\n664 L6.ACATGGTTCAGCTCGG-1\n665 L6.TGATTTCGTAAAGTCA-1\n666 L6.GATGAAATCAAAGACA-1\n667 L6.GAAGCAGCATACCATG-1\n668 2|2|2|2|2|47|2|2|2|2|2|2|2|2|0|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n669 L6.GTAACGTAGACAGACC-1\n670 2|2|2|2|2|0|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n671 2|2|2|2|2|20|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n672 2|2|2|2|2|5|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n673 2|2|2|2|2|6|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n674 2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|0|0|0\n675 2|2|2|2|2|0|2|2|2|-|-|-|9|2|15|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n676 L6.ATGGGAGAGAGACTTA-1\n677 L6.TGACGGCCATGCCTAA-1\n678 L6.TGACTAGAGAGCTGCA-1\n679 L6.TCGGTAACACCGCTAG-1\n680 L6.GCTGGGTTCTTGAGGT-1\n681 L6.TAAGAGATCGCATGGC-1\n682 L6.TGACAACTCCTCAACC-1\n683 L6.TCTTTCCTCCACGTGG-1\n684 L6.TCTCTAACACTCGACG-1\n685 L6.TCGCGAGGTGCACTTA-1\n686 L6.TACTTACAGCTGATAA-1\n687 L6.TACCTTAAGATGCCAG-1\n688 L6.GGTATTGGTGTGACGA-1\n689 L6.GGGACCTAGGGTCTCC-1\n690 L6.GGGAATGGTGCCTGGT-1\n691 L6.GGACATTCAGTAACGG-1\n692 L6.GCGCAGTGTAGATTAG-1\n693 L6.GACCTGGGTCTGGTCG-1\n694 L6.CTGATCCAGATCCGAG-1\n695 L6.CGCGTTTGTTCAGTAC-1\n696 L6.CCTTCCCTCCCACTTG-1\n697 L6.CCTTCCCAGTGGTAGC-1\n698 L6.CCTTACGAGCCGTCGT-1\n699 L6.CCTACACCATCACGTA-1\n700 L6.CCATTCGTCCACGTGG-1\n701 L6.CCATTCGTCATCGCTC-1\n702 L6.CACACCTTCTTGCAAG-1\n703 L6.CAAGGCCAGCGACGTA-1\n704 L6.AGCTCTCAGGGTTCCC-1\n705 L6.AGCTCCTCAGCCAGAA-1\n706 L6.ACATACGAGTAGTGCG-1\n707 3|2|2|2|4|5|4|5|2|2|2|5|2|4|5|4|2|0|2|5|4|-|-|3|6|5|2|2|2\n708 L6.TTGGAACGTTCGGGCT-1\n709 L6.TCGGGACTCACAATGC-1\n710 L6.AGGGTGACACCACGTG-1\n711 3|2|2|2|4|5|4|9|2|2|2|5|0|4|25|4|2|0|2|5|4|-|-|3|6|5|2|2|2\n712 L6.GACCAATTCCTTGGTC-1\n713 L6.TTCGAAGGTGATGTGG-1\n714 L6.AGCTCCTTCGTGGACC-1\n715 L6.TGGTTAGGTAAAGGAG-1\n716 L6.GGACAGAAGAACTCGG-1\n717 2|2|2|2|7|0|7|10|2|2|2|3|3|4|10|8|2|2|7|5|4|3|0|3|13|11|2|2|2\n718 L6.GCTGCTTAGTACGACG-1\n719 L6.GTAACTGGTGGCTCCA-1\n720 2|2|2|2|7|5|7|10|2|2|2|3|3|4|5|8|2|2|7|5|12|-|-|3|13|11|2|2|2\n721 L6.AGGCCGTCAGGACCCT-1\n722 L6.CCTACACGTTATCACG-1\n723 L6.GGTGTTAAGCCAACAG-1\n724 L6.ACCGTAAAGTGTTTGC-1\n725 L6.TCAGGATGTGCAACTT-1\n726 L6.TAAGCGTAGCTCCCAG-1\n727 L6.GACTAACCAGTTCATG-1\n728 L6.ACCGTAACAATGCCAT-1\n729 L6.ACATGGTGTACTTGAC-1\n730 L6.ACAGCTATCCGCTGTT-1\n731 2|2|2|2|2|2|-|-|-|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n732 L6.TTTGTCACACATCCAA-1\n733 L6.TTCTCCTAGAGCCTAG-1\n734 L6.TTATGCTCAAGGACAC-1\n735 L6.TTAGGCAAGGTTCCTA-1\n736 L6.TTAGGACAGCTAGTCT-1\n737 L6.TGTTCCGCAAGCCTAT-1\n738 L6.TGACGGCCAATAGAGT-1\n739 L6.TGACAACCACCAGATT-1\n740 L6.TCTGAGAGTCCCGACA-1\n741 L6.TCATTACCACGAGGTA-1\n742 L6.TCAGGATAGCTAACTC-1\n743 L6.TCACGAACAAGTTAAG-1\n744 L6.TAAGTGCAGCTGATAA-1\n745 L6.GTTTCTAGTTCGTCTC-1\n746 L6.GTCGTAAGTTGCGTTA-1\n747 L6.GTATTCTCACGGTAAG-1\n748 L6.GTAACGTGTTACCGAT-1\n749 L6.GGTGCGTAGCTGATAA-1\n750 L6.GGTGAAGCACCCTATC-1\n751 L6.GGGAGATTCCGGCACA-1\n752 L6.GGCGTGTTCATCGCTC-1\n753 L6.GGCCGATTCTGTCAAG-1\n754 L6.GCTTCCACAACGATGG-1\n755 L6.GCTCTGTCAAGTAGTA-1\n756 L6.GCTCCTACATTCCTCG-1\n757 L6.GCGACCACAAGCGAGT-1\n758 L6.GCAGTTATCCCAAGTA-1\n759 L6.GATCGCGGTTCCGTCT-1\n760 L6.CTGTTTAAGGCAGGTT-1\n761 L6.CTCACACTCATATCGG-1\n762 L6.CTAATGGCATTTCAGG-1\n763 L6.CGTTGGGTCCACTGGG-1\n764 L6.CGTTAGACACGTGAGA-1\n765 L6.CGTCCATTCAAGGTAA-1\n766 L6.CGTAGCGGTCTAACGT-1\n767 L6.CGTAGCGGTACCGGCT-1\n768 L6.CGGGTCAAGGATTCGG-1\n769 L6.CGGAGCTCAATGGACG-1\n770 L6.CGGACTGCACGGTAGA-1\n771 L6.CGCCAAGGTTGATTCG-1\n772 L6.CCGTGGAAGAATGTTG-1\n773 L6.CCGGTAGGTAGCTTGT-1\n774 L6.CCATGTCAGGCAGTCA-1\n775 L6.CCACGGAGTACATGTC-1\n776 L6.CATATGGCAGACGTAG-1\n777 L6.CAGTAACGTACTCTCC-1\n778 L6.CAGCTGGTCGGCCGAT-1\n779 L6.CAGCTAAAGTCTCGGC-1\n780 L6.CACATTTGTCGGCATC-1\n781 L6.CAAGTTGCAGCCTTGG-1\n782 L6.CAAGAAAGTACCCAAT-1\n783 L6.CAACCTCTCCCGGATG-1\n784 L6.ATGAGGGAGGCATGGT-1\n785 L6.AGCTCTCTCTCGAGTA-1\n786 L6.AGCGTCGTCTGTGCAA-1\n787 L6.AGACGTTGTTCCATGA-1\n788 L6.ACTATCTAGCTGATAA-1\n789 L6.ACGGGTCTCTGGTTCC-1\n790 L6.ACGGGCTGTCAACTGT-1\n791 L6.ACGCAGCTCAGAGCTT-1\n792 L6.ACGCAGCGTCTAAAGA-1\n793 L6.ACGATGTGTAGATTAG-1\n794 L6.ACCAGTAAGCTGTTCA-1\n795 L6.ACACCCTAGAATCTCC-1\n796 L6.AATCGGTAGGAGTACC-1\n797 L6.AAGTCTGGTATTACCG-1\n798 L6.AAGGAGCTCCGAAGAG-1\n799 L6.AAGCCGCAGTGGAGTC-1\n800 L6.AAATGCCCAAAGCGGT-1\n801 L6.AAAGCAAAGTTTAGGA-1\n802 L6.AAAGATGCATCGGTTA-1\n803 L6.AAACGGGCAAGAGGCT-1\n804 L6.TGAGAGGAGCGTTCCG-1\n805 L6.GTCTCGTGTAGAGTGC-1\n806 L6.GACAGAGTCCGTCAAA-1\n807 L6.CGCTGGAAGAGGTTGC-1\n808 L6.CGCCAAGAGGCAATTA-1\n809 L6.AGAGCTTTCCGCAGTG-1\n810 2|2|2|2|2|16|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n811 L6.TCGCGAGAGTCTCAAC-1\n812 L6.TAAGCGTTCACAGGCC-1\n813 2|2|2|2|2|32|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n814 L6.GTACTCCTCAAGGCTT-1\n815 L6.GATTCAGTCGTTACGA-1\n816 L6.GAAGCAGTCGGGAGTA-1\n817 L6.ATTGGACTCTGATTCT-1\n818 L6.ACGGGTCGTGGTCTCG-1\n819 L6.TTCTTAGAGAATCTCC-1\n820 L6.TTCGGTCAGAAGGGTA-1\n821 L6.TTAACTCCAGGAATGC-1\n822 L6.TGTTCCGTCGAGGTAG-1\n823 L6.TGTTCCGTCAGCGATT-1\n824 L6.TGGACGCTCTTCAACT-1\n825 L6.TGCCCTACATGGTCTA-1\n826 L6.TGAGCATGTCTGCCAG-1\n827 L6.TGACTAGCACTCGACG-1\n828 L6.TGACGGCTCAACACCA-1\n829 L6.TCTGAGACAAGCCGCT-1\n830 L6.TCTATTGTCGACAGCC-1\n831 L6.TCGTACCGTATGAAAC-1\n832 L6.TCGGTAAGTGTGACCC-1\n833 L6.TCCCGATAGACCTAGG-1\n834 L6.TACTTACTCTCCAGGG-1\n835 L6.TACGGTACAAAGGTGC-1\n836 L6.GTGAAGGTCAAAGTAG-1\n837 L6.GTCTCGTCATATACGC-1\n838 L6.GGTGAAGCACTGTCGG-1\n839 L6.GGGCACTCAATGAAAC-1\n840 L6.GGCCGATTCAAGATCC-1\n841 L6.GCTTGAAGTTATTCTC-1\n842 L6.GCGCAACCACGTAAGG-1\n843 L6.GCAAACTCATTTCACT-1\n844 L6.GATGCTAGTCCCTTGT-1\n845 L6.GACTAACTCGGACAAG-1\n846 L6.GAACATCCAACGATCT-1\n847 L6.CTTCTCTTCGGCTTGG-1\n848 L6.CTGTTTAAGGGCTCTC-1\n849 L6.CTGTGCTCACAACGCC-1\n850 L6.CTCTAATGTGCCTGTG-1\n851 L6.CTAGAGTAGCTGAAAT-1\n852 L6.CTACATTAGTACCGGA-1\n853 L6.CTAATGGGTACGCTGC-1\n854 L6.CGTGAGCGTGCAGTAG-1\n855 L6.CGTGAGCCATATGAGA-1\n856 L6.CGTAGGCAGATGTTAG-1\n857 L6.CGCTTCAAGTAGCCGA-1\n858 L6.CGCGTTTCATAGTAAG-1\n859 L6.CGCCAAGGTCAGCTAT-1\n860 L6.CGATTGAAGAGTCGGT-1\n861 L6.CGACCTTCATATACGC-1\n862 L6.CCTCAGTAGCTGATAA-1\n863 L6.CCGGGATAGAGAGCTC-1\n864 L6.CCATTCGCATTTCACT-1\n865 L6.CCATTCGCACAGTCGC-1\n866 L6.CAGTCCTTCAGAGGTG-1\n867 L6.CACAGTAGTTTCCACC-1\n868 L6.CAAGGCCGTCGGGTCT-1\n869 L6.ATTCTACTCTCATTCA-1\n870 L6.ATTCTACTCGCATGAT-1\n871 L6.ATTATCCCAGATCTGT-1\n872 L6.ATGTGTGAGCTAACTC-1\n873 L6.AGTAGTCGTCGAACAG-1\n874 L6.AGGTCATCACCATCCT-1\n875 L6.ACTTTCATCACCCGAG-1\n876 L6.ACTGCTCGTGTCGCTG-1\n877 L6.ACTATCTGTTGTTTGG-1\n878 L6.ACGGCCAAGCGTGAAC-1\n879 L6.ACGCCAGCACCTCGGA-1\n880 L6.ACCAGTATCTGGCGAC-1\n881 L6.ACACCGGAGGATCGCA-1\n882 L6.ACACCCTGTAGCACGA-1\n883 L6.AATCCAGTCCTTTCGG-1\n884 2|2|2|2|2|7|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n885 L6.CCAATCCAGTTACCCA-1\n886 L6.GCTGGGTAGATGTGGC-1\n887 L6.CATTCGCAGGGAAACA-1\n888 2|2|2|2|2|18|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n889 2|2|2|2|2|7|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n890 L6.TTATGCTGTGGTCCGT-1\n891 2|2|2|2|2|33|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n892 L6.GGGACCTCAAATTGCC-1\n893 L6.GCGCAACTCCACTCCA-1\n894 L6.CAACCTCAGCCCAATT-1\n895 L6.TTAGTTCAGCACACAG-1\n896 L6.TGCGTGGTCAATCACG-1\n897 L6.TCAGGATAGATGCGAC-1\n898 L6.TAAACCGGTCTTGATG-1\n899 L6.GTGTTAGCACACGCTG-1\n900 L6.GGCTGGTGTCCCTTGT-1\n901 L6.GGAATAATCCGTACAA-1\n902 L6.GGAACTTAGATCCCAT-1\n903 L6.GCTCTGTTCAGGCCCA-1\n904 L6.GCATGTATCAGCTCGG-1\n905 L6.GCATGTAGTATAGGGC-1\n906 L6.GCACATAAGTGTTGAA-1\n907 L6.CTGGTCTTCTTTCCTC-1\n908 L6.CTAGCCTTCGTTACGA-1\n909 L6.CGTCACTTCTCTTATG-1\n910 L6.CGGTTAAGTATGAATG-1\n911 L6.CGATTGATCTGCTGCT-1\n912 L6.CGATGTACAGTTAACC-1\n913 L6.ATAGACCGTCGTTGTA-1\n914 L6.ATAAGAGAGCGAGAAA-1\n915 L6.AGCGTCGCAAATCCGT-1\n916 L6.AGATTGCTCCGAACGC-1\n917 L6.AGATTGCCAAGCCCAC-1\n918 L6.AGATTGCAGGCCCGTT-1\n919 L6.ACTATCTCATCACAAC-1\n920 L6.ACATACGTCTCAACTT-1\n921 L6.AAGCCGCGTCGTCTTC-1\n922 L6.AACCGCGCAGCCTATA-1\n923 2|2|2|2|2|20|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n924 L6.TACCTATTCGGCCGAT-1\n925 L6.CTCCTAGCACGGCGTT-1\n926 L6.CCTTCGATCTCACATT-1\n927 L6.CTCGTCACAAAGCGGT-1\n928 2|2|2|2|2|5|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n929 L6.GTGTTAGTCATCGATG-1\n930 L6.AGGTCATGTGTGCGTC-1\n931 2|2|2|2|2|6|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n932 L6.GAACATCCAGCGATCC-1\n933 L6.CAAGATCGTCTGCCAG-1\n934 L6.GGCTCGACACGAAACG-1\n935 2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|3|5|0|-|-|-\n936 L6.CGATCGGCATGAACCT-1\n937 L6.AGCTCTCCATCACCCT-1\n938 3|2|2|2|4|5|4|5|2|2|2|5|2|4|5|4|2|2|2|5|4|-|-|3|6|5|2|2|2\n939 L6.GGACAGACAGCCACCA-1\n940 3|2|2|2|4|5|4|9|2|2|2|5|10|4|25|4|2|11|2|5|4|-|-|3|6|5|2|2|2\n941 L6.AGGGATGTCGTTGACA-1\n942 2|2|2|2|7|13|7|10|2|2|2|3|3|4|10|8|2|2|7|5|4|3|0|3|13|11|2|2|2\n943 L6.CAGCATATCATGCTCC-1\n944 2|2|2|2|7|0|7|10|2|2|2|3|3|4|10|8|2|2|7|5|4|-|-|3|13|11|-|-|-\n945 L6.GTCACAAGTATAGTAG-1\n946 L6.GCGGGTTGTCTAGCCG-1\n947 L6.GCGAGAAGTGTGGCTC-1\n948 L6.CTGAAACCAGGACGTA-1\n949 L6.CTTGGCTTCTCTGTCG-1\n950 L6.CTGCTGTGTACCGTTA-1\n951 L6.AAGGAGCGTGTTTGGT-1\n952 L6.TGCTACCAGGGATACC-1\n953 L6.CGTGTAACATTGGTAC-1\n954 L6.GTCTCGTAGCCTTGAT-1\n955 L6.CCTACACAGTCTCGGC-1\n956 L6.TCAGGTAAGGCGTACA-1\n957 L6.TAAGTGCCAAGTTAAG-1\n958 L6.GTCCTCATCTGGTATG-1\n959 L6.GGACAAGTCTCATTCA-1\n960 L6.GACCTGGCAATCTACG-1\n961 L6.CTTTGCGCAAATACAG-1\n962 L6.CTGTGCTAGGCAAAGA-1\n963 L6.CCTCAGTGTCACACGC-1\n964 L6.CATATGGGTGTGCGTC-1\n965 L6.ATCTACTAGTATCGAA-1\n966 L6.AGATCTGAGGGCTTGA-1\n967 L6.AAATGCCCACATGTGT-1\n968 2|2|2|2|2|18|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n969 L6.TGCACCTGTGTCGCTG-1\n970 L6.CGGAGCTGTGTGACCC-1\n971 L6.ATCATCTCACCGAAAG-1\n972 L6.TTCTCAAAGGTGCAAC-1\n973 L6.TGGGCGTAGGGTGTGT-1\n974 L6.TACCTATGTGAAGGCT-1\n975 L6.CATCGAAAGTGGAGAA-1\n976 L6.AAGGAGCAGGGCTCTC-1\n977 L6.GTCCTCAGTCCATGAT-1\n978 L6.GCATGATGTATGCTTG-1\n979 2|2|2|-|-|-|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n980 L6.TGAGCATGTGCATCTA-1\n981 L6.CTGAAGTAGAGATGAG-1\n982 L6.CTCGTACGTGTGGTTT-1\n983 L6.CGTCCATTCGGAAACG-1\n984 L6.AGCGGTCTCGCCTGAG-1\n985 L6.AACTCAGTCCTAGAAC-1\n986 2|2|2|2|2|5|2|2|2|-|-|-|2|2|2|2|2|2|2|2|2|2|2|-|-|-|-|-|-\n987 L6.TTCTCCTCACCTCGGA-1\n988 L6.TTCGAAGAGCTCCTCT-1\n989 L6.CGATCGGAGTATCGAA-1\n990 L6.ATCGAGTTCAATACCG-1\n991 L6.ACGTCAAAGTGGAGAA-1\n992 L6.TTGGAACGTGTGCCTG-1\n993 L6.TTGCGTCCACACCGAC-1\n994 L6.TGGTTAGCACATGGGA-1\n995 L6.TGAAAGATCAGTTTGG-1\n996 L6.TATTACCCAAGCGAGT-1\n997 L6.AAGCCGCCACATTAGC-1\n998 L6.TTTATGCTCGCCGTGA-1\n999 L6.TTATGCTCAAGGACTG-1\n1000 L6.TGGCGCAGTACATGTC-1\n1001 L6.TGGCCAGGTTGAACTC-1\n1002 L6.TGCTGCTTCGTCTGAA-1\n1003 L6.TGCCCTACAAAGAATC-1\n1004 L6.TCGGTAACAGATGGGT-1\n1005 L6.TAGTTGGTCAAGATCC-1\n1006 L6.TACTCGCAGACTTGAA-1\n1007 L6.GTACTCCGTACATCCA-1\n1008 L6.GGTGAAGAGAAGCCCA-1\n1009 L6.GGCCGATAGGAGTTGC-1\n1010 L6.GGCAATTTCTGCGACG-1\n1011 L6.GCTGCGAGTCTAGCGC-1\n1012 L6.GCGCAGTAGTGATCGG-1\n1013 L6.GCGAGAACATCACCCT-1\n1014 L6.GCCTCTAGTTATCACG-1\n1015 L6.GCAGTTAAGTCCGGTC-1\n1016 L6.GATGCTAGTTGGGACA-1\n1017 L6.GATGAAACATCGACGC-1\n1018 L6.CTGCTGTCAATGGAGC-1\n1019 L6.CTGCTGTAGCCCGAAA-1\n1020 L6.CGGGTCACAAGGTTCT-1\n1021 L6.CGATGTACATGATCCA-1\n1022 L6.CGACTTCTCTTAGAGC-1\n1023 L6.CACCACTTCCAAACAC-1\n1024 L6.CAAGTTGTCCTATTCA-1\n1025 L6.ATGGGAGGTTAAAGTG-1\n1026 L6.AGGCCGTGTCCCTACT-1\n1027 L6.AGAGTGGGTCTCGTTC-1\n1028 L6.AGAGTGGGTAGGCTGA-1\n1029 L6.ACTGATGGTGACCAAG-1\n1030 L6.ACACCCTGTGTGTGCC-1\n1031 L6.AAGTCTGTCTCTAGGA-1\n1032 L6.AACTGGTCAGATCGGA-1\n1033 L6.AACTCTTAGCGTTCCG-1\n1034 L6.TGGTTCCTCAACACTG-1\n1035 L6.TGAGAGGGTAGCTAAA-1\n1036 L6.GGGAGATAGCAGCCTC-1\n1037 L6.CGTTCTGCACAGAGGT-1\n1038 L6.CCTTCGAGTCTGATCA-1\n1039 L6.CCATTCGCAGGAATGC-1\n1040 L6.AGCAGCCTCGTACGGC-1\n1041 L6.ATGAGGGCATCAGTCA-1\n1042 L6.GTGGGTCTCTCAACTT-1\n1043 L6.CCACTACCAGTAGAGC-1\n1044 L6.TGTGTTTTCGGCATCG-1\n1045 L6.GATCGTAGTATCACCA-1\n1046 L6.TGAGAGGTCGTTGACA-1\n1047 L6.CACACAATCACCGTAA-1\n1048 L6.ACGTCAAGTATCACCA-1\n1049 L6.TCTCATAGTTTCCACC-1\n1050 L6.TGAGAGGCACACAGAG-1\n1051 L6.TAGTTGGTCTGGTATG-1\n1052 L6.TAGTTGGGTTCTGGTA-1\n1053 L6.GTTACAGTCCTCTAGC-1\n1054 L6.GGGTTGCTCGATAGAA-1\n1055 L6.GCTGCGACACAGCCCA-1\n1056 L6.GATTCAGAGGCCCTTG-1\n1057 L6.GACGTTAAGAGGGATA-1\n1058 L6.CTACCCACATCTACGA-1\n1059 L6.CTACACCTCAGATAAG-1\n1060 L6.CGTGTAAGTAGCGTCC-1\n1061 L6.CCTCAGTAGTGCCATT-1\n1062 L6.CCCAGTTAGACCTTTG-1\n1063 L6.CATTATCCACGAAGCA-1\n1064 L6.CACACAAAGTGGGTTG-1\n1065 L6.AGAGTGGTCTATCGCC-1\n1066 L6.ACGCCAGTCACCTCGT-1\n1067 L6.ACGATGTAGATGCGAC-1\n1068 L6.ACAGCTATCGAATGGG-1\n1069 L6.AAGCCGCGTCTGATTG-1\n1070 L6.TCTTTCCAGTCGTACT-1\n1071 L6.CAACCAACAAGAAGAG-1\n1072 L6.ATAGACCCAGGGTATG-1\n1073 L6.TGGTTCCAGCCGGTAA-1\n1074 L6.GGTGAAGAGATACACA-1\n1075 L6.GCACATACATCCCATC-1\n1076 L6.CTGTGCTTCATTGCGA-1\n1077 L6.CTAGCCTCATCAGTCA-1\n1078 L6.CAGAGAGAGAAGCCCA-1\n1079 L6.ACTGAGTGTAATCACC-1\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(tree.traverse()):\n",
    "    print(i, n.name)\n",
    "\n",
    "split_node = tree.search_nodes(name='2|2|2|2|2|0|2|2|2|0|2|2|0|2|0|2|2|2|0|2|2|2|2|0|0|0|0|0|0')[0]\n",
    "sub_leaves = [n.name for n in split_node.get_leaves()]\n",
    "tree.prune(sub_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.poisson_glm import Poisson_GLM\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from models import *;\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from inference.inference import UnsupervisedTrainer\n",
    "from scvi.inference import posterior\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.metrics import ks_pvalue, accuracy_imputation, correlations, knn_purity, knn_purity_stratified\n",
    "from external.utils.plots_util import plot_histograms, plot_scatter_mean, plot_ecdf_ks, plot_density\n",
    "from external.utils.plots_util import plot_losses, plot_elbo, plot_common_ancestor, plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import avg_weighted_baseline, scvi_baseline, scvi_baseline_z, cascvi_baseline_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulations (Poisson GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "g = 100\n",
    "vis = False\n",
    "leaves_only = False\n",
    "var = 1.0\n",
    "\n",
    "glm = Poisson_GLM(tree, g, d, vis, leaves_only, var)\n",
    "\n",
    "glm.simulate_latent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate gene expression count data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(474, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "glm.simulate_ge()\n",
    "# Quality Control (i.e Gene Filtering)\n",
    "glm.gene_qc()\n",
    "glm.X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GLM parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((100, 5), (100,))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "glm.W.shape, glm.beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Binomial thinning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts: 0.4418565400843882\nProportion of dropouts after Binomial thinning: 0.9072995780590717\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts: {}\".format(np.mean(glm.X == 0)))\n",
    "glm.binomial_thinning(p=0.01)\n",
    "print(\"Proportion of dropouts after Binomial thinning: {}\".format(np.mean(glm.X == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data and the indexes at the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((402, 100), (402, 100), (72, 100), (72, 100), (402, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Latent vectors\n",
    "leaves_z, _, _ = get_leaves(glm.z, glm.mu, tree)\n",
    "\n",
    "#FIXED training set\n",
    "leaves_X, leaves_idx, mu = get_leaves(glm.X, glm.mu, tree)\n",
    "\n",
    "# internal nodes data (for imputation)\n",
    "internal_X, internal_idx, internal_mu = get_internal(glm.X, glm.mu, tree)\n",
    "\n",
    "leaves_X.shape, mu.shape, internal_X.shape, internal_mu.shape, leaves_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualizing the data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_one_gene(tree=tree,\n",
    "#             X=glm.X,\n",
    "#             g=0,\n",
    "#             node_sizes=[800 for i in range(glm.X.shape[0])],\n",
    "#             var='latent',\n",
    "#             size=1000,\n",
    "#             show_index=True,\n",
    "#             save_fig=True,\n",
    "#             figsize=(180, 100)\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Simulated latent space***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_common_ancestor(tree,\n",
    "                     glm.z,\n",
    "                     embedding='umap',\n",
    "                     give_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# anndata + gene and celle filtering\n",
    "adata = AnnData(leaves_X)\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]\n",
    "adata.obs_names = [n.name for n in leaves]\n",
    "#sc.pp.filter_genes(adata, min_counts=3)\n",
    "#sc.pp.filter_cells(adata, min_counts=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go\n",
      "[2021-05-02 23:19:37,121] WARNING - scvi.dataset.dataset | This dataset has some empty cells, this might fail scVI inference.Data should be filtered with `my_dataset.filter_cells_by_count()\n",
      "[2021-05-02 23:19:37,123] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-02 23:19:37,124] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n"
     ]
    }
   ],
   "source": [
    "scvi_dataset = AnnDatasetFromAnnData(adata, filtering=False)\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', adata.obs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2021-05-02 23:19:53,953] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2021-05-02 23:19:53,954] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2021-05-02 23:19:53,956] INFO - scvi.dataset.dataset | Keeping 100 genes\n",
      "[2021-05-02 23:19:53,958] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2021-05-02 23:19:53,960] WARNING - scvi.dataset.dataset | This dataset has some empty cells, this might fail scVI inference.Data should be filtered with `my_dataset.filter_cells_by_count()\n",
      "[2021-05-02 23:19:53,961] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-02 23:19:53,962] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2021-05-02 23:19:53,965] WARNING - scvi.dataset.dataset | This dataset has some empty cells, this might fail scVI inference.Data should be filtered with `my_dataset.filter_cells_by_count()\n",
      "[2021-05-02 23:19:53,966] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-02 23:19:53,967] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2021-05-02 23:19:53,970] WARNING - scvi.dataset.dataset | This dataset has some empty cells, this might fail scVI inference.Data should be filtered with `my_dataset.filter_cells_by_count()\n"
     ]
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "tree_bis = copy.deepcopy(tree)\n",
    "cas_dataset = TreeDataset(scvi_dataset, tree=tree_bis, filtering=False)\n",
    "cas_dataset\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = False\n",
    "use_MP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = var,\n",
    "              ldvae = True,\n",
    "              use_MP=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearDecoder(\n",
       "  (factor_regressor): FCLayers(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (0): Linear(in_features=5, out_features=100, bias=True)\n",
       "        (1): None\n",
       "        (2): None\n",
       "        (3): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_weight = torch.from_numpy(glm.W).float()\n",
    "new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    treevae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "    treevae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "    \n",
    "for param in treevae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "treevae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(treevae.decoder.factor_regressor.fc_layers[0][0].weight.numpy().all() == glm.W.T.all())\n",
    "assert(treevae.decoder.factor_regressor.fc_layers[0][0].bias.numpy().all() == glm.beta.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Are we able to generate the gene expression data by decoding the simulated latent space?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_scale, px_rate, raw_px_scale = treevae.decoder(treevae.dispersion,\n",
    "                                        torch.from_numpy(leaves_z).float(),\n",
    "                                        torch.from_numpy(np.array([np.log(10000)])).float()\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance between the Poisson and the NB means is 1.1905326647433946e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "foo = np.clip(a=np.exp(raw_px_scale.detach().cpu().numpy()),\n",
    "              a_min=0,\n",
    "              a_max=5000\n",
    ")\n",
    "mse = mean_squared_error(mu, foo)\n",
    "print(\"the distance between the Poisson and the NB means is {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "lr = 1e-3\n",
    "lambda_ = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_leaves:  [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401]]\ntest_leaves:  []\nvalidation leaves:  []\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "996459227151\n",
      "Varitional Likelihood: 1.1591878103379587\n",
      "ELBO Loss: 135.14704753138102\n",
      "training:  77%|███████▋  | 307/400 [05:52<01:38,  1.06s/it]Reconstruction Loss: 128.1396047108209\n",
      "Encodings MP Likelihood: 5.668449224874802\n",
      "Varitional Likelihood: 1.2635177688219061\n",
      "ELBO Loss: 135.0715717045176\n",
      "training:  77%|███████▋  | 308/400 [05:53<01:37,  1.06s/it]Reconstruction Loss: 127.95967428482587\n",
      "Encodings MP Likelihood: 5.67236822908187\n",
      "Varitional Likelihood: 1.3484676228233832\n",
      "ELBO Loss: 134.9805101367311\n",
      "training:  77%|███████▋  | 309/400 [05:54<01:35,  1.05s/it]Reconstruction Loss: 128.01919115360695\n",
      "Encodings MP Likelihood: 5.691763664636124\n",
      "Varitional Likelihood: 1.1286088174848414\n",
      "ELBO Loss: 134.83956363572793\n",
      "training:  78%|███████▊  | 310/400 [05:55<01:43,  1.15s/it]Reconstruction Loss: 127.8945992692786\n",
      "Encodings MP Likelihood: 5.6656756097959375\n",
      "Varitional Likelihood: 1.1863814776216572\n",
      "ELBO Loss: 134.7466563566962\n",
      "training:  78%|███████▊  | 311/400 [05:57<01:49,  1.24s/it]Reconstruction Loss: 127.74245957711443\n",
      "Encodings MP Likelihood: 5.7004176227562615\n",
      "Varitional Likelihood: 1.2485729616079757\n",
      "ELBO Loss: 134.69145016147868\n",
      "training:  78%|███████▊  | 312/400 [05:58<01:53,  1.29s/it]Reconstruction Loss: 127.76113572761194\n",
      "Encodings MP Likelihood: 5.672217234691644\n",
      "Varitional Likelihood: 1.2357681805814678\n",
      "ELBO Loss: 134.66912114288505\n",
      "training:  78%|███████▊  | 313/400 [05:59<01:51,  1.28s/it]Reconstruction Loss: 127.85389458955224\n",
      "Encodings MP Likelihood: 5.673445093317819\n",
      "Varitional Likelihood: 1.206231340247007\n",
      "ELBO Loss: 134.73357102311707\n",
      "training:  78%|███████▊  | 314/400 [06:00<01:45,  1.23s/it]Reconstruction Loss: 127.9501710199005\n",
      "Encodings MP Likelihood: 5.681257007573668\n",
      "Varitional Likelihood: 1.1522262345499068\n",
      "ELBO Loss: 134.78365426202407\n",
      "training:  79%|███████▉  | 315/400 [06:02<01:48,  1.28s/it]Reconstruction Loss: 127.33323616293532\n",
      "Encodings MP Likelihood: 5.680884389568052\n",
      "Varitional Likelihood: 1.376512062490283\n",
      "ELBO Loss: 134.39063261499365\n",
      "training:  79%|███████▉  | 316/400 [06:03<01:46,  1.27s/it]Reconstruction Loss: 127.52266013681592\n",
      "Encodings MP Likelihood: 5.689811547988189\n",
      "Varitional Likelihood: 1.4151392694729477\n",
      "ELBO Loss: 134.62761095427706\n",
      "training:  79%|███████▉  | 317/400 [06:04<01:49,  1.32s/it]Reconstruction Loss: 127.49605488184079\n",
      "Encodings MP Likelihood: 5.683091980317546\n",
      "Varitional Likelihood: 1.352379642315765\n",
      "ELBO Loss: 134.5315265044741\n",
      "training:  80%|███████▉  | 318/400 [06:06<01:49,  1.33s/it]Reconstruction Loss: 127.56420048196517\n",
      "Encodings MP Likelihood: 5.696972451555302\n",
      "Varitional Likelihood: 1.2884931421991606\n",
      "ELBO Loss: 134.54966607571964\n",
      "training:  80%|███████▉  | 319/400 [06:07<01:49,  1.36s/it]Reconstruction Loss: 127.339746579602\n",
      "Encodings MP Likelihood: 5.682314766446274\n",
      "Varitional Likelihood: 1.381204937227923\n",
      "ELBO Loss: 134.4032662832762\n",
      "training:  80%|████████  | 320/400 [06:09<01:49,  1.37s/it]Reconstruction Loss: 127.54985813121891\n",
      "Encodings MP Likelihood: 5.700602743307809\n",
      "Varitional Likelihood: 1.2461405132540422\n",
      "ELBO Loss: 134.49660138778077\n",
      "training:  80%|████████  | 321/400 [06:10<01:48,  1.38s/it]Reconstruction Loss: 127.96444535136816\n",
      "Encodings MP Likelihood: 5.690220270127139\n",
      "Varitional Likelihood: 1.3012452386504976\n",
      "ELBO Loss: 134.9559108601458\n",
      "training:  80%|████████  | 322/400 [06:11<01:48,  1.39s/it]Reconstruction Loss: 127.9686722636816\n",
      "Encodings MP Likelihood: 5.691427417617589\n",
      "Varitional Likelihood: 1.1870994757657027\n",
      "ELBO Loss: 134.8471991570649\n",
      "training:  81%|████████  | 323/400 [06:13<01:46,  1.38s/it]Reconstruction Loss: 127.52499222636816\n",
      "Encodings MP Likelihood: 5.666536731023206\n",
      "Varitional Likelihood: 1.4847488023748445\n",
      "ELBO Loss: 134.6762777597662\n",
      "training:  81%|████████  | 324/400 [06:14<01:38,  1.29s/it]Reconstruction Loss: 127.7510688743781\n",
      "Encodings MP Likelihood: 5.6744526078187985\n",
      "Varitional Likelihood: 1.2712166250048584\n",
      "ELBO Loss: 134.69673810720178\n",
      "training:  81%|████████▏ | 325/400 [06:15<01:32,  1.23s/it]Reconstruction Loss: 127.7617381840796\n",
      "Encodings MP Likelihood: 5.693035392264395\n",
      "Varitional Likelihood: 1.2092789227689678\n",
      "ELBO Loss: 134.66405249911296\n",
      "training:  82%|████████▏ | 326/400 [06:16<01:26,  1.17s/it]Reconstruction Loss: 127.84850163246269\n",
      "Encodings MP Likelihood: 5.69028711779475\n",
      "Varitional Likelihood: 1.2949768370063743\n",
      "ELBO Loss: 134.8337655872638\n",
      "training:  82%|████████▏ | 327/400 [06:17<01:21,  1.12s/it]Reconstruction Loss: 127.6963813743781\n",
      "Encodings MP Likelihood: 5.667684652379218\n",
      "Varitional Likelihood: 1.3213475165675528\n",
      "ELBO Loss: 134.68541354332487\n",
      "training:  82%|████████▏ | 328/400 [06:18<01:18,  1.09s/it]Reconstruction Loss: 127.77398165422886\n",
      "Encodings MP Likelihood: 5.669326423210185\n",
      "Varitional Likelihood: 1.3103364403568096\n",
      "ELBO Loss: 134.75364451779583\n",
      "training:  82%|████████▏ | 329/400 [06:19<01:15,  1.07s/it]Reconstruction Loss: 127.35576997823384\n",
      "Encodings MP Likelihood: 5.672712256559469\n",
      "Varitional Likelihood: 1.4595787845440764\n",
      "ELBO Loss: 134.48806101933738\n",
      "training:  82%|████████▎ | 330/400 [06:20<01:13,  1.04s/it]Reconstruction Loss: 127.59315726057214\n",
      "Encodings MP Likelihood: 5.682022935109526\n",
      "Varitional Likelihood: 1.3633192071867226\n",
      "ELBO Loss: 134.6384994028684\n",
      "training:  83%|████████▎ | 331/400 [06:21<01:16,  1.10s/it]Reconstruction Loss: 127.33998950559702\n",
      "Encodings MP Likelihood: 5.6784481999623075\n",
      "Varitional Likelihood: 1.3208914230118936\n",
      "ELBO Loss: 134.33932912857122\n",
      "training:  83%|████████▎ | 332/400 [06:23<01:21,  1.19s/it]Reconstruction Loss: 128.06875777363183\n",
      "Encodings MP Likelihood: 5.655402423393478\n",
      "Varitional Likelihood: 1.3207795252254353\n",
      "ELBO Loss: 135.04493972225075\n",
      "training:  83%|████████▎ | 333/400 [06:24<01:24,  1.26s/it]Reconstruction Loss: 127.18145600124379\n",
      "Encodings MP Likelihood: 5.691081538995892\n",
      "Varitional Likelihood: 1.318045696808924\n",
      "ELBO Loss: 134.1905832370486\n",
      "training:  84%|████████▎ | 334/400 [06:25<01:26,  1.31s/it]Reconstruction Loss: 127.66228428171642\n",
      "Encodings MP Likelihood: 5.673392697985724\n",
      "Varitional Likelihood: 1.435359670155084\n",
      "ELBO Loss: 134.77103664985722\n",
      "training:  84%|████████▍ | 335/400 [06:27<01:26,  1.34s/it]Reconstruction Loss: 127.51828746890547\n",
      "Encodings MP Likelihood: 5.682894559234583\n",
      "Varitional Likelihood: 1.3675568993411846\n",
      "ELBO Loss: 134.56873892748123\n",
      "training:  84%|████████▍ | 336/400 [06:28<01:26,  1.35s/it]Reconstruction Loss: 127.4520755597015\n",
      "Encodings MP Likelihood: 5.678805775925566\n",
      "Varitional Likelihood: 1.2682616580185013\n",
      "ELBO Loss: 134.39914299364554\n",
      "training:  84%|████████▍ | 337/400 [06:30<01:23,  1.33s/it]Reconstruction Loss: 127.74951414800995\n",
      "Encodings MP Likelihood: 5.674861768973369\n",
      "Varitional Likelihood: 1.3824913822003264\n",
      "ELBO Loss: 134.80686729918364\n",
      "training:  84%|████████▍ | 338/400 [06:31<01:21,  1.31s/it]Reconstruction Loss: 127.49788168532338\n",
      "Encodings MP Likelihood: 5.666664886969867\n",
      "Varitional Likelihood: 1.3210139488106343\n",
      "ELBO Loss: 134.48556052110388\n",
      "training:  85%|████████▍ | 339/400 [06:32<01:21,  1.34s/it]Reconstruction Loss: 127.77866526741293\n",
      "Encodings MP Likelihood: 5.670238445679801\n",
      "Varitional Likelihood: 1.3702879948402518\n",
      "ELBO Loss: 134.819191707933\n",
      "training:  85%|████████▌ | 340/400 [06:33<01:17,  1.29s/it]Reconstruction Loss: 127.64388992537313\n",
      "Encodings MP Likelihood: 5.68374336063514\n",
      "Varitional Likelihood: 1.3524948803346548\n",
      "ELBO Loss: 134.68012816634294\n",
      "training:  85%|████████▌ | 341/400 [06:34<01:10,  1.20s/it]Reconstruction Loss: 127.70306669776119\n",
      "Encodings MP Likelihood: 5.677967225973859\n",
      "Varitional Likelihood: 1.298808539091651\n",
      "ELBO Loss: 134.6798424628267\n",
      "training:  86%|████████▌ | 342/400 [06:35<01:07,  1.16s/it]Reconstruction Loss: 127.6914159670398\n",
      "Encodings MP Likelihood: 5.6792964985331436\n",
      "Varitional Likelihood: 1.2555851722831157\n",
      "ELBO Loss: 134.62629763785606\n",
      "training:  86%|████████▌ | 343/400 [06:36<01:04,  1.13s/it]Reconstruction Loss: 127.25441153606965\n",
      "Encodings MP Likelihood: 5.660734950035279\n",
      "Varitional Likelihood: 1.4561432036594373\n",
      "ELBO Loss: 134.37128968976438\n",
      "training:  86%|████████▌ | 344/400 [06:38<01:02,  1.11s/it]Reconstruction Loss: 127.55172380286069\n",
      "Encodings MP Likelihood: 5.676586831065444\n",
      "Varitional Likelihood: 1.4343264755324938\n",
      "ELBO Loss: 134.66263710945864\n",
      "training:  86%|████████▋ | 345/400 [06:39<01:03,  1.15s/it]Reconstruction Loss: 127.52666355721394\n",
      "Encodings MP Likelihood: 5.666962686479758\n",
      "Varitional Likelihood: 1.4740019081836908\n",
      "ELBO Loss: 134.66762815187738\n",
      "training:  86%|████████▋ | 346/400 [06:40<01:06,  1.23s/it]Reconstruction Loss: 127.47928327114428\n",
      "Encodings MP Likelihood: 5.687121966167438\n",
      "Varitional Likelihood: 1.3689497762651586\n",
      "ELBO Loss: 134.5353550135769\n",
      "training:  87%|████████▋ | 347/400 [06:42<01:08,  1.29s/it]Reconstruction Loss: 127.7557719216418\n",
      "Encodings MP Likelihood: 5.681741222582992\n",
      "Varitional Likelihood: 1.3356286803288246\n",
      "ELBO Loss: 134.7731418245536\n",
      "training:  87%|████████▋ | 348/400 [06:43<01:08,  1.32s/it]Reconstruction Loss: 127.68601329291045\n",
      "Encodings MP Likelihood: 5.678918460841334\n",
      "Varitional Likelihood: 1.4899557692494558\n",
      "ELBO Loss: 134.85488752300122\n",
      "training:  87%|████████▋ | 349/400 [06:44<01:07,  1.33s/it]Reconstruction Loss: 127.6171680659204\n",
      "Encodings MP Likelihood: 5.662706909507136\n",
      "Varitional Likelihood: 1.3775879209907493\n",
      "ELBO Loss: 134.65746289641828\n",
      "training:  88%|████████▊ | 350/400 [06:45<01:02,  1.25s/it]Reconstruction Loss: 127.42779267723881\n",
      "Encodings MP Likelihood: 5.682531346062448\n",
      "Varitional Likelihood: 1.413335695788635\n",
      "ELBO Loss: 134.5236597190899\n",
      "training:  88%|████████▊ | 351/400 [06:47<00:58,  1.19s/it]Reconstruction Loss: 127.31855371579601\n",
      "Encodings MP Likelihood: 5.678684795794833\n",
      "Varitional Likelihood: 1.4728639517257462\n",
      "ELBO Loss: 134.4701024633166\n",
      "training:  88%|████████▊ | 352/400 [06:48<00:58,  1.22s/it]Reconstruction Loss: 127.43116449004975\n",
      "Encodings MP Likelihood: 5.658645948156172\n",
      "Varitional Likelihood: 1.3233590956351058\n",
      "ELBO Loss: 134.41316953384103\n",
      "training:  88%|████████▊ | 353/400 [06:49<00:53,  1.14s/it]Reconstruction Loss: 127.29307563743781\n",
      "Encodings MP Likelihood: 5.658758977478094\n",
      "Varitional Likelihood: 1.5890761038557213\n",
      "ELBO Loss: 134.5409107187716\n",
      "training:  88%|████████▊ | 354/400 [06:50<00:49,  1.09s/it]Reconstruction Loss: 127.49138098569652\n",
      "Encodings MP Likelihood: 5.667894626728767\n",
      "Varitional Likelihood: 1.4661874344099814\n",
      "ELBO Loss: 134.62546304683528\n",
      "training:  89%|████████▉ | 355/400 [06:51<00:49,  1.11s/it]Reconstruction Loss: 127.7412255130597\n",
      "Encodings MP Likelihood: 5.66989339171956\n",
      "Varitional Likelihood: 1.3072889337492226\n",
      "ELBO Loss: 134.71840783852846\n",
      "training:  89%|████████▉ | 356/400 [06:52<00:47,  1.08s/it]Reconstruction Loss: 127.38338580534825\n",
      "Encodings MP Likelihood: 5.672443171900735\n",
      "Varitional Likelihood: 1.4722145801752955\n",
      "ELBO Loss: 134.5280435574243\n",
      "training:  89%|████████▉ | 357/400 [06:53<00:47,  1.11s/it]Reconstruction Loss: 127.14735890858209\n",
      "Encodings MP Likelihood: 5.6634982685203425\n",
      "Varitional Likelihood: 1.5742977009483832\n",
      "ELBO Loss: 134.38515487805083\n",
      "training:  90%|████████▉ | 358/400 [06:54<00:50,  1.20s/it]Reconstruction Loss: 127.22111124067165\n",
      "Encodings MP Likelihood: 5.684671570927669\n",
      "Varitional Likelihood: 1.3897916120083178\n",
      "ELBO Loss: 134.29557442360763\n",
      "training:  90%|████████▉ | 359/400 [06:56<00:52,  1.27s/it]Reconstruction Loss: 127.09981343283582\n",
      "Encodings MP Likelihood: 5.674187143392836\n",
      "Varitional Likelihood: 1.553583401352612\n",
      "ELBO Loss: 134.32758397758127\n",
      "training:  90%|█████████ | 360/400 [06:57<00:52,  1.32s/it]Reconstruction Loss: 127.38221976057214\n",
      "Encodings MP Likelihood: 5.67452464685289\n",
      "Varitional Likelihood: 1.3095968825307058\n",
      "ELBO Loss: 134.36634128995573\n",
      "training:  90%|█████████ | 361/400 [06:59<00:52,  1.36s/it]Reconstruction Loss: 127.24065220771145\n",
      "Encodings MP Likelihood: 5.667688284109528\n",
      "Varitional Likelihood: 1.5435617930853545\n",
      "ELBO Loss: 134.45190228490634\n",
      "training:  90%|█████████ | 362/400 [07:00<00:49,  1.30s/it]Reconstruction Loss: 127.41642374067165\n",
      "Encodings MP Likelihood: 5.67539395433725\n",
      "Varitional Likelihood: 1.408995367401275\n",
      "ELBO Loss: 134.50081306241017\n",
      "training:  91%|█████████ | 363/400 [07:01<00:44,  1.20s/it]Reconstruction Loss: 127.07010844216418\n",
      "Encodings MP Likelihood: 5.66818758136727\n",
      "Varitional Likelihood: 1.4925508261913092\n",
      "ELBO Loss: 134.23084684972275\n",
      "training:  91%|█████████ | 364/400 [07:02<00:40,  1.13s/it]Reconstruction Loss: 127.55990555037313\n",
      "Encodings MP Likelihood: 5.658698433388716\n",
      "Varitional Likelihood: 1.4173205930795243\n",
      "ELBO Loss: 134.63592457684138\n",
      "training:  91%|█████████▏| 365/400 [07:03<00:37,  1.09s/it]Reconstruction Loss: 127.34065026430348\n",
      "Encodings MP Likelihood: 5.67202415643093\n",
      "Varitional Likelihood: 1.5169533497065455\n",
      "ELBO Loss: 134.52962777044095\n",
      "training:  92%|█████████▏| 366/400 [07:04<00:38,  1.14s/it]Reconstruction Loss: 127.44388409514926\n",
      "Encodings MP Likelihood: 5.675936862658561\n",
      "Varitional Likelihood: 1.5000447894803328\n",
      "ELBO Loss: 134.61986574728815\n",
      "training:  92%|█████████▏| 367/400 [07:06<00:40,  1.23s/it]Reconstruction Loss: 127.22526041666667\n",
      "Encodings MP Likelihood: 5.674123624153329\n",
      "Varitional Likelihood: 1.4667616507307213\n",
      "ELBO Loss: 134.3661456915507\n",
      "training:  92%|█████████▏| 368/400 [07:07<00:38,  1.22s/it]Reconstruction Loss: 127.48657105099502\n",
      "Encodings MP Likelihood: 5.676505383759783\n",
      "Varitional Likelihood: 1.4931608740963154\n",
      "ELBO Loss: 134.65623730885113\n",
      "training:  92%|█████████▏| 369/400 [07:08<00:36,  1.17s/it]Reconstruction Loss: 127.33094294154229\n",
      "Encodings MP Likelihood: 5.679043393929908\n",
      "Varitional Likelihood: 1.508317178754664\n",
      "ELBO Loss: 134.51830351422686\n",
      "training:  92%|█████████▎| 370/400 [07:09<00:35,  1.19s/it]Reconstruction Loss: 127.45904267723881\n",
      "Encodings MP Likelihood: 5.672588636272063\n",
      "Varitional Likelihood: 1.3150769893209733\n",
      "ELBO Loss: 134.44670830283184\n",
      "training:  93%|█████████▎| 371/400 [07:10<00:36,  1.26s/it]Reconstruction Loss: 127.48279112251244\n",
      "Encodings MP Likelihood: 5.680588678955697\n",
      "Varitional Likelihood: 1.384664507054571\n",
      "ELBO Loss: 134.54804430852272\n",
      "training:  93%|█████████▎| 372/400 [07:12<00:35,  1.28s/it]Reconstruction Loss: 126.92192358519901\n",
      "Encodings MP Likelihood: 5.663392007651306\n",
      "Varitional Likelihood: 1.5214087642840486\n",
      "ELBO Loss: 134.10672435713434\n",
      "training:  93%|█████████▎| 373/400 [07:13<00:32,  1.21s/it]Reconstruction Loss: 127.60661536069652\n",
      "Encodings MP Likelihood: 5.658307570096284\n",
      "Varitional Likelihood: 1.3961949894084267\n",
      "ELBO Loss: 134.66111792020124\n",
      "training:  94%|█████████▎| 374/400 [07:14<00:30,  1.16s/it]Reconstruction Loss: 127.03437888681592\n",
      "Encodings MP Likelihood: 5.6588377208169245\n",
      "Varitional Likelihood: 1.47219286866449\n",
      "ELBO Loss: 134.16540947629733\n",
      "training:  94%|█████████▍| 375/400 [07:15<00:28,  1.13s/it]Reconstruction Loss: 127.39885533271145\n",
      "Encodings MP Likelihood: 5.6550668162821\n",
      "Varitional Likelihood: 1.5977965397621268\n",
      "ELBO Loss: 134.65171868875566\n",
      "training:  94%|█████████▍| 376/400 [07:16<00:26,  1.11s/it]Reconstruction Loss: 127.54086986940298\n",
      "Encodings MP Likelihood: 5.666577721158865\n",
      "Varitional Likelihood: 1.6303305554745802\n",
      "ELBO Loss: 134.83777814603644\n",
      "training:  94%|█████████▍| 377/400 [07:17<00:25,  1.09s/it]Reconstruction Loss: 127.47125699626865\n",
      "Encodings MP Likelihood: 5.665652473329114\n",
      "Varitional Likelihood: 1.5768160843730565\n",
      "ELBO Loss: 134.71372555397085\n",
      "training:  94%|█████████▍| 378/400 [07:18<00:24,  1.12s/it]Reconstruction Loss: 126.91716223569652\n",
      "Encodings MP Likelihood: 5.668086442538363\n",
      "Varitional Likelihood: 1.5582090159553794\n",
      "ELBO Loss: 134.14345769419026\n",
      "training:  95%|█████████▍| 379/400 [07:19<00:23,  1.10s/it]Reconstruction Loss: 127.24640469527363\n",
      "Encodings MP Likelihood: 5.659871109786945\n",
      "Varitional Likelihood: 1.51934131223764\n",
      "ELBO Loss: 134.4256171172982\n",
      "training:  95%|█████████▌| 380/400 [07:20<00:21,  1.09s/it]Reconstruction Loss: 127.27060984141791\n",
      "Encodings MP Likelihood: 5.650108611472689\n",
      "Varitional Likelihood: 1.5985465737717661\n",
      "ELBO Loss: 134.51926502666237\n",
      "training:  95%|█████████▌| 381/400 [07:21<00:20,  1.07s/it]Reconstruction Loss: 127.7518753886816\n",
      "Encodings MP Likelihood: 5.675659398446634\n",
      "Varitional Likelihood: 1.4959030530939055\n",
      "ELBO Loss: 134.92343784022214\n",
      "training:  96%|█████████▌| 382/400 [07:23<00:19,  1.07s/it]Reconstruction Loss: 127.39068330223881\n",
      "Encodings MP Likelihood: 5.6847825614064895\n",
      "Varitional Likelihood: 1.5464198174168222\n",
      "ELBO Loss: 134.62188568106214\n",
      "training:  96%|█████████▌| 383/400 [07:24<00:18,  1.07s/it]Reconstruction Loss: 126.77604166666667\n",
      "Encodings MP Likelihood: 5.671149618966696\n",
      "Varitional Likelihood: 1.6400155594099814\n",
      "ELBO Loss: 134.08720684504334\n",
      "training:  96%|█████████▌| 384/400 [07:25<00:16,  1.04s/it]Reconstruction Loss: 127.53431086753731\n",
      "Encodings MP Likelihood: 5.655501856010432\n",
      "Varitional Likelihood: 1.4631801624203202\n",
      "ELBO Loss: 134.65299288596808\n",
      "training:  96%|█████████▋| 385/400 [07:25<00:15,  1.01s/it]Reconstruction Loss: 126.93134911380596\n",
      "Encodings MP Likelihood: 5.684416068498315\n",
      "Varitional Likelihood: 1.6888312344527363\n",
      "ELBO Loss: 134.30459641675702\n",
      "training:  96%|█████████▋| 386/400 [07:26<00:14,  1.01s/it]Reconstruction Loss: 127.60387515547264\n",
      "Encodings MP Likelihood: 5.671810676877551\n",
      "Varitional Likelihood: 1.495873142830768\n",
      "ELBO Loss: 134.77155897518094\n",
      "training:  97%|█████████▋| 387/400 [07:28<00:13,  1.06s/it]Reconstruction Loss: 127.68773320895522\n",
      "Encodings MP Likelihood: 5.6834637180956395\n",
      "Varitional Likelihood: 1.3998666639944808\n",
      "ELBO Loss: 134.77106359104533\n",
      "training:  97%|█████████▋| 388/400 [07:29<00:12,  1.07s/it]Reconstruction Loss: 127.92603389303483\n",
      "Encodings MP Likelihood: 5.68338103980412\n",
      "Varitional Likelihood: 1.2920504897388059\n",
      "ELBO Loss: 134.90146542257776\n",
      "training:  97%|█████████▋| 389/400 [07:30<00:11,  1.07s/it]Reconstruction Loss: 127.23416122512438\n",
      "Encodings MP Likelihood: 5.6674556847557085\n",
      "Varitional Likelihood: 1.4945041030200559\n",
      "ELBO Loss: 134.39612101290015\n",
      "training:  98%|█████████▊| 390/400 [07:31<00:10,  1.06s/it]Reconstruction Loss: 127.37937266791045\n",
      "Encodings MP Likelihood: 5.679912076472935\n",
      "Varitional Likelihood: 1.3608471315298507\n",
      "ELBO Loss: 134.42013187591323\n",
      "training:  98%|█████████▊| 391/400 [07:32<00:09,  1.06s/it]Reconstruction Loss: 127.29432913557214\n",
      "Encodings MP Likelihood: 5.670915985197231\n",
      "Varitional Likelihood: 1.5064021627701336\n",
      "ELBO Loss: 134.4716472835395\n",
      "training:  98%|█████████▊| 392/400 [07:33<00:08,  1.06s/it]Reconstruction Loss: 126.89712569962687\n",
      "Encodings MP Likelihood: 5.674647474079744\n",
      "Varitional Likelihood: 1.4970531558516014\n",
      "ELBO Loss: 134.0688263295582\n",
      "training:  98%|█████████▊| 393/400 [07:34<00:07,  1.06s/it]Reconstruction Loss: 127.26503226057214\n",
      "Encodings MP Likelihood: 5.671026480729494\n",
      "Varitional Likelihood: 1.6252933331389925\n",
      "ELBO Loss: 134.56135207444063\n",
      "training:  98%|█████████▊| 394/400 [07:35<00:06,  1.06s/it]Reconstruction Loss: 127.48409320584577\n",
      "Encodings MP Likelihood: 5.675497962726644\n",
      "Varitional Likelihood: 1.5229886942241915\n",
      "ELBO Loss: 134.6825798627966\n",
      "training:  99%|█████████▉| 395/400 [07:36<00:05,  1.07s/it]Reconstruction Loss: 126.8252876243781\n",
      "Encodings MP Likelihood: 5.671137752789201\n",
      "Varitional Likelihood: 1.571990966796875\n",
      "ELBO Loss: 134.0684163439642\n",
      "training:  99%|█████████▉| 396/400 [07:37<00:04,  1.07s/it]Reconstruction Loss: 127.71253109452736\n",
      "Encodings MP Likelihood: 5.673260485227177\n",
      "Varitional Likelihood: 1.4717888523690144\n",
      "ELBO Loss: 134.85758043212354\n",
      "training:  99%|█████████▉| 397/400 [07:38<00:03,  1.02s/it]Reconstruction Loss: 127.54505791355722\n",
      "Encodings MP Likelihood: 5.6741883772172885\n",
      "Varitional Likelihood: 1.5209402207711442\n",
      "ELBO Loss: 134.74018651154566\n",
      "training: 100%|█████████▉| 398/400 [07:39<00:02,  1.07s/it]Reconstruction Loss: 126.98660991915423\n",
      "Encodings MP Likelihood: 5.657956628762341\n",
      "Varitional Likelihood: 1.643385550275964\n",
      "ELBO Loss: 134.28795209819253\n",
      "training: 100%|█████████▉| 399/400 [07:41<00:01,  1.12s/it]Reconstruction Loss: 127.05520250310946\n",
      "Encodings MP Likelihood: 5.663101829586552\n",
      "Varitional Likelihood: 1.4912409995918843\n",
      "ELBO Loss: 134.2095453322879\n",
      "computing elbo\n",
      "training: 100%|██████████| 400/400 [07:42<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior and MV imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Missing Value imputation By Posterior Predictive sampling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_l = np.mean(np.sum(glm.X, axis=1))\n",
    "\n",
    "# CascVI impitations\n",
    "imputed = {}\n",
    "imputed_z = {}\n",
    "imputed_gt = {}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        imputed[n.name], imputed_z[n.name] = full_posterior.imputation_internal(n.name,\n",
    "                                                            give_mean=False,\n",
    "                                                            library_size=empirical_l\n",
    "                                                           )\n",
    "        imputed_gt[n.name] = glm.X[n.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_X = [x for x in imputed.values()]\n",
    "imputed_X = np.array(imputed_X).reshape(-1, cas_dataset.X.shape[1])\n",
    "#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 1 (Average of reconstructions)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed_cascvi_1 = scvi_baseline(tree, full_posterior, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 2 (Reconstruction of Averaged latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_cascvi_2, imputed_cascvi_2_z = scvi_baseline_z(tree=tree,\n",
    "                                   model=treevae,\n",
    "                                   posterior=full_posterior,\n",
    "                                   weighted=True,\n",
    "                                   n_samples_z=1,\n",
    "                                   library_size=empirical_l,\n",
    "                                   gaussian=False\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Baseline 1: Unweighted Average of gene expression in Clade***\n",
    "\n",
    "The simple idea here is to impute the value of an internal node, with the (un)weighted average of the gene expression values of the leaves, taking the query internal node as the root of the subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = True\n",
    "imputed_avg = avg_weighted_baseline(tree, True, glm.X, rounding=True)\n",
    "\n",
    "#get internal nodes\n",
    "avg_X = np.array([x for x in imputed_avg.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_avg_X, _, _ = get_internal(avg_X, glm.mu, tree)\n",
    "#plot_histograms(avg_X,\n",
    "#               'Histograms of imputed GE values with the average baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Baseline 2: (Un)weighted Average of decoded latent vectors, with scVI***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same averaging of the subtrees leaves in **Baseline 1**, only this time, the gene expression data is recovered with scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2021-05-02 23:59:01,561] WARNING - scvi.dataset.dataset | This dataset has some empty cells, this might fail scVI inference.Data should be filtered with `my_dataset.filter_cells_by_count()\n",
      "[2021-05-02 23:59:01,564] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-02 23:59:01,565] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n"
     ]
    }
   ],
   "source": [
    "# anndata\n",
    "gene_dataset = GeneExpressionDataset()\n",
    "gene_dataset.populate_from_data(leaves_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== We will use a linear decoder ===\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs = 400\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=128,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=glm.latent,\n",
    "                  ldvae=True\n",
    "              )\n",
    "\n",
    "\n",
    "new_weight = torch.from_numpy(glm.W).float()\n",
    "new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    vae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "    vae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "    \n",
    "for param in vae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 400/400 [00:16<00:00, 23.84it/s]\n",
      "Press Ctrl+C to stop WebAgg server\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-347810f1309e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train history scVI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/matplotlib/backends/backend_webagg.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To view figure, visit {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mWebAggApplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/matplotlib/backends/backend_webagg.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcatch_sigint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mioloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/tornado/platform/asyncio.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyncio_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyncio_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;34m\"\"\"Run until stop() is called.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_coroutine_origin_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ident\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_runnung\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=None)\n",
    "\n",
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 1 (Average of reconstructions)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior(model=vae,\n",
    "                                              gene_dataset=gene_dataset\n",
    "                                              )\n",
    "\n",
    "#imputed_scvi, imputed_scvi_z = scvi_baseline(tree=tree, \n",
    "#                                             posterior=scvi_posterior, \n",
    "#                                             weighted=True,\n",
    "#                                            give_latent=True,\n",
    "#                                            n_samples_z=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 2 (Decoded Average Latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "nb. categorical args provided doesn't match init. params.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-2d98dd0db84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0mknown_latent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                  \u001b[0mlatent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscvi_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                  \u001b[0mgaussian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                                 )\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cassiopeia_Transcriptome/scvi/external/utils/baselines.py\u001b[0m in \u001b[0;36mscvi_baseline_z\u001b[0;34m(tree, posterior, model, weighted, n_samples_z, library_size, gaussian, known_latent, latent)\u001b[0m\n\u001b[1;32m    184\u001b[0m                         px_scale, px_rate, raw_px_scale = posterior.model.decoder(dispersion=model.dispersion,\n\u001b[1;32m    185\u001b[0m                                                                                 \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                                                                                 \u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                                                                                 )\n\u001b[1;32m    188\u001b[0m                         \u001b[0mpx_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_px_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scvi-tools/scvi/models/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dispersion, z, library, *cat_list)\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# The decoder returns values for the parameters of the ZINB distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mraw_px_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactor_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcat_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mpx_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_px_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mpx_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpx_dropout_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcat_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scvi-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scvi-tools/scvi/models/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, instance_id, *cat_list)\u001b[0m\n\u001b[1;32m    112\u001b[0m         assert len(self.n_cat_list) <= len(\n\u001b[1;32m    113\u001b[0m             \u001b[0mcat_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         ), \"nb. categorical args provided doesn't match init. params.\"\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cat_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             assert not (\n",
      "\u001b[0;31mAssertionError\u001b[0m: nb. categorical args provided doesn't match init. params."
     ]
    }
   ],
   "source": [
    "library_size = np.mean(np.sum(glm.X, axis=1))\n",
    "scvi_latent = np.array([scvi_posterior.get_latent(give_mean=False)[0] for i in range(10)])\n",
    "\n",
    "imputed_scvi_2, imputed_scvi_2_z = scvi_baseline_z(tree=tree,\n",
    "                                 model=vae,\n",
    "                                 posterior=scvi_posterior,\n",
    "                                 weighted=True,\n",
    "                                 n_samples_z=1,\n",
    "                                 library_size=library_size,\n",
    "                                 known_latent=True,\n",
    "                                 latent=scvi_latent,\n",
    "                                 gaussian=False\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((402, 5), (402, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "cascvi_latent = full_posterior.get_latent()\n",
    "scvi_latent = scvi_posterior.get_latent()[0]\n",
    "\n",
    "scvi_latent.shape, cascvi_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of scVI encodings:  -2834.7010432824986\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(scvi_latent, cas_dataset.barcodes, scvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), scvi_latent.shape[1], False)\n",
    "mp_lik_scvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of scVI encodings: \", mp_lik_scvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  -13622.112278252629\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(cascvi_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood Ratio: tensor(-10787.4112, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood ratio\n",
    "lambda_ = (mp_lik_cascvi - mp_lik_scvi)\n",
    "print(\"Likelihood Ratio:\", lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Latent Space Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_common_ancestor(tree,\n",
    "                     cascvi_latent,\n",
    "                     embedding='umap',\n",
    "                     give_labels=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge internal nodes and leaves\n",
    "full_cascvi_latent = construct_latent(tree, cascvi_latent, imputed_z)\n",
    "\n",
    "\n",
    "print(\"CascVI latent space\")\n",
    "plot_common_ancestor(tree,\n",
    "                     full_cascvi_latent,\n",
    "                     embedding='umap',\n",
    "                     give_labels=False\n",
    "                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cascvi_latent_2 = construct_latent(tree, cascvi_latent, imputed_cascvi_2_z)\n",
    "\n",
    "print(\"CascVI + averaging latent space\")\n",
    "plot_common_ancestor(tree,\n",
    "                     full_cascvi_latent_2,\n",
    "                     embedding='umap',\n",
    "                     give_labels=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge internal nodes and leaves\n",
    "full_scvi_latent = construct_latent(tree, scvi_latent, imputed_scvi_2_z)\n",
    "\n",
    "print(\"scVI latent space\")\n",
    "plot_common_ancestor(tree,\n",
    "                 full_scvi_latent,\n",
    "                 embedding='umap',\n",
    "                 give_labels=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***k-NN purity***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaves only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Leaves Only\")\n",
    "scores = knn_purity(max_neighbors=50,\n",
    "              data=[leaves_z, scvi_latent, cascvi_latent],\n",
    "              plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal nodes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Internal nodes Only\")\n",
    "internal_z, internal_idx, internal_mu = get_internal(glm.z, glm.mu, tree)\n",
    "internal_scvi_z, _, _ = get_internal(full_scvi_latent, glm.mu, tree)\n",
    "internal_scvi_z_2, _, _ = get_internal(full_scvi_latent_2, glm.mu, tree)\n",
    "internal_cascvi_z, _, _ = get_internal(full_cascvi_latent, glm.mu, tree)\n",
    "internal_cascvi_z_2, _, _ = get_internal(full_cascvi_latent_2, glm.mu, tree)\n",
    "\n",
    "scores = knn_purity(max_neighbors=50,\n",
    "              data=[internal_z, internal_scvi_z, internal_scvi_z_2, internal_cascvi_z, internal_cascvi_z_2],\n",
    "              plot=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full tree\")\n",
    "scores = knn_purity(max_neighbors=50,\n",
    "              data=[glm.z, full_scvi_latent, full_scvi_latent_2, full_cascvi_latent, full_cascvi_latent_2],\n",
    "              plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stratified k-NN purity***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in [2, 5, 10, 20, 35, 50]:\n",
    "    print(\"For {} neighbors\".format(k))\n",
    "    if k == 10:\n",
    "        min_depth = 3\n",
    "    elif k == 20:\n",
    "        min_depth = 4\n",
    "    elif k == 35:\n",
    "        min_depth = 6\n",
    "    elif k == 50:\n",
    "        min_depth = 7\n",
    "    else:\n",
    "        min_depth = 2\n",
    "    scores = knn_purity_stratified(n_neighbors=k,\n",
    "                                   tree=tree,\n",
    "                                   data=[glm.z, full_scvi_latent, full_scvi_latent_2, full_cascvi_latent, full_cascvi_latent_2],\n",
    "                                   min_depth=min_depth,\n",
    "                                   plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Variance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get empirical variance of the encoder\n",
    "qz_v_norm = full_posterior.empirical_qz_v(\n",
    "                                    n_samples=1000,\n",
    "                                    norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Empirical Variance of observed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {}\n",
    "node_sizes = []\n",
    "\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    if not n.is_leaf():\n",
    "        # if n is an internal node, we get the variance from the prior\n",
    "        z = full_posterior.get_latent(give_mean=False)[0]\n",
    "        _, v_nu = vae.posterior_predictive_density(query_node=n.name, \n",
    "                                                  evidence=z) \n",
    "        \n",
    "        var_dict[n.name] = str(np.around(a=v_nu,\n",
    "                             decimals=4)\n",
    "                              )\n",
    "        node_sizes.append(np.around(a=v_nu,\n",
    "                             decimals=4)\n",
    "                         )\n",
    "    else:\n",
    "        idx = leaves_idx.index(n.index)\n",
    "        # if n is a leaf, we get the variance from the encoder\n",
    "        \n",
    "        # variational distribution variance\n",
    "        \n",
    "        # Empirical variance\n",
    "        var_dict[n.name] = str(np.around(a=qz_v_norm[idx],\n",
    "                                 decimals=4)\n",
    "                             )\n",
    "        node_sizes.append(np.around(a=qz_v_norm[idx],\n",
    "                                 decimals=4) \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot variance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_gene(tree=tree, \n",
    "              X=imputed_X, \n",
    "              g=0, \n",
    "              node_sizes=[int(n*10000) for n in node_sizes], #+[1000]\n",
    "              var='latent',\n",
    "              size=1000,\n",
    "              show_index=True,\n",
    "              save_fig=True,\n",
    "              figsize=(180, 100)\n",
    "             )\n",
    "\n",
    "print(\"CascVI: Imputations + Variance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CPM Normalization (for sample-sample correlation)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get imputations into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(72, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "#internal_scvi_X_2 = np.array([x for x in imputed_scvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_cascvi_X_2 = np.array([x for x in imputed_cascvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "\n",
    "internal_cascvi_X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: Some cells have total count of genes equal to zero\n",
      "WARNING: Some cells have total count of genes equal to zero\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(72, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_internal_X = sc.pp.normalize_total(AnnData(internal_X), target_sum=1e4, inplace=False)['X'] \n",
    "#norm_scvi_X_2 = sc.pp.normalize_total(AnnData(internal_scvi_X_2), target_sum=1e4, inplace=False)['X']\n",
    "norm_avg_X = sc.pp.normalize_total(AnnData(internal_avg_X), target_sum=1e4, inplace=False)['X']\n",
    "norm_imputed_X = sc.pp.normalize_total(AnnData(imputed_X), target_sum=1e4, inplace=False)['X']\n",
    "norm_cascvi_X_2 = sc.pp.normalize_total(AnnData(internal_cascvi_X_2), target_sum=1e4, inplace=False)['X']\n",
    "\n",
    "norm_internal_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sample-Sample Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Sample-Sample correlation (Without Normalization)***\n",
    "\n",
    "We will use Scipy to compute a nonparametric rank correlation between the imputed and the groundtruth profiles. The correlation is based on the Spearman Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth':internal_X.T, 'cascVI':imputed_X.T, \n",
    "        'Average':internal_avg_X.T , 'cascVI + Avg': internal_cascvi_X_2.T\n",
    "        }\n",
    "df1 = correlations(data, 'None', True)\n",
    "df1.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Sample-Sample correlation (With ScanPy Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': norm_internal_X.T, 'cascVI': norm_imputed_X.T, \n",
    "        'Average': norm_avg_X.T , 'cascVI + Avg': norm_cascvi_X_2.T\n",
    "        }\n",
    "\n",
    "df2 = correlations(data, 'None', True)\n",
    "df2.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II. Gene-Gene Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, \n",
    "        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2\n",
    "        }\n",
    "\n",
    "df4 = correlations(data, 'None', True)\n",
    "df4.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Gene-Gene correlation (With Rank Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, \n",
    "        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2\n",
    "        }\n",
    "\n",
    "df5 = correlations(data, 'rank', True)\n",
    "df5.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Method\", \"Spearman CC\", \"Pearson CC\", \"Kendall Tau\"]\n",
    "data = [df1, df2, df3, df4, df5]\n",
    "\n",
    "tables = [[] for i in range(len(data))]\n",
    "\n",
    "#task = [\"Sample-Sample (None)\", \"Sample-Sample (CPM)\", \"Gene-Gene (None)\", \n",
    "           #\"Gene-Gene(CPM)\", \"Gene-Gene (Rank)\" ]\n",
    "\n",
    "for (df, t) in zip(data, tables):\n",
    "    for m in np.unique(df.Method):\n",
    "        sub_df = np.round(df[df['Method'] == m].mean(), decimals=3)\n",
    "        t.append([m, sub_df['Spearman CC'], sub_df['Pearson CC'], sub_df['Pearson CC']])\n",
    "        \n",
    "# Create and style Data Frames\n",
    "df_table1 = pd.DataFrame(tables[0], columns=columns)\n",
    "df_table2 = pd.DataFrame(tables[1], columns=columns)\n",
    "df_table3 = pd.DataFrame(tables[2], columns=columns)\n",
    "df_table4 = pd.DataFrame(tables[3], columns=columns)\n",
    "df_table5 = pd.DataFrame(tables[4], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" >>> Sample-Sample | No Normalization <<<\")\n",
    "df_table1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Sample-Sample | CPM Normalization <<<\")\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Gene-Gene | No Normalization <<<\")\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Gene-Gene | CPM Normalization <<<\")\n",
    "df_table4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Gene-Gene | Rank Normalization <<<\")\n",
    "df_table5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Exploratory grapghical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Box plot (Samples)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 10\n",
    "idx = random.sample(range(1, 300), n_samples)\n",
    "n_rows = 2\n",
    "n_cols = int(n_samples / 2)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(30, 12))\n",
    "\n",
    "h = 0\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        k = idx[h]\n",
    "        h += 1\n",
    "        # stack arrrays\n",
    "        data_boxplot = np.vstack([np.log(1 + norm_internal_X[k]), np.log(1 + norm_imputed_X[k]),\n",
    "          np.log(1 + norm_avg_X[k]), np.log(1 + norm_scvi_X[k]),\n",
    "           np.log(1 + norm_cascvi_X[k])]).T\n",
    "        \n",
    "        # data frame\n",
    "        df_boxplot = pd.DataFrame(data=data_boxplot,\n",
    "                          columns=['Ground truth', 'CascVI', 'Avg', 'Avg scVI', 'Avg CascVI'])\n",
    "        \n",
    "        sns.boxplot(ax=ax[i][j], data=df_boxplot, orient=\"v\", palette=\"Set2\", showfliers=True)\n",
    "        ax[i][j].set_title('Sample ' + str(k))\n",
    "\n",
    "fig.suptitle(\"Boxplots of gene expression values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Box plot (Genes)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 10\n",
    "idx = random.sample(range(1, 1000), n_samples)\n",
    "n_rows = 2\n",
    "n_cols = int(n_samples / 2)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(30, 12))\n",
    "\n",
    "h = 0\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        k = idx[h]\n",
    "        h += 1\n",
    "        # stack arrrays\n",
    "        data_boxplot = np.vstack([np.log(1 + norm_internal_X[:, k]), np.log(1 + norm_imputed_X[:, k]),\n",
    "          np.log(1 + norm_avg_X[:, k]), np.log(1 + norm_scvi_X[:, k]),\n",
    "           np.log(1 + norm_cascvi_X[:, k])]).T\n",
    "        \n",
    "        # data frame\n",
    "        df_boxplot = pd.DataFrame(data=data_boxplot,\n",
    "                          columns=['Ground truth', 'CascVI', 'Avg', 'Avg scVI', 'Avg CascVI'])\n",
    "        \n",
    "        sns.boxplot(ax=ax[i][j], data=df_boxplot, orient=\"v\", palette=\"Set2\", showfliers=True)\n",
    "        ax[i][j].set_title('Gene ' + str(k))\n",
    "\n",
    "fig.suptitle(\"Boxplots of gene expression values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Density plots***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 12\n",
    "idx = random.sample(range(1, 1000), n_samples)\n",
    "n_rows = 3\n",
    "n_cols = int(n_samples / n_rows)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(35, 15))\n",
    "\n",
    "h = 0\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        k = idx[h]\n",
    "        h += 1\n",
    "        # density plots\n",
    "        sns.distplot(ax=ax[i][j], a=np.log(1 + norm_imputed_X[:, k]), hist=False,\n",
    "                     kde=True, kde_kws={'shade': True}, label='cascVI')\n",
    "        sns.distplot(ax=ax[i][j], a=np.log(1 + norm_internal_X[:, k]), hist=False,\n",
    "                     kde=True, kde_kws={'shade': True}, label='groundtruth')\n",
    "        sns.distplot(ax=ax[i][j], a=np.log(1 + norm_scvi_X[:, k]), hist=False,\n",
    "                     kde=True, kde_kws={'shade': True}, label='Avg scVI')\n",
    "        sns.distplot(ax=ax[i][j], a=np.log(1 + norm_avg_X[:, k]), hist=False,\n",
    "                     kde=True, kde_kws={'shade': True}, label='Avg')\n",
    "        sns.distplot(ax=ax[i][j], a=np.log(1 + norm_cascvi_X[:, k]), hist=False,\n",
    "             kde=True, kde_kws={'shade': True}, label='Avg cascVI')\n",
    "        # set title\n",
    "        ax[i][j].set_title('Gene ' + str(k))\n",
    "\n",
    "fig.suptitle(\"Combined gene density plots\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7.10 64-bit ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}