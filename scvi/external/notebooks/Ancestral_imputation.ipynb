{
 "cells": [
  {
   "source": [
    "# 0. Standard imports"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/external\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***import ete3 Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "#tree_name = \"/Users/khalilouardini/Desktop/projects/scVI/scvi/data/Cassiopeia_trees/3726_NT_T1_tree.processed.collapsed.tree\"\n",
    "#tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/Cassiopeia_trees/tree_test.txt\"\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/newick_objects/500cells/low_fitness/topology1.nwk\"\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "#tree = Tree()\n",
    "#tree.populate(60)\n",
    "\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)\n",
    "    if not n.is_leaf():\n",
    "        n.name = str(i)\n",
    "\n",
    "eps = 1e-3\n",
    "branch_length = {}\n",
    "for node in tree.traverse('levelorder'):\n",
    "    if node.name == '0':\n",
    "        branch_length[node.name] = 0.1\n",
    "        continue\n",
    "    branch_length[node.name] = node.dist\n",
    "branch_length['prior_root'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for i, n in enumerate(tree.traverse()):\n",
    "#    print(i, n.name)\n",
    "\n",
    "#split_node = tree.search_nodes(name='2|2|2|2|2|0|2|2|2|0|2|2|0|2|0|2|2|2|0|2|2|2|2|0|0|0|0|0|0')[0]\n",
    "#sub_leaves = [n.name for n in split_node.get_leaves()]\n",
    "#tree.prune(sub_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.poisson_glm import Poisson_GLM\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from models.vae import VAE\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from inference.inference import UnsupervisedTrainer\n",
    "from scvi.inference import posterior\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.metrics import ks_pvalue, accuracy_imputation, correlations, knn_purity, knn_purity_stratified\n",
    "from external.utils.plots_util import plot_histograms, plot_scatter_mean, plot_ecdf_ks, plot_density\n",
    "from external.utils.plots_util import plot_losses, plot_elbo, plot_common_ancestor, plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import avg_weighted_baseline, scvi_baseline, scvi_baseline_z, cascvi_baseline_z, avg_baseline_z, construct_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f425e00d7b0>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulations (Poisson GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "g = 1000\n",
    "vis = False\n",
    "leaves_only = False\n",
    "var = 1.0\n",
    "\n",
    "glm = Poisson_GLM(tree, g, d, vis, leaves_only, branch_length)\n",
    "\n",
    "glm.simulate_latent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate gene expression count data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 1000), (1000, 10), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "glm.simulate_ge()\n",
    "# Quality Control (i.e Gene Filtering)\n",
    "#glm.gene_qc()\n",
    "\n",
    "glm.X.shape, glm.W.shape, glm.beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Binomial thinning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts: 0.407328\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts: {}\".format(np.mean(glm.X == 0)))\n",
    "#glm.binomial_thinning(p=0.2)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts after Binomial thinning: 0.407328\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts after Binomial thinning: {}\".format(np.mean(glm.X == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get the data and the indexes at the leaves***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 1000), (500, 1000), (500, 1000), (500, 1000), (500, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Latent vectors\n",
    "leaves_z, _, _ = get_leaves(glm.z, glm.mu, tree)\n",
    "\n",
    "#FIXED training set\n",
    "leaves_X, leaves_idx, mu = get_leaves(glm.X, glm.mu, tree)\n",
    "\n",
    "# internal nodes data (for imputation)\n",
    "internal_X, internal_idx, internal_mu = get_internal(glm.X, glm.mu, tree)\n",
    "\n",
    "leaves_X.shape, mu.shape, internal_X.shape, internal_mu.shape, leaves_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Simulated latent space***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_common_ancestor(tree,\n",
    "#                     glm.z,\n",
    "#                     embedding='umap',\n",
    "#                     give_labels=False\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# anndata + gene and celle filtering\n",
    "adata = AnnData(leaves_X)\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]\n",
    "adata.obs_names = [n.name for n in leaves]\n",
    "#sc.pp.filter_genes(adata, min_counts=3)\n",
    "#sc.pp.filter_cells(adata, min_counts=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go\n",
      "[2021-05-09 19:31:28,924] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-09 19:31:28,930] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2021-05-09 19:31:28,977] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2021-05-09 19:31:28,981] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2021-05-09 19:31:28,987] INFO - scvi.dataset.dataset | Keeping 1000 genes\n",
      "[2021-05-09 19:31:29,008] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2021-05-09 19:31:29,014] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-09 19:31:29,018] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2021-05-09 19:31:29,027] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-09 19:31:29,032] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n"
     ]
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "tree_bis = copy.deepcopy(tree)\n",
    "scvi_dataset = AnnDatasetFromAnnData(adata, filtering=False)\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', adata.obs_names)\n",
    "cas_dataset = TreeDataset(scvi_dataset, tree=tree_bis, filtering=False)\n",
    "cas_dataset\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = True\n",
    "use_MP = True\n",
    "ldvae = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=use_MP\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (px_decoder): FCLayers(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (px_scale_decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1000, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "freeze = False\n",
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in treevae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "treevae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(treevae.decoder.factor_regressor.fc_layers[0][0].weight.numpy().all() == glm.W.T.all())\n",
    "#assert(treevae.decoder.factor_regressor.fc_layers[0][0].bias.numpy().all() == glm.beta.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Are we able to generate the gene expression data by decoding the simulated latent space?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance between the Poisson and the NB means is 5961.456851048614\n"
     ]
    }
   ],
   "source": [
    "px_scale, px_rate, raw_px_scale = treevae.decoder(treevae.dispersion,\n",
    "                                        torch.from_numpy(leaves_z).float(),\n",
    "                                        torch.from_numpy(np.array([np.log(10000)])).float()\n",
    "                                       )\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if ldvae:\n",
    "    foo = np.clip(a=np.exp(raw_px_scale.detach().cpu().numpy()),\n",
    "            a_min=0,\n",
    "            a_max=1e8\n",
    "    )\n",
    "    mse = mean_squared_error(mu, foo)\n",
    "else:\n",
    "    mse = mean_squared_error(mu, px_rate.detach().numpy())\n",
    "\n",
    "print(\"the distance between the Poisson and the NB means is {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "lambda_ = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_leaves:  [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499]]\ntest_leaves:  []\nvalidation leaves:  []\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raining:  85%|████████▌ | 850/1000 [04:46<00:34,  4.39it/s]Encodings MP Likelihood: 4.177644773841108\n",
      "ELBO Loss: 1519.6735763921838\n",
      "training:  85%|████████▌ | 851/1000 [04:46<00:33,  4.43it/s]Encodings MP Likelihood: 4.289974997708087\n",
      "ELBO Loss: 1521.6270742590707\n",
      "training:  85%|████████▌ | 852/1000 [04:46<00:33,  4.47it/s]Encodings MP Likelihood: 4.310018913155966\n",
      "ELBO Loss: 1509.954266918695\n",
      "training:  85%|████████▌ | 853/1000 [04:46<00:32,  4.52it/s]Encodings MP Likelihood: 4.0857642343762395\n",
      "ELBO Loss: 1512.5344878408089\n",
      "training:  85%|████████▌ | 854/1000 [04:46<00:32,  4.54it/s]Encodings MP Likelihood: 4.605856241834162\n",
      "ELBO Loss: 1523.764256768937\n",
      "training:  86%|████████▌ | 855/1000 [04:47<00:33,  4.37it/s]Encodings MP Likelihood: 4.26481207395714\n",
      "ELBO Loss: 1518.1411845932869\n",
      "training:  86%|████████▌ | 856/1000 [04:47<00:35,  4.06it/s]Encodings MP Likelihood: 4.042291842533213\n",
      "ELBO Loss: 1518.9833534618942\n",
      "training:  86%|████████▌ | 857/1000 [04:47<00:36,  3.87it/s]Encodings MP Likelihood: 4.081579219356258\n",
      "ELBO Loss: 1519.868258400065\n",
      "training:  86%|████████▌ | 858/1000 [04:48<00:37,  3.75it/s]Encodings MP Likelihood: 4.775622820577486\n",
      "ELBO Loss: 1520.0087184256925\n",
      "training:  86%|████████▌ | 859/1000 [04:48<00:35,  3.97it/s]Encodings MP Likelihood: 4.344811425519696\n",
      "ELBO Loss: 1522.5021862154758\n",
      "training:  86%|████████▌ | 860/1000 [04:48<00:36,  3.85it/s]Encodings MP Likelihood: 4.561575351180188\n",
      "ELBO Loss: 1518.8234188198055\n",
      "training:  86%|████████▌ | 861/1000 [04:48<00:34,  4.06it/s]Encodings MP Likelihood: 4.042941626614806\n",
      "ELBO Loss: 1517.0737909798393\n",
      "training:  86%|████████▌ | 862/1000 [04:49<00:32,  4.24it/s]Encodings MP Likelihood: 4.215825127740274\n",
      "ELBO Loss: 1514.013524312541\n",
      "training:  86%|████████▋ | 863/1000 [04:49<00:31,  4.37it/s]Encodings MP Likelihood: 4.012228635794345\n",
      "ELBO Loss: 1514.231019027859\n",
      "training:  86%|████████▋ | 864/1000 [04:49<00:31,  4.36it/s]Encodings MP Likelihood: 4.086262545806381\n",
      "ELBO Loss: 1515.8535550081838\n",
      "training:  86%|████████▋ | 865/1000 [04:49<00:30,  4.47it/s]Encodings MP Likelihood: 4.228013007733953\n",
      "ELBO Loss: 1524.5527658161177\n",
      "training:  87%|████████▋ | 866/1000 [04:49<00:29,  4.53it/s]Encodings MP Likelihood: 4.329315724140658\n",
      "ELBO Loss: 1517.4472674998347\n",
      "training:  87%|████████▋ | 867/1000 [04:50<00:29,  4.58it/s]Encodings MP Likelihood: 4.279237231666936\n",
      "ELBO Loss: 1510.299210937415\n",
      "training:  87%|████████▋ | 868/1000 [04:50<00:29,  4.50it/s]Encodings MP Likelihood: 4.239443254619373\n",
      "ELBO Loss: 1510.5431454430443\n",
      "training:  87%|████████▋ | 869/1000 [04:50<00:29,  4.42it/s]Encodings MP Likelihood: 4.283713352111295\n",
      "ELBO Loss: 1521.786548851287\n",
      "training:  87%|████████▋ | 870/1000 [04:50<00:28,  4.50it/s]Encodings MP Likelihood: 4.362196593708305\n",
      "ELBO Loss: 1515.26921479478\n",
      "training:  87%|████████▋ | 871/1000 [04:51<00:29,  4.33it/s]Encodings MP Likelihood: 4.3075015289362835\n",
      "ELBO Loss: 1524.16491566061\n",
      "training:  87%|████████▋ | 872/1000 [04:51<00:28,  4.42it/s]Encodings MP Likelihood: 4.2467414347055\n",
      "ELBO Loss: 1515.9707985558152\n",
      "training:  87%|████████▋ | 873/1000 [04:51<00:29,  4.27it/s]Encodings MP Likelihood: 4.3724082013986525\n",
      "ELBO Loss: 1521.3740370523842\n",
      "training:  87%|████████▋ | 874/1000 [04:51<00:29,  4.30it/s]Encodings MP Likelihood: 4.227342942963814\n",
      "ELBO Loss: 1518.9371161784893\n",
      "training:  88%|████████▊ | 875/1000 [04:51<00:29,  4.24it/s]Encodings MP Likelihood: 4.5127844907577925\n",
      "ELBO Loss: 1517.0920531852498\n",
      "training:  88%|████████▊ | 876/1000 [04:52<00:28,  4.37it/s]Encodings MP Likelihood: 3.9968300253823026\n",
      "ELBO Loss: 1521.7280996473592\n",
      "training:  88%|████████▊ | 877/1000 [04:52<00:27,  4.46it/s]Encodings MP Likelihood: 4.3381352286182295\n",
      "ELBO Loss: 1519.0300409326667\n",
      "training:  88%|████████▊ | 878/1000 [04:52<00:26,  4.54it/s]Encodings MP Likelihood: 4.303560883108809\n",
      "ELBO Loss: 1518.7120591046346\n",
      "training:  88%|████████▊ | 879/1000 [04:52<00:26,  4.51it/s]Encodings MP Likelihood: 4.315676540293253\n",
      "ELBO Loss: 1521.1967282745313\n",
      "training:  88%|████████▊ | 880/1000 [04:53<00:27,  4.44it/s]Encodings MP Likelihood: 4.044779893267912\n",
      "ELBO Loss: 1522.9747580742217\n",
      "training:  88%|████████▊ | 881/1000 [04:53<00:26,  4.51it/s]Encodings MP Likelihood: 4.01619484509241\n",
      "ELBO Loss: 1518.1478875541416\n",
      "training:  88%|████████▊ | 882/1000 [04:53<00:25,  4.57it/s]Encodings MP Likelihood: 4.212418414819374\n",
      "ELBO Loss: 1518.3993432236862\n",
      "training:  88%|████████▊ | 883/1000 [04:53<00:26,  4.39it/s]Encodings MP Likelihood: 3.877164325245128\n",
      "ELBO Loss: 1515.0545265241606\n",
      "training:  88%|████████▊ | 884/1000 [04:53<00:26,  4.45it/s]Encodings MP Likelihood: 4.109645364134801\n",
      "ELBO Loss: 1513.5839869125566\n",
      "training:  88%|████████▊ | 885/1000 [04:54<00:26,  4.31it/s]Encodings MP Likelihood: 4.140346185127934\n",
      "ELBO Loss: 1518.548311052909\n",
      "training:  89%|████████▊ | 886/1000 [04:54<00:28,  3.97it/s]Encodings MP Likelihood: 4.31209393672852\n",
      "ELBO Loss: 1512.384487618481\n",
      "training:  89%|████████▊ | 887/1000 [04:54<00:28,  3.92it/s]Encodings MP Likelihood: 4.166064181316627\n",
      "ELBO Loss: 1524.6191486349915\n",
      "training:  89%|████████▉ | 888/1000 [04:54<00:27,  4.10it/s]Encodings MP Likelihood: 4.266262676537992\n",
      "ELBO Loss: 1516.0583366782218\n",
      "training:  89%|████████▉ | 889/1000 [04:55<00:26,  4.26it/s]Encodings MP Likelihood: 4.150526314614053\n",
      "ELBO Loss: 1524.5472452648671\n",
      "training:  89%|████████▉ | 890/1000 [04:55<00:25,  4.32it/s]Encodings MP Likelihood: 4.371706486621557\n",
      "ELBO Loss: 1512.3354592564078\n",
      "training:  89%|████████▉ | 891/1000 [04:55<00:25,  4.31it/s]Encodings MP Likelihood: 4.479186870106937\n",
      "ELBO Loss: 1521.985013484638\n",
      "training:  89%|████████▉ | 892/1000 [04:55<00:24,  4.40it/s]Encodings MP Likelihood: 3.9264384556700827\n",
      "ELBO Loss: 1526.9025910883345\n",
      "training:  89%|████████▉ | 893/1000 [04:56<00:23,  4.49it/s]Encodings MP Likelihood: 4.247037236683749\n",
      "ELBO Loss: 1513.5662047560788\n",
      "training:  89%|████████▉ | 894/1000 [04:56<00:24,  4.40it/s]Encodings MP Likelihood: 4.358358505109548\n",
      "ELBO Loss: 1509.330279292883\n",
      "training:  90%|████████▉ | 895/1000 [04:56<00:23,  4.47it/s]Encodings MP Likelihood: 4.252740313862054\n",
      "ELBO Loss: 1512.9027561289856\n",
      "training:  90%|████████▉ | 896/1000 [04:56<00:22,  4.55it/s]Encodings MP Likelihood: 4.174169295502675\n",
      "ELBO Loss: 1521.9900692068945\n",
      "training:  90%|████████▉ | 897/1000 [04:56<00:22,  4.51it/s]Encodings MP Likelihood: 4.163804624832214\n",
      "ELBO Loss: 1514.4118369084515\n",
      "training:  90%|████████▉ | 898/1000 [04:57<00:22,  4.58it/s]Encodings MP Likelihood: 4.269107312153613\n",
      "ELBO Loss: 1518.3053932113005\n",
      "training:  90%|████████▉ | 899/1000 [04:57<00:21,  4.63it/s]Encodings MP Likelihood: 4.264322378356415\n",
      "ELBO Loss: 1522.4113973179212\n",
      "computing elbo\n",
      "training:  90%|█████████ | 900/1000 [04:57<00:24,  4.11it/s]Encodings MP Likelihood: 4.5859359858224105\n",
      "ELBO Loss: 1525.6635679406377\n",
      "training:  90%|█████████ | 901/1000 [04:57<00:23,  4.27it/s]Encodings MP Likelihood: 4.451856846638696\n",
      "ELBO Loss: 1510.371310435757\n",
      "training:  90%|█████████ | 902/1000 [04:58<00:22,  4.37it/s]Encodings MP Likelihood: 4.106164188314005\n",
      "ELBO Loss: 1509.7378489764742\n",
      "training:  90%|█████████ | 903/1000 [04:58<00:22,  4.37it/s]Encodings MP Likelihood: 4.27485374329961\n",
      "ELBO Loss: 1515.4363250454487\n",
      "training:  90%|█████████ | 904/1000 [04:58<00:21,  4.47it/s]Encodings MP Likelihood: 4.088522671463111\n",
      "ELBO Loss: 1519.427489141686\n",
      "training:  90%|█████████ | 905/1000 [04:58<00:20,  4.54it/s]Encodings MP Likelihood: 3.9406390089311585\n",
      "ELBO Loss: 1516.2799595473305\n",
      "training:  91%|█████████ | 906/1000 [04:58<00:20,  4.59it/s]Encodings MP Likelihood: 4.237664513518669\n",
      "ELBO Loss: 1514.3236676508407\n",
      "training:  91%|█████████ | 907/1000 [04:59<00:20,  4.63it/s]Encodings MP Likelihood: 4.2499679498943586\n",
      "ELBO Loss: 1517.636724822983\n",
      "training:  91%|█████████ | 908/1000 [04:59<00:19,  4.65it/s]Encodings MP Likelihood: 4.04020263750722\n",
      "ELBO Loss: 1519.098074574777\n",
      "training:  91%|█████████ | 909/1000 [04:59<00:19,  4.68it/s]Encodings MP Likelihood: 4.2815232009682616\n",
      "ELBO Loss: 1514.440351559845\n",
      "training:  91%|█████████ | 910/1000 [04:59<00:19,  4.67it/s]Encodings MP Likelihood: 4.089094306286673\n",
      "ELBO Loss: 1517.8437955267711\n",
      "training:  91%|█████████ | 911/1000 [05:00<00:19,  4.67it/s]Encodings MP Likelihood: 4.035348520613137\n",
      "ELBO Loss: 1521.7514765303492\n",
      "training:  91%|█████████ | 912/1000 [05:00<00:18,  4.68it/s]Encodings MP Likelihood: 4.139934030017252\n",
      "ELBO Loss: 1521.1651338593142\n",
      "training:  91%|█████████▏| 913/1000 [05:00<00:18,  4.74it/s]Encodings MP Likelihood: 4.164533456422462\n",
      "ELBO Loss: 1507.3146480217044\n",
      "training:  91%|█████████▏| 914/1000 [05:00<00:18,  4.73it/s]Encodings MP Likelihood: 4.240623933207504\n",
      "ELBO Loss: 1507.4438420491583\n",
      "training:  92%|█████████▏| 915/1000 [05:00<00:17,  4.73it/s]Encodings MP Likelihood: 4.114919355456693\n",
      "ELBO Loss: 1508.4841206104013\n",
      "training:  92%|█████████▏| 916/1000 [05:01<00:17,  4.74it/s]Encodings MP Likelihood: 4.271598441753659\n",
      "ELBO Loss: 1522.9648459465307\n",
      "training:  92%|█████████▏| 917/1000 [05:01<00:18,  4.60it/s]Encodings MP Likelihood: 4.144021133788897\n",
      "ELBO Loss: 1526.670317087494\n",
      "training:  92%|█████████▏| 918/1000 [05:01<00:17,  4.65it/s]Encodings MP Likelihood: 4.178118904493756\n",
      "ELBO Loss: 1517.9852707860532\n",
      "training:  92%|█████████▏| 919/1000 [05:01<00:18,  4.46it/s]Encodings MP Likelihood: 4.447159659618034\n",
      "ELBO Loss: 1516.200639409654\n",
      "training:  92%|█████████▏| 920/1000 [05:01<00:17,  4.54it/s]Encodings MP Likelihood: 4.06925668073182\n",
      "ELBO Loss: 1516.669852753409\n",
      "training:  92%|█████████▏| 921/1000 [05:02<00:18,  4.25it/s]Encodings MP Likelihood: 4.185085801139018\n",
      "ELBO Loss: 1519.72323887261\n",
      "training:  92%|█████████▏| 922/1000 [05:02<00:18,  4.19it/s]Encodings MP Likelihood: 4.02282021357244\n",
      "ELBO Loss: 1518.417140854604\n",
      "training:  92%|█████████▏| 923/1000 [05:02<00:18,  4.14it/s]Encodings MP Likelihood: 3.9695227701975426\n",
      "ELBO Loss: 1523.1833206769245\n",
      "training:  92%|█████████▏| 924/1000 [05:02<00:17,  4.29it/s]Encodings MP Likelihood: 3.980328917980122\n",
      "ELBO Loss: 1508.8873372903945\n",
      "training:  92%|█████████▎| 925/1000 [05:03<00:16,  4.42it/s]Encodings MP Likelihood: 4.014545175632932\n",
      "ELBO Loss: 1515.7607008124148\n",
      "training:  93%|█████████▎| 926/1000 [05:03<00:16,  4.51it/s]Encodings MP Likelihood: 3.8845644826163936\n",
      "ELBO Loss: 1508.4394894422906\n",
      "training:  93%|█████████▎| 927/1000 [05:03<00:16,  4.49it/s]Encodings MP Likelihood: 4.007648951299534\n",
      "ELBO Loss: 1511.6090558838805\n",
      "training:  93%|█████████▎| 928/1000 [05:03<00:15,  4.56it/s]Encodings MP Likelihood: 4.1889313605371985\n",
      "ELBO Loss: 1505.9225778298235\n",
      "training:  93%|█████████▎| 929/1000 [05:04<00:15,  4.55it/s]Encodings MP Likelihood: 3.8858042273954556\n",
      "ELBO Loss: 1507.5590745033987\n",
      "training:  93%|█████████▎| 930/1000 [05:04<00:15,  4.54it/s]Encodings MP Likelihood: 4.215809346467484\n",
      "ELBO Loss: 1523.2680853028594\n",
      "training:  93%|█████████▎| 931/1000 [05:04<00:15,  4.57it/s]Encodings MP Likelihood: 4.383899386095655\n",
      "ELBO Loss: 1520.6200652805114\n",
      "training:  93%|█████████▎| 932/1000 [05:04<00:14,  4.60it/s]Encodings MP Likelihood: 4.135651096961754\n",
      "ELBO Loss: 1510.373131949068\n",
      "training:  93%|█████████▎| 933/1000 [05:04<00:14,  4.53it/s]Encodings MP Likelihood: 3.9190698519158698\n",
      "ELBO Loss: 1517.1549822577808\n",
      "training:  93%|█████████▎| 934/1000 [05:05<00:14,  4.59it/s]Encodings MP Likelihood: 4.422979834977846\n",
      "ELBO Loss: 1512.2785071170363\n",
      "training:  94%|█████████▎| 935/1000 [05:05<00:14,  4.62it/s]Encodings MP Likelihood: 4.072027370095218\n",
      "ELBO Loss: 1516.7509947559188\n",
      "training:  94%|█████████▎| 936/1000 [05:05<00:14,  4.37it/s]Encodings MP Likelihood: 4.396608651201024\n",
      "ELBO Loss: 1504.963715160934\n",
      "training:  94%|█████████▎| 937/1000 [05:05<00:14,  4.45it/s]Encodings MP Likelihood: 4.247884796327187\n",
      "ELBO Loss: 1515.5960409899483\n",
      "training:  94%|█████████▍| 938/1000 [05:06<00:13,  4.53it/s]Encodings MP Likelihood: 4.037401879648562\n",
      "ELBO Loss: 1510.2085155462548\n",
      "training:  94%|█████████▍| 939/1000 [05:06<00:13,  4.59it/s]Encodings MP Likelihood: 4.293648927269498\n",
      "ELBO Loss: 1514.0426374808515\n",
      "training:  94%|█████████▍| 940/1000 [05:06<00:13,  4.30it/s]Encodings MP Likelihood: 4.383730358656146\n",
      "ELBO Loss: 1516.181544435281\n",
      "training:  94%|█████████▍| 941/1000 [05:06<00:13,  4.39it/s]Encodings MP Likelihood: 4.312445030355202\n",
      "ELBO Loss: 1504.4255209929115\n",
      "training:  94%|█████████▍| 942/1000 [05:06<00:12,  4.47it/s]Encodings MP Likelihood: 4.011059806378227\n",
      "ELBO Loss: 1519.4595912517511\n",
      "training:  94%|█████████▍| 943/1000 [05:07<00:12,  4.53it/s]Encodings MP Likelihood: 4.129449697474264\n",
      "ELBO Loss: 1523.1501400241389\n",
      "training:  94%|█████████▍| 944/1000 [05:07<00:12,  4.59it/s]Encodings MP Likelihood: 4.351715376958422\n",
      "ELBO Loss: 1516.350656401934\n",
      "training:  94%|█████████▍| 945/1000 [05:07<00:13,  4.14it/s]Encodings MP Likelihood: 4.3391170251159865\n",
      "ELBO Loss: 1520.559093923793\n",
      "training:  95%|█████████▍| 946/1000 [05:07<00:12,  4.29it/s]Encodings MP Likelihood: 4.103989664750378\n",
      "ELBO Loss: 1511.0089699229534\n",
      "training:  95%|█████████▍| 947/1000 [05:08<00:12,  4.40it/s]Encodings MP Likelihood: 4.312941719919193\n",
      "ELBO Loss: 1507.9016543454393\n",
      "training:  95%|█████████▍| 948/1000 [05:08<00:12,  4.25it/s]Encodings MP Likelihood: 4.2879823244034405\n",
      "ELBO Loss: 1511.434955900222\n",
      "training:  95%|█████████▍| 949/1000 [05:08<00:11,  4.27it/s]Encodings MP Likelihood: 3.842138735702964\n",
      "ELBO Loss: 1503.4746611218043\n",
      "training:  95%|█████████▌| 950/1000 [05:08<00:11,  4.30it/s]Encodings MP Likelihood: 4.132152244089783\n",
      "ELBO Loss: 1514.3678251419954\n",
      "training:  95%|█████████▌| 951/1000 [05:09<00:11,  4.41it/s]Encodings MP Likelihood: 4.2083582292689705\n",
      "ELBO Loss: 1505.869383707706\n",
      "training:  95%|█████████▌| 952/1000 [05:09<00:10,  4.51it/s]Encodings MP Likelihood: 4.297185507937105\n",
      "ELBO Loss: 1515.715066294833\n",
      "training:  95%|█████████▌| 953/1000 [05:09<00:10,  4.59it/s]Encodings MP Likelihood: 4.204724874913973\n",
      "ELBO Loss: 1514.2960981076712\n",
      "training:  95%|█████████▌| 954/1000 [05:09<00:09,  4.64it/s]Encodings MP Likelihood: 4.494303729308532\n",
      "ELBO Loss: 1511.597154322476\n",
      "training:  96%|█████████▌| 955/1000 [05:09<00:09,  4.66it/s]Encodings MP Likelihood: 4.120119385758026\n",
      "ELBO Loss: 1505.0680699722334\n",
      "training:  96%|█████████▌| 956/1000 [05:10<00:09,  4.69it/s]Encodings MP Likelihood: 3.971810384082407\n",
      "ELBO Loss: 1515.0942112796847\n",
      "training:  96%|█████████▌| 957/1000 [05:10<00:09,  4.58it/s]Encodings MP Likelihood: 4.157666487398483\n",
      "ELBO Loss: 1515.3605591610728\n",
      "training:  96%|█████████▌| 958/1000 [05:10<00:09,  4.59it/s]Encodings MP Likelihood: 4.01990466965944\n",
      "ELBO Loss: 1511.9342042320447\n",
      "training:  96%|█████████▌| 959/1000 [05:10<00:08,  4.64it/s]Encodings MP Likelihood: 4.091118086082362\n",
      "ELBO Loss: 1512.11959731036\n",
      "training:  96%|█████████▌| 960/1000 [05:10<00:08,  4.60it/s]Encodings MP Likelihood: 4.154669343589727\n",
      "ELBO Loss: 1513.8836486491252\n",
      "training:  96%|█████████▌| 961/1000 [05:11<00:08,  4.51it/s]Encodings MP Likelihood: 4.081731573952162\n",
      "ELBO Loss: 1510.8700305902812\n",
      "training:  96%|█████████▌| 962/1000 [05:11<00:08,  4.56it/s]Encodings MP Likelihood: 4.148092029755606\n",
      "ELBO Loss: 1514.7296734907798\n",
      "training:  96%|█████████▋| 963/1000 [05:11<00:08,  4.55it/s]Encodings MP Likelihood: 4.01486687246522\n",
      "ELBO Loss: 1516.7383951703582\n",
      "training:  96%|█████████▋| 964/1000 [05:11<00:07,  4.61it/s]Encodings MP Likelihood: 4.320181096247054\n",
      "ELBO Loss: 1520.7865087366508\n",
      "training:  96%|█████████▋| 965/1000 [05:12<00:07,  4.64it/s]Encodings MP Likelihood: 4.162179386139745\n",
      "ELBO Loss: 1522.3404480182676\n",
      "training:  97%|█████████▋| 966/1000 [05:12<00:07,  4.62it/s]Encodings MP Likelihood: 4.087015222396582\n",
      "ELBO Loss: 1511.7079567802566\n",
      "training:  97%|█████████▋| 967/1000 [05:12<00:07,  4.53it/s]Encodings MP Likelihood: 4.079970889737693\n",
      "ELBO Loss: 1507.6642910074133\n",
      "training:  97%|█████████▋| 968/1000 [05:12<00:06,  4.59it/s]Encodings MP Likelihood: 4.491984837234396\n",
      "ELBO Loss: 1511.8690880863\n",
      "training:  97%|█████████▋| 969/1000 [05:12<00:06,  4.63it/s]Encodings MP Likelihood: 3.991455149802126\n",
      "ELBO Loss: 1516.703650280623\n",
      "training:  97%|█████████▋| 970/1000 [05:13<00:06,  4.65it/s]Encodings MP Likelihood: 3.9893375359220786\n",
      "ELBO Loss: 1507.7637738967\n",
      "training:  97%|█████████▋| 971/1000 [05:13<00:06,  4.67it/s]Encodings MP Likelihood: 4.053643599261198\n",
      "ELBO Loss: 1507.987194869427\n",
      "training:  97%|█████████▋| 972/1000 [05:13<00:06,  4.47it/s]Encodings MP Likelihood: 4.209569589679226\n",
      "ELBO Loss: 1504.022729664562\n",
      "training:  97%|█████████▋| 973/1000 [05:13<00:05,  4.56it/s]Encodings MP Likelihood: 4.178226485367486\n",
      "ELBO Loss: 1508.2826251595127\n",
      "training:  97%|█████████▋| 974/1000 [05:14<00:05,  4.62it/s]Encodings MP Likelihood: 4.1165168211315715\n",
      "ELBO Loss: 1522.6051528791484\n",
      "training:  98%|█████████▊| 975/1000 [05:14<00:05,  4.67it/s]Encodings MP Likelihood: 4.3899409592693655\n",
      "ELBO Loss: 1514.6561990533216\n",
      "training:  98%|█████████▊| 976/1000 [05:14<00:05,  4.52it/s]Encodings MP Likelihood: 4.28121433628621\n",
      "ELBO Loss: 1514.029680347466\n",
      "training:  98%|█████████▊| 977/1000 [05:14<00:05,  4.57it/s]Encodings MP Likelihood: 3.9429663087790185\n",
      "ELBO Loss: 1511.1274018433776\n",
      "training:  98%|█████████▊| 978/1000 [05:14<00:04,  4.54it/s]Encodings MP Likelihood: 4.120107404957912\n",
      "ELBO Loss: 1506.8581253776342\n",
      "training:  98%|█████████▊| 979/1000 [05:15<00:04,  4.61it/s]Encodings MP Likelihood: 4.492800304956409\n",
      "ELBO Loss: 1512.1199153723792\n",
      "training:  98%|█████████▊| 980/1000 [05:15<00:04,  4.37it/s]Encodings MP Likelihood: 4.124197556835441\n",
      "ELBO Loss: 1513.6047534372215\n",
      "training:  98%|█████████▊| 981/1000 [05:15<00:04,  4.47it/s]Encodings MP Likelihood: 3.9665099333195024\n",
      "ELBO Loss: 1515.4859809929042\n",
      "training:  98%|█████████▊| 982/1000 [05:15<00:03,  4.55it/s]Encodings MP Likelihood: 4.197157384160715\n",
      "ELBO Loss: 1510.2961947343588\n",
      "training:  98%|█████████▊| 983/1000 [05:15<00:03,  4.60it/s]Encodings MP Likelihood: 4.437345106482444\n",
      "ELBO Loss: 1508.2166074382158\n",
      "training:  98%|█████████▊| 984/1000 [05:16<00:03,  4.63it/s]Encodings MP Likelihood: 4.37633604743601\n",
      "ELBO Loss: 1515.4692797284383\n",
      "training:  98%|█████████▊| 985/1000 [05:16<00:03,  4.67it/s]Encodings MP Likelihood: 4.39859866784471\n",
      "ELBO Loss: 1519.7272741895852\n",
      "training:  99%|█████████▊| 986/1000 [05:16<00:02,  4.70it/s]Encodings MP Likelihood: 4.323418952699127\n",
      "ELBO Loss: 1516.562963612163\n",
      "training:  99%|█████████▊| 987/1000 [05:16<00:02,  4.68it/s]Encodings MP Likelihood: 4.04531708022391\n",
      "ELBO Loss: 1511.0546645714132\n",
      "training:  99%|█████████▉| 988/1000 [05:17<00:02,  4.71it/s]Encodings MP Likelihood: 4.339952864380884\n",
      "ELBO Loss: 1516.3085518343246\n",
      "training:  99%|█████████▉| 989/1000 [05:17<00:02,  4.72it/s]Encodings MP Likelihood: 4.251076600065158\n",
      "ELBO Loss: 1510.2231160573892\n",
      "training:  99%|█████████▉| 990/1000 [05:17<00:02,  4.68it/s]Encodings MP Likelihood: 4.201906232529923\n",
      "ELBO Loss: 1517.029185257218\n",
      "training:  99%|█████████▉| 991/1000 [05:17<00:02,  4.50it/s]Encodings MP Likelihood: 4.186665201350141\n",
      "ELBO Loss: 1510.5065491643602\n",
      "training:  99%|█████████▉| 992/1000 [05:17<00:01,  4.54it/s]Encodings MP Likelihood: 4.161347784818742\n",
      "ELBO Loss: 1514.4632185519094\n",
      "training:  99%|█████████▉| 993/1000 [05:18<00:01,  4.59it/s]Encodings MP Likelihood: 3.852715622455495\n",
      "ELBO Loss: 1509.6430872793867\n",
      "training:  99%|█████████▉| 994/1000 [05:18<00:01,  4.62it/s]Encodings MP Likelihood: 4.047504331153799\n",
      "ELBO Loss: 1521.2368731324711\n",
      "training: 100%|█████████▉| 995/1000 [05:18<00:01,  4.46it/s]Encodings MP Likelihood: 3.937093553560593\n",
      "ELBO Loss: 1514.6150772502638\n",
      "training: 100%|█████████▉| 996/1000 [05:18<00:00,  4.15it/s]Encodings MP Likelihood: 4.147430079307172\n",
      "ELBO Loss: 1512.8572638207345\n",
      "training: 100%|█████████▉| 997/1000 [05:19<00:00,  3.96it/s]Encodings MP Likelihood: 4.138490618422668\n",
      "ELBO Loss: 1510.0725075653127\n",
      "training: 100%|█████████▉| 998/1000 [05:19<00:00,  3.95it/s]Encodings MP Likelihood: 4.167655357555341\n",
      "ELBO Loss: 1501.175085872934\n",
      "training: 100%|█████████▉| 999/1000 [05:19<00:00,  4.14it/s]Encodings MP Likelihood: 4.143418119553462\n",
      "ELBO Loss: 1509.4359625951563\n",
      "computing elbo\n",
      "training: 100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior and MV imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance is 1.509854666059559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "error = mean_squared_error(full_posterior.get_latent(), leaves_z)\n",
    "print(\"the distance is {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Missing Value imputation By Posterior Predictive sampling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_l = np.mean(np.sum(glm.X, axis=1))\n",
    "\n",
    "# CascVI impitations\n",
    "imputed = {}\n",
    "imputed_z = {}\n",
    "imputed_gt = {}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        imputed[n.name], imputed_z[n.name] = full_posterior.imputation_internal(n.name,\n",
    "                                                            give_mean=False,\n",
    "                                                            library_size=empirical_l\n",
    "                                                           )\n",
    "        imputed_gt[n.name] = glm.X[n.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_X = [x for x in imputed.values()]\n",
    "imputed_X = np.array(imputed_X).reshape(-1, cas_dataset.X.shape[1])\n",
    "#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 1 (MP Oracle)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CascVI impitations\n",
    "imputed_cascvi_1 = {}\n",
    "imputed_cascvi_1_z ={}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        _, imputed_cascvi_1_z[n.name] = full_posterior.imputation_internal(n.name,\n",
    "                                                                    give_mean=False,\n",
    "                                                                    library_size=empirical_l,\n",
    "                                                                    known_latent=leaves_z\n",
    "        )\n",
    "        mu_z = np.clip(a=np.exp(glm.W @ imputed_cascvi_1_z[n.name].cpu().numpy() + glm.beta),\n",
    "                        a_min=0,\n",
    "                        a_max=1e8\n",
    "                        )\n",
    "        samples = np.array([np.random.poisson(mu_z) for i in range(100)])\n",
    "        imputed_cascvi_1[n.name] = np.clip(a=np.mean(samples, axis=0),\n",
    "                                           a_min=0,\n",
    "                                           a_max=1e8\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 2 (Reconstruction of Averaged latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_cascvi_2, imputed_cascvi_2_z, _ = avg_baseline_z(tree=tree,\n",
    "                                   model=treevae,\n",
    "                                   posterior=full_posterior,\n",
    "                                   weighted=False,\n",
    "                                   n_samples_z=1,\n",
    "                                   library_size=empirical_l,\n",
    "                                   gaussian=False,\n",
    "                                   use_cuda=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Unweighted Average of gene expression in Clade\n",
    "\n",
    "The simple idea here is to impute the value of an internal node, with the (un)weighted average of the gene expression values of the leaves, taking the query internal node as the root of the subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = False\n",
    "imputed_avg = avg_weighted_baseline(tree, weighted, glm.X, rounding=True)\n",
    "\n",
    "#get internal nodes\n",
    "avg_X = np.array([x for x in imputed_avg.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_avg_X, _, _ = get_internal(avg_X, glm.mu, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: (Un)weighted Average of decoded latent vectors, with scVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same averaging of the subtrees leaves in **Baseline 1**, only this time, the gene expression data is recovered with scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2021-05-09 19:38:16,163] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2021-05-09 19:38:16,164] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n"
     ]
    }
   ],
   "source": [
    "# anndata\n",
    "gene_dataset = GeneExpressionDataset()\n",
    "gene_dataset.populate_from_data(leaves_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecoderSCVI(\n",
       "  (px_decoder): FCLayers(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (px_scale_decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1000, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       "  (px_r_decoder): Linear(in_features=128, out_features=1000, bias=True)\n",
       "  (px_dropout_decoder): Linear(in_features=128, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs =800\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=128,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=glm.latent,\n",
    "                  ldvae=ldvae\n",
    "              )\n",
    "\n",
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in vae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance between the Poisson and the NB means is 5957.122873700708\n"
     ]
    }
   ],
   "source": [
    "px_scale, px_r, px_rate, px_dropout = vae.decoder.forward(vae.dispersion,\n",
    "                                        torch.from_numpy(leaves_z).float(),\n",
    "                                        torch.from_numpy(np.array([np.log(10000)])).float(),\n",
    "                                        None\n",
    "                                        )\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "if ldvae:\n",
    "    foo = np.clip(a=np.exp(px_r.detach().numpy()),\n",
    "            a_min=0,\n",
    "            a_max=5000\n",
    "    )\n",
    "    mse = mean_squared_error(mu, foo)\n",
    "else:\n",
    "    mse = mean_squared_error(mu, px_rate.detach().numpy())\n",
    "\n",
    "print(\"the distance between the Poisson and the NB means is {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 800/800 [00:50<00:00, 15.87it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=200)\n",
    "\n",
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "        \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance is 2.5303279201849365\n"
     ]
    }
   ],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior(model=vae,\n",
    "                                               gene_dataset=gene_dataset \n",
    "                                                )\n",
    "\n",
    "error = mean_squared_error(scvi_posterior.get_latent()[0], leaves_z)\n",
    "print(\"the distance is {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 2 (Decoded Average Latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_size = np.mean(np.sum(glm.X, axis=1))\n",
    "scvi_latent = np.array([scvi_posterior.get_latent(give_mean=False)[0] for i in range(10)])\n",
    "\n",
    "imputed_scvi_2, imputed_scvi_2_z = scvi_baseline_z(tree,\n",
    "                                        posterior=scvi_posterior,\n",
    "                                        model=vae,\n",
    "                                        weighted=False,\n",
    "                                        n_samples_z=1,\n",
    "                                        library_size=library_size,\n",
    "                                        use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 10), (500, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "cascvi_latent = full_posterior.get_latent()\n",
    "scvi_latent = scvi_posterior.get_latent()[0]\n",
    "\n",
    "scvi_latent.shape, cascvi_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of scVI encodings:  -35218.86535160112\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(scvi_latent, cas_dataset.barcodes, scvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), scvi_latent.shape[1], False)\n",
    "mp_lik_scvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of scVI encodings: \", mp_lik_scvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  -1184.4970744292652\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(cascvi_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of observations:  -11072.953737048714\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(leaves_z, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of observations: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood Ratio: tensor(24145.9116, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood ratio\n",
    "lambda_ = (mp_lik_cascvi - mp_lik_scvi)\n",
    "print(\"Likelihood Ratio:\", lambda_)"
   ]
  },
  {
   "source": [
    "# 6. Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CPM Normalization (for sample-sample correlation)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get imputations into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 1000), (500, 1000), (500, 1000), (500, 1000), (500, 1000), (500, 1000))"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "internal_scvi_X_2 = np.array([x for x in imputed_scvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_cascvi_X = np.array([x for x in imputed_cascvi_1.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_cascvi_X_2 = np.array([x for x in imputed_cascvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "\n",
    "internal_cascvi_X_2.shape, internal_cascvi_X.shape, internal_scvi_X_2.shape, imputed_X.shape, internal_avg_X.shape, internal_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_internal_X = sc.pp.normalize_total(AnnData(internal_X), target_sum=1e4, inplace=False)['X'] \n",
    "norm_scvi_X_2 = sc.pp.normalize_total(AnnData(internal_scvi_X_2), target_sum=1e4, inplace=False)['X']\n",
    "norm_avg_X = sc.pp.normalize_total(AnnData(internal_avg_X), target_sum=1e4, inplace=False)['X']\n",
    "norm_imputed_X = sc.pp.normalize_total(AnnData(imputed_X), target_sum=1e4, inplace=False)['X']\n",
    "norm_cascvi_X = sc.pp.normalize_total(AnnData(internal_cascvi_X), target_sum=1e4, inplace=False)['X']\n",
    "norm_cascvi_X_2 = sc.pp.normalize_total(AnnData(internal_cascvi_X_2), target_sum=1e4, inplace=False)['X']\n",
    "\n",
    "norm_internal_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((0.6192368945282855, 2.892094357821938e-54),\n",
       " KendalltauResult(correlation=0.41598314063070274, pvalue=7.113542769021682e-36))"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "data0 = internal_X[:, 0]\n",
    "data1 = imputed_X[:, 0]\n",
    "\n",
    "stats.pearsonr(data1, data0), stats.kendalltau(data1, data0), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sample-Sample Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Sample-Sample correlation (Without Normalization)***\n",
    "\n",
    "We will use Scipy to compute a nonparametric rank correlation between the imputed and the groundtruth profiles. The correlation is based on the Spearman Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'groundtruth': internal_X.T, 'cascVI': imputed_X.T, 'scVI': internal_scvi_X_2.T,\n",
    "        'Average': internal_avg_X.T , 'cascVI + Avg': internal_cascvi_X_2.T,\n",
    "        'MP Oracle': internal_cascvi_X.T\n",
    "        }\n",
    "df1 = correlations(data, 'None', True)\n",
    "df1.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Sample-Sample correlation (With ScanPy Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': norm_internal_X.T, 'cascVI': norm_imputed_X.T, 'scVI': norm_scvi_X_2.T, \n",
    "        'Average': norm_avg_X.T , 'cascVI + Avg': norm_cascvi_X_2.T,\n",
    "        'MP Oracle': norm_cascvi_X.T\n",
    "        }\n",
    "\n",
    "df2 = correlations(data, 'None', True)\n",
    "df2.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II. Gene-Gene Correlations"
   ]
  },
  {
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2,\n",
    "        'MP Oracle': internal_cascvi_X\n",
    "        }\n",
    "\n",
    "df3 = correlations(data, 'None', True)\n",
    "df3.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2,\n",
    "        'MP Oracle': norm_cascvi_X\n",
    "        }\n",
    "\n",
    "df4 = correlations(data, 'None', True)\n",
    "df4.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Gene-Gene correlation (With Rank Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "#        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2,\n",
    "#        'MP Oracle': norm_cascvi_X\n",
    "#        }\n",
    "\n",
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2,\n",
    "        'MP Oracle': internal_cascvi_X\n",
    "        }\n",
    "        \n",
    "df5 = correlations(data, 'rank', True)\n",
    "df5.head(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Method\", \"Spearman CC\", \"Pearson CC\", \"Kendall Tau\"]\n",
    "data = [df1, df2, df3, df4, df5]\n",
    "#data = [df2, df4]\n",
    "\n",
    "data \n",
    "tables = [[] for i in range(len(data))]\n",
    "\n",
    "#task = [\"Sample-Sample (None)\", \"Sample-Sample (CPM)\", \"Gene-Gene (None)\", \n",
    "           #\"Gene-Gene(CPM)\", \"Gene-Gene (Rank)\" ]\n",
    "\n",
    "for (df, t) in zip(data, tables):\n",
    "    for m in np.unique(df.Method):\n",
    "        sub_df = np.round(df[df['Method'] == m].mean(), decimals=3)\n",
    "        t.append([m, sub_df['Spearman CC'], sub_df['Pearson CC'], sub_df['Kendall Tau']])\n",
    "        \n",
    "# Create and style Data Frames\n",
    "df_table1 = pd.DataFrame(tables[0], columns=columns)\n",
    "df_table2 = pd.DataFrame(tables[1], columns=columns)\n",
    "df_table3 = pd.DataFrame(tables[2], columns=columns)\n",
    "df_table4 = pd.DataFrame(tables[3], columns=columns)\n",
    "df_table5 = pd.DataFrame(tables[4], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " >>> Sample-Sample | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.747       0.881        0.653\n",
       "1     MP Oracle        0.795       0.929        0.659\n",
       "2        cascVI        0.784       0.912        0.646\n",
       "3  cascVI + Avg        0.774       0.891        0.637\n",
       "4          scVI        0.769       0.888        0.632"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.747</td>\n      <td>0.881</td>\n      <td>0.653</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MP Oracle</td>\n      <td>0.795</td>\n      <td>0.929</td>\n      <td>0.659</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI</td>\n      <td>0.784</td>\n      <td>0.912</td>\n      <td>0.646</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cascVI + Avg</td>\n      <td>0.774</td>\n      <td>0.891</td>\n      <td>0.637</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>scVI</td>\n      <td>0.769</td>\n      <td>0.888</td>\n      <td>0.632</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "print(\" >>> Sample-Sample | No Normalization <<<\")\n",
    "df_table1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Sample-Sample | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.747       0.881        0.653\n",
       "1     MP Oracle        0.795       0.929        0.659\n",
       "2        cascVI        0.784       0.912        0.646\n",
       "3  cascVI + Avg        0.774       0.891        0.637\n",
       "4          scVI        0.769       0.888        0.632"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.747</td>\n      <td>0.881</td>\n      <td>0.653</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MP Oracle</td>\n      <td>0.795</td>\n      <td>0.929</td>\n      <td>0.659</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI</td>\n      <td>0.784</td>\n      <td>0.912</td>\n      <td>0.646</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cascVI + Avg</td>\n      <td>0.774</td>\n      <td>0.891</td>\n      <td>0.637</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>scVI</td>\n      <td>0.769</td>\n      <td>0.888</td>\n      <td>0.632</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "print(\">>> Sample-Sample | CPM Normalization <<<\")\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.627       0.729        0.537\n",
       "1     MP Oracle        0.695       0.805        0.564\n",
       "2        cascVI        0.601       0.629        0.471\n",
       "3  cascVI + Avg        0.590       0.609        0.461\n",
       "4          scVI        0.593       0.611        0.465"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.627</td>\n      <td>0.729</td>\n      <td>0.537</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MP Oracle</td>\n      <td>0.695</td>\n      <td>0.805</td>\n      <td>0.564</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI</td>\n      <td>0.601</td>\n      <td>0.629</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cascVI + Avg</td>\n      <td>0.590</td>\n      <td>0.609</td>\n      <td>0.461</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>scVI</td>\n      <td>0.593</td>\n      <td>0.611</td>\n      <td>0.465</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | No Normalization <<<\")\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.610       0.696        0.491\n",
       "1     MP Oracle        0.677       0.769        0.530\n",
       "2        cascVI        0.664       0.749        0.518\n",
       "3  cascVI + Avg        0.651       0.727        0.505\n",
       "4          scVI        0.650       0.724        0.504"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.610</td>\n      <td>0.696</td>\n      <td>0.491</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MP Oracle</td>\n      <td>0.677</td>\n      <td>0.769</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI</td>\n      <td>0.664</td>\n      <td>0.749</td>\n      <td>0.518</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cascVI + Avg</td>\n      <td>0.651</td>\n      <td>0.727</td>\n      <td>0.505</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>scVI</td>\n      <td>0.650</td>\n      <td>0.724</td>\n      <td>0.504</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | CPM Normalization <<<\")\n",
    "df_table4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | Rank Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.627       0.627        0.537\n",
       "1     MP Oracle        0.695       0.695        0.564\n",
       "2        cascVI        0.601       0.601        0.471\n",
       "3  cascVI + Avg        0.590       0.590        0.461\n",
       "4          scVI        0.593       0.593        0.465"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.627</td>\n      <td>0.627</td>\n      <td>0.537</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MP Oracle</td>\n      <td>0.695</td>\n      <td>0.695</td>\n      <td>0.564</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI</td>\n      <td>0.601</td>\n      <td>0.601</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cascVI + Avg</td>\n      <td>0.590</td>\n      <td>0.590</td>\n      <td>0.461</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>scVI</td>\n      <td>0.593</td>\n      <td>0.593</td>\n      <td>0.465</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | Rank Normalization <<<\")\n",
    "df_table5.head(10)"
   ]
  },
  {
   "source": [
    "# 7. Variance analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Experiement 2.a.iii: log variance vs depth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 8. Latent Space Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***cascVI***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge internal nodes and leaves\n",
    "#full_cascvi_latent = construct_latent(tree, cascvi_latent, imputed_z)\n",
    "\n",
    "\n",
    "#print(\"CascVI latent space\")\n",
    "#plot_common_ancestor(tree,\n",
    "#                     full_cascvi_latent,\n",
    "#                     embedding='umap',\n",
    "#                     give_labels=False\n",
    "#                             )"
   ]
  },
  {
   "source": [
    "***CascVI + avg***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_cascvi_latent_2 = construct_latent(tree, cascvi_latent, imputed_cascvi_2_z)\n",
    "\n",
    "#print(\"CascVI + averaging latent space\")\n",
    "#plot_common_ancestor(tree,\n",
    "#                     full_cascvi_latent_2,\n",
    "#                     embedding='umap',\n",
    "#                     give_labels=False\n",
    "#                             )"
   ]
  },
  {
   "source": [
    "***scVI***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge internal nodes and leaves\n",
    "#full_scvi_latent = construct_latent(tree, scvi_latent, imputed_scvi_2_z)\n",
    "\n",
    "#print(\"scVI latent space\")\n",
    "#plot_common_ancestor(tree,\n",
    " #                full_scvi_latent,\n",
    " #                embedding='umap',\n",
    " #                give_labels=False\n",
    " #                   )"
   ]
  },
  {
   "source": [
    "### k-NN purity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***LEAVES only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Leaves Only\")\n",
    "#data = {'groundtruth': leaves_z, 'scVI': scvi_latent,\n",
    "#        'cascVI': cascvi_latent\n",
    "#        }\n",
    "#scores = knn_purity(max_neighbors=50,\n",
    "#                    data=data,\n",
    "#                    plot=True,\n",
    "#                    save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp'\n",
    "#                    )"
   ]
  },
  {
   "source": [
    "*** Internal nodes only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Internal nodes Only\")\n",
    "#internal_z, internal_idx, internal_mu = get_internal(glm.z, glm.mu, tree)\n",
    "#internal_scvi_z, _, _ = get_internal(full_scvi_latent, glm.mu, tree)\n",
    "#internal_cascvi_z, _, _ = get_internal(full_cascvi_latent, glm.mu, tree)\n",
    "#internal_cascvi_z_2, _, _ = get_internal(full_cascvi_latent_2, glm.mu, tree)\n",
    "\n",
    "#data = {'groundtruth': internal_z, 'scVI + avg': internal_scvi_z,\n",
    "#        'cascVI': internal_cascvi_z, 'cascVI + avg': internal_cascvi_z_2\n",
    "#        }\n",
    "\n",
    "#scores = knn_purity(max_neighbors=50,\n",
    "#              data=data,\n",
    "#              plot=True,\n",
    "#              save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp/'\n",
    "#              )"
   ]
  },
  {
   "source": [
    "***Full tree***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Full tree\")\n",
    "#data = {'groundtruth': glm.z, 'scVI + avg': full_scvi_latent,\n",
    "#        'cascVI': full_cascvi_latent, 'cascVI + avg': full_cascvi_latent_2\n",
    "#        }\n",
    "#scores = knn_purity(max_neighbors=50,\n",
    "#              data=data,\n",
    "#              plot=True)"
   ]
  },
  {
   "source": [
    "***Stratified k-NN purity***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'groundtruth': glm.z, 'scVI + avg': full_scvi_latent,\n",
    "#        'cascVI': full_cascvi_latent, 'cascVI + avg': full_cascvi_latent_2\n",
    "#        }\n",
    "\n",
    "#for k in [2, 5, 10, 20, 35, 50]:\n",
    "#    print(\"For {} neighbors\".format(k))\n",
    "#    if k == 10:\n",
    "#        min_depth = 3\n",
    "#    elif k == 20:\n",
    "#        min_depth = 4\n",
    "#    elif k == 35:\n",
    "#        min_depth = 6\n",
    "#    elif k == 50:\n",
    "#        min_depth = 7\n",
    "#    else:\n",
    "#        min_depth = 2\n",
    "#    scores = knn_purity_stratified(n_neighbors=k,\n",
    "#                                   tree=tree,\n",
    "#                                   data=data,\n",
    "#                                   min_depth=min_depth,\n",
    "#                                   plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7.10 64-bit ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}