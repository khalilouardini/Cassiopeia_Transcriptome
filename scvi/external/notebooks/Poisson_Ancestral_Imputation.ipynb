{
 "cells": [
  {
   "source": [
    "# 0. Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/external\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***import ete3 Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/newick_objects/500cells/no_fitness/topology0.nwk\"\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "# Renaming nodes with levelorder indexing\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)\n",
    "    n.name = str(i)\n",
    "\n",
    "k = 1\n",
    "branch_length = {}\n",
    "for node in tree.traverse('levelorder'):\n",
    "    if node.name == '0':\n",
    "        branch_length[node.name] = 0.0\n",
    "        continue\n",
    "    branch_length[node.name] = k * node.dist\n",
    "branch_length['prior_root'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.poisson_glm import Poisson_GLM\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from external.models.vae import VAE\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from external.inference.inference import UnsupervisedTrainer\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.metrics import ks_pvalue, accuracy_imputation, correlations, knn_purity, knn_purity_stratified\n",
    "from external.utils.plots_util import plot_histograms, plot_scatter_mean, plot_ecdf_ks, plot_density\n",
    "from external.utils.plots_util import plot_losses, plot_elbo, plot_common_ancestor, plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import avg_weighted_baseline, scvi_baseline, scvi_baseline_z, cascvi_baseline_z, avg_baseline_z, construct_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulations (Poisson GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "g = 100\n",
    "vis = False\n",
    "leaves_only = False\n",
    "# Inverse dispersion parameter for negative binomial simulation\n",
    "alpha=1.0\n",
    "\n",
    "glm = Poisson_GLM(tree, g, d, vis, leaves_only, branch_length, alpha)\n",
    "\n",
    "glm.simulate_latent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate gene expression count data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of full Gene expression matrix :(1000, 99)\n"
     ]
    }
   ],
   "source": [
    "glm.simulate_ge(negative_binomial=False)\n",
    "# Quality Control (i.e Gene Filtering)\n",
    "glm.gene_qc()\n",
    "\n",
    "print(\"shape of full Gene expression matrix :{}\".format(glm.X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Binomial thinning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts before binomial thinning: 0.4025858585858586\nProportion of dropouts after binomial thinning: 0.8377474747474748\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts before binomial thinning: {}\".format(np.mean(glm.X == 0)))\n",
    "glm.binomial_thinning(p=0.1)\n",
    "print(\"Proportion of dropouts after binomial thinning: {}\".format(np.mean(glm.X == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split dataset in leaves/internal nodes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 99), (500, 99), (500, 99), (500, 99), (500, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Latent vectors\n",
    "leaves_z, _, _ = get_leaves(glm.z, glm.mu, tree)\n",
    "\n",
    "#FIXED training set\n",
    "leaves_X, leaves_idx, mu = get_leaves(glm.X, glm.mu, tree)\n",
    "\n",
    "# internal nodes data (for imputation)\n",
    "internal_X, internal_idx, internal_mu = get_internal(glm.X, glm.mu, tree)\n",
    "\n",
    "leaves_X.shape, mu.shape, internal_X.shape, internal_mu.shape, leaves_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# anndata + gene and celle filtering\n",
    "adata = AnnData(leaves_X)\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]\n",
    "adata.obs_names = [n.name for n in leaves]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "tree_bis = copy.deepcopy(tree)\n",
    "scvi_dataset = AnnDatasetFromAnnData(adata, filtering=False)\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', adata.obs_names)\n",
    "cas_dataset = TreeDataset(scvi_dataset, tree=tree_bis, filtering=False)\n",
    "cas_dataset\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = True\n",
    "use_MP = True\n",
    "ldvae = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=use_MP\n",
    "             )"
   ]
  },
  {
   "source": [
    "***Set freeze=True to set treeVAE decoder to the true linear decoder used in the simulations***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "freeze = False\n",
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in treevae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "lambda_ = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_leaves:  [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499]]\ntest_leaves:  []\nvalidation leaves:  []\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "training:  85%|████████▌ | 850/1000 [02:39<00:26,  5.74it/s]Encodings MP Likelihood: 2.7463123310357673\n",
      "ELBO Loss: 336.73234241717694\n",
      "training:  85%|████████▌ | 851/1000 [02:39<00:25,  5.77it/s]Encodings MP Likelihood: 3.099520838667402\n",
      "ELBO Loss: 337.2308047584422\n",
      "training:  85%|████████▌ | 852/1000 [02:39<00:25,  5.79it/s]Encodings MP Likelihood: 3.264432632881722\n",
      "ELBO Loss: 337.422221495124\n",
      "training:  85%|████████▌ | 853/1000 [02:40<00:25,  5.81it/s]Encodings MP Likelihood: 3.3498932028033472\n",
      "ELBO Loss: 337.39394277172056\n",
      "training:  85%|████████▌ | 854/1000 [02:40<00:25,  5.83it/s]Encodings MP Likelihood: 2.73421659805315\n",
      "ELBO Loss: 336.66126181040016\n",
      "training:  86%|████████▌ | 855/1000 [02:40<00:24,  5.84it/s]Encodings MP Likelihood: 3.4467494695228797\n",
      "ELBO Loss: 337.78747749460524\n",
      "training:  86%|████████▌ | 856/1000 [02:40<00:24,  5.84it/s]Encodings MP Likelihood: 2.9927994920736394\n",
      "ELBO Loss: 337.5025317505975\n",
      "training:  86%|████████▌ | 857/1000 [02:40<00:24,  5.84it/s]Encodings MP Likelihood: 3.295414067399499\n",
      "ELBO Loss: 337.6379402544291\n",
      "training:  86%|████████▌ | 858/1000 [02:40<00:24,  5.85it/s]Encodings MP Likelihood: 3.168926732204261\n",
      "ELBO Loss: 337.6327978487164\n",
      "training:  86%|████████▌ | 859/1000 [02:41<00:24,  5.80it/s]Encodings MP Likelihood: 3.0471691711161792\n",
      "ELBO Loss: 337.34056535378545\n",
      "training:  86%|████████▌ | 860/1000 [02:41<00:24,  5.81it/s]Encodings MP Likelihood: 3.3623043298869986\n",
      "ELBO Loss: 337.4386540868282\n",
      "training:  86%|████████▌ | 861/1000 [02:41<00:23,  5.83it/s]Encodings MP Likelihood: 2.981924969699352\n",
      "ELBO Loss: 336.4420887033414\n",
      "training:  86%|████████▌ | 862/1000 [02:41<00:23,  5.82it/s]Encodings MP Likelihood: 2.922764055341211\n",
      "ELBO Loss: 336.5816931482997\n",
      "training:  86%|████████▋ | 863/1000 [02:41<00:23,  5.78it/s]Encodings MP Likelihood: 2.9583137741222383\n",
      "ELBO Loss: 336.8095567386862\n",
      "training:  86%|████████▋ | 864/1000 [02:41<00:23,  5.79it/s]Encodings MP Likelihood: 3.2373870951427444\n",
      "ELBO Loss: 336.86277356889855\n",
      "training:  86%|████████▋ | 865/1000 [02:42<00:23,  5.81it/s]Encodings MP Likelihood: 3.318671036617387\n",
      "ELBO Loss: 337.80106287222947\n",
      "training:  87%|████████▋ | 866/1000 [02:42<00:22,  5.83it/s]Encodings MP Likelihood: 3.195012013024299\n",
      "ELBO Loss: 336.844939392972\n",
      "training:  87%|████████▋ | 867/1000 [02:42<00:22,  5.84it/s]Encodings MP Likelihood: 3.04728389563096\n",
      "ELBO Loss: 336.87329565209893\n",
      "training:  87%|████████▋ | 868/1000 [02:42<00:22,  5.84it/s]Encodings MP Likelihood: 2.776884731140855\n",
      "ELBO Loss: 336.0629589797958\n",
      "training:  87%|████████▋ | 869/1000 [02:42<00:22,  5.85it/s]Encodings MP Likelihood: 3.0489371286107874\n",
      "ELBO Loss: 336.6659467239164\n",
      "training:  87%|████████▋ | 870/1000 [02:42<00:22,  5.85it/s]Encodings MP Likelihood: 2.8812085346387883\n",
      "ELBO Loss: 336.48253834455505\n",
      "training:  87%|████████▋ | 871/1000 [02:43<00:22,  5.72it/s]Encodings MP Likelihood: 2.991268796388842\n",
      "ELBO Loss: 336.56747047054336\n",
      "training:  87%|████████▋ | 872/1000 [02:43<00:22,  5.70it/s]Encodings MP Likelihood: 3.179169326961204\n",
      "ELBO Loss: 336.6431851531866\n",
      "training:  87%|████████▋ | 873/1000 [02:43<00:22,  5.75it/s]Encodings MP Likelihood: 3.075938428342972\n",
      "ELBO Loss: 336.5749038276281\n",
      "training:  87%|████████▋ | 874/1000 [02:43<00:21,  5.76it/s]Encodings MP Likelihood: 3.2163551052002624\n",
      "ELBO Loss: 337.05588123500206\n",
      "training:  88%|████████▊ | 875/1000 [02:43<00:21,  5.78it/s]Encodings MP Likelihood: 2.8633455459055503\n",
      "ELBO Loss: 336.36349642164924\n",
      "training:  88%|████████▊ | 876/1000 [02:43<00:21,  5.80it/s]Encodings MP Likelihood: 2.8939775484338326\n",
      "ELBO Loss: 335.71640406632685\n",
      "training:  88%|████████▊ | 877/1000 [02:44<00:21,  5.79it/s]Encodings MP Likelihood: 3.1055226433595595\n",
      "ELBO Loss: 336.26341856653704\n",
      "training:  88%|████████▊ | 878/1000 [02:44<00:21,  5.80it/s]Encodings MP Likelihood: 2.723791283289607\n",
      "ELBO Loss: 336.0740148357968\n",
      "training:  88%|████████▊ | 879/1000 [02:44<00:20,  5.81it/s]Encodings MP Likelihood: 3.2596696478990848\n",
      "ELBO Loss: 336.62869842637383\n",
      "training:  88%|████████▊ | 880/1000 [02:44<00:20,  5.82it/s]Encodings MP Likelihood: 2.9543299834761902\n",
      "ELBO Loss: 336.22900562887423\n",
      "training:  88%|████████▊ | 881/1000 [02:44<00:21,  5.48it/s]Encodings MP Likelihood: 2.97485291716445\n",
      "ELBO Loss: 335.8120868397496\n",
      "training:  88%|████████▊ | 882/1000 [02:45<00:22,  5.25it/s]Encodings MP Likelihood: 2.9918169177161658\n",
      "ELBO Loss: 336.5047672728611\n",
      "training:  88%|████████▊ | 883/1000 [02:45<00:23,  5.06it/s]Encodings MP Likelihood: 3.206126187918848\n",
      "ELBO Loss: 336.48255072077933\n",
      "training:  88%|████████▊ | 884/1000 [02:45<00:23,  4.95it/s]Encodings MP Likelihood: 3.100235547981533\n",
      "ELBO Loss: 335.9756345377298\n",
      "training:  88%|████████▊ | 885/1000 [02:45<00:23,  4.90it/s]Encodings MP Likelihood: 3.2384681405729445\n",
      "ELBO Loss: 336.42456984096856\n",
      "training:  89%|████████▊ | 886/1000 [02:45<00:22,  5.15it/s]Encodings MP Likelihood: 3.166118756425215\n",
      "ELBO Loss: 336.1200779750809\n",
      "training:  89%|████████▊ | 887/1000 [02:46<00:21,  5.35it/s]Encodings MP Likelihood: 3.2104160039731537\n",
      "ELBO Loss: 335.7558567440618\n",
      "training:  89%|████████▉ | 888/1000 [02:46<00:20,  5.48it/s]Encodings MP Likelihood: 3.0676193469643414\n",
      "ELBO Loss: 336.1793671538047\n",
      "training:  89%|████████▉ | 889/1000 [02:46<00:19,  5.59it/s]Encodings MP Likelihood: 3.0315440103105047\n",
      "ELBO Loss: 335.6191978516288\n",
      "training:  89%|████████▉ | 890/1000 [02:46<00:19,  5.67it/s]Encodings MP Likelihood: 3.047328435875933\n",
      "ELBO Loss: 335.9410106712748\n",
      "training:  89%|████████▉ | 891/1000 [02:46<00:19,  5.72it/s]Encodings MP Likelihood: 2.9312367508105486\n",
      "ELBO Loss: 335.24783398702834\n",
      "training:  89%|████████▉ | 892/1000 [02:46<00:18,  5.76it/s]Encodings MP Likelihood: 3.0133399962637206\n",
      "ELBO Loss: 336.11800391345935\n",
      "training:  89%|████████▉ | 893/1000 [02:47<00:19,  5.47it/s]Encodings MP Likelihood: 2.893855960078908\n",
      "ELBO Loss: 335.9509310355931\n",
      "training:  89%|████████▉ | 894/1000 [02:47<00:19,  5.45it/s]Encodings MP Likelihood: 3.2357895611863414\n",
      "ELBO Loss: 335.42352662390687\n",
      "training:  90%|████████▉ | 895/1000 [02:47<00:19,  5.50it/s]Encodings MP Likelihood: 3.0430246839670563\n",
      "ELBO Loss: 335.6613619050133\n",
      "training:  90%|████████▉ | 896/1000 [02:47<00:18,  5.60it/s]Encodings MP Likelihood: 2.8339067323913523\n",
      "ELBO Loss: 335.4077728757766\n",
      "training:  90%|████████▉ | 897/1000 [02:47<00:18,  5.68it/s]Encodings MP Likelihood: 2.9553342845242754\n",
      "ELBO Loss: 335.8445749892819\n",
      "training:  90%|████████▉ | 898/1000 [02:47<00:17,  5.74it/s]Encodings MP Likelihood: 3.3035354500179612\n",
      "ELBO Loss: 335.89802426536295\n",
      "training:  90%|████████▉ | 899/1000 [02:48<00:17,  5.79it/s]Encodings MP Likelihood: 2.9748527092891246\n",
      "ELBO Loss: 335.10031500725785\n",
      "training:  90%|█████████ | 900/1000 [02:48<00:19,  5.14it/s]Encodings MP Likelihood: 3.035314277271541\n",
      "ELBO Loss: 335.9083296959638\n",
      "training:  90%|█████████ | 901/1000 [02:48<00:18,  5.34it/s]Encodings MP Likelihood: 3.0209402243588537\n",
      "ELBO Loss: 335.2951937037262\n",
      "training:  90%|█████████ | 902/1000 [02:48<00:17,  5.49it/s]Encodings MP Likelihood: 3.110864346198077\n",
      "ELBO Loss: 335.56083850226196\n",
      "training:  90%|█████████ | 903/1000 [02:48<00:17,  5.60it/s]Encodings MP Likelihood: 3.1345455854763706\n",
      "ELBO Loss: 336.2040657836779\n",
      "training:  90%|█████████ | 904/1000 [02:49<00:16,  5.68it/s]Encodings MP Likelihood: 3.0174715529134972\n",
      "ELBO Loss: 335.2843580065253\n",
      "training:  90%|█████████ | 905/1000 [02:49<00:16,  5.74it/s]Encodings MP Likelihood: 3.265674432903813\n",
      "ELBO Loss: 335.9029466657467\n",
      "training:  91%|█████████ | 906/1000 [02:49<00:16,  5.73it/s]Encodings MP Likelihood: 3.324590067802141\n",
      "ELBO Loss: 335.9993374465527\n",
      "training:  91%|█████████ | 907/1000 [02:49<00:16,  5.75it/s]Encodings MP Likelihood: 3.1716886650112914\n",
      "ELBO Loss: 336.10301214988897\n",
      "training:  91%|█████████ | 908/1000 [02:49<00:16,  5.73it/s]Encodings MP Likelihood: 3.1407416910021677\n",
      "ELBO Loss: 335.36943384753596\n",
      "training:  91%|█████████ | 909/1000 [02:49<00:16,  5.46it/s]Encodings MP Likelihood: 3.1461299802860982\n",
      "ELBO Loss: 335.72714949198735\n",
      "training:  91%|█████████ | 910/1000 [02:50<00:16,  5.46it/s]Encodings MP Likelihood: 2.9597190539625187\n",
      "ELBO Loss: 335.20967248062584\n",
      "training:  91%|█████████ | 911/1000 [02:50<00:17,  5.06it/s]Encodings MP Likelihood: 3.1026947747204567\n",
      "ELBO Loss: 335.0555297321394\n",
      "training:  91%|█████████ | 912/1000 [02:50<00:16,  5.27it/s]Encodings MP Likelihood: 2.9176516484242754\n",
      "ELBO Loss: 334.57057829071607\n",
      "training:  91%|█████████▏| 913/1000 [02:50<00:16,  5.40it/s]Encodings MP Likelihood: 3.1917762323910646\n",
      "ELBO Loss: 335.0744127989412\n",
      "training:  91%|█████████▏| 914/1000 [02:50<00:15,  5.52it/s]Encodings MP Likelihood: 3.0044353332515508\n",
      "ELBO Loss: 335.07081951599037\n",
      "training:  92%|█████████▏| 915/1000 [02:51<00:15,  5.59it/s]Encodings MP Likelihood: 2.9048342535355665\n",
      "ELBO Loss: 334.79870265271865\n",
      "training:  92%|█████████▏| 916/1000 [02:51<00:15,  5.35it/s]Encodings MP Likelihood: 3.1096007917775315\n",
      "ELBO Loss: 335.19528632756317\n",
      "training:  92%|█████████▏| 917/1000 [02:51<00:15,  5.34it/s]Encodings MP Likelihood: 3.025037191596171\n",
      "ELBO Loss: 334.91203647938096\n",
      "training:  92%|█████████▏| 918/1000 [02:51<00:15,  5.15it/s]Encodings MP Likelihood: 2.8935726953285643\n",
      "ELBO Loss: 335.1539411459726\n",
      "training:  92%|█████████▏| 919/1000 [02:51<00:15,  5.34it/s]Encodings MP Likelihood: 2.964577945934043\n",
      "ELBO Loss: 334.82001006406966\n",
      "training:  92%|█████████▏| 920/1000 [02:52<00:14,  5.48it/s]Encodings MP Likelihood: 3.160143707376378\n",
      "ELBO Loss: 334.9358741970853\n",
      "training:  92%|█████████▏| 921/1000 [02:52<00:14,  5.58it/s]Encodings MP Likelihood: 3.037096254337046\n",
      "ELBO Loss: 335.5347207586552\n",
      "training:  92%|█████████▏| 922/1000 [02:52<00:13,  5.62it/s]Encodings MP Likelihood: 2.970047610512123\n",
      "ELBO Loss: 334.9549406772551\n",
      "training:  92%|█████████▏| 923/1000 [02:52<00:13,  5.68it/s]Encodings MP Likelihood: 3.1631069719611316\n",
      "ELBO Loss: 334.74778262325185\n",
      "training:  92%|█████████▏| 924/1000 [02:52<00:13,  5.64it/s]Encodings MP Likelihood: 3.109707673040152\n",
      "ELBO Loss: 334.87907832367347\n",
      "training:  92%|█████████▎| 925/1000 [02:52<00:13,  5.46it/s]Encodings MP Likelihood: 3.128826370108373\n",
      "ELBO Loss: 334.4918593654735\n",
      "training:  93%|█████████▎| 926/1000 [02:53<00:13,  5.56it/s]Encodings MP Likelihood: 2.958547497392925\n",
      "ELBO Loss: 335.0863413033181\n",
      "training:  93%|█████████▎| 927/1000 [02:53<00:12,  5.66it/s]Encodings MP Likelihood: 3.099887922827894\n",
      "ELBO Loss: 334.728341878383\n",
      "training:  93%|█████████▎| 928/1000 [02:53<00:12,  5.72it/s]Encodings MP Likelihood: 3.1329997908361813\n",
      "ELBO Loss: 334.7354772206695\n",
      "training:  93%|█████████▎| 929/1000 [02:53<00:12,  5.76it/s]Encodings MP Likelihood: 3.136010504535247\n",
      "ELBO Loss: 334.42903712687234\n",
      "training:  93%|█████████▎| 930/1000 [02:53<00:12,  5.65it/s]Encodings MP Likelihood: 2.8811239789268264\n",
      "ELBO Loss: 334.0576427027669\n",
      "training:  93%|█████████▎| 931/1000 [02:53<00:12,  5.67it/s]Encodings MP Likelihood: 2.9893552792641658\n",
      "ELBO Loss: 334.39581242780343\n",
      "training:  93%|█████████▎| 932/1000 [02:54<00:11,  5.70it/s]Encodings MP Likelihood: 3.02980396566747\n",
      "ELBO Loss: 334.2128558789107\n",
      "training:  93%|█████████▎| 933/1000 [02:54<00:12,  5.37it/s]Encodings MP Likelihood: 3.0773908804460643\n",
      "ELBO Loss: 334.4649122910306\n",
      "training:  93%|█████████▎| 934/1000 [02:54<00:12,  5.20it/s]Encodings MP Likelihood: 3.142197573357935\n",
      "ELBO Loss: 334.54572840977545\n",
      "training:  94%|█████████▎| 935/1000 [02:54<00:12,  5.33it/s]Encodings MP Likelihood: 2.9336761205196513\n",
      "ELBO Loss: 334.22183796709965\n",
      "training:  94%|█████████▎| 936/1000 [02:54<00:11,  5.45it/s]Encodings MP Likelihood: 2.977229002019256\n",
      "ELBO Loss: 334.4357586438606\n",
      "training:  94%|█████████▎| 937/1000 [02:55<00:11,  5.54it/s]Encodings MP Likelihood: 3.0166860664781088\n",
      "ELBO Loss: 334.4020743435817\n",
      "training:  94%|█████████▍| 938/1000 [02:55<00:11,  5.60it/s]Encodings MP Likelihood: 3.049883464571073\n",
      "ELBO Loss: 334.5311767310883\n",
      "training:  94%|█████████▍| 939/1000 [02:55<00:10,  5.65it/s]Encodings MP Likelihood: 2.875028452425108\n",
      "ELBO Loss: 334.16094267469765\n",
      "training:  94%|█████████▍| 940/1000 [02:55<00:11,  5.28it/s]Encodings MP Likelihood: 3.0282786602617184\n",
      "ELBO Loss: 334.0543320258475\n",
      "training:  94%|█████████▍| 941/1000 [02:55<00:11,  5.10it/s]Encodings MP Likelihood: 3.1751973746461193\n",
      "ELBO Loss: 333.98333066020945\n",
      "training:  94%|█████████▍| 942/1000 [02:56<00:11,  5.00it/s]Encodings MP Likelihood: 3.1863912609580898\n",
      "ELBO Loss: 334.5175759858055\n",
      "training:  94%|█████████▍| 943/1000 [02:56<00:11,  5.17it/s]Encodings MP Likelihood: 3.077354276040347\n",
      "ELBO Loss: 334.3562786114324\n",
      "training:  94%|█████████▍| 944/1000 [02:56<00:10,  5.25it/s]Encodings MP Likelihood: 3.2279226504031926\n",
      "ELBO Loss: 334.13644407475743\n",
      "training:  94%|█████████▍| 945/1000 [02:56<00:10,  5.01it/s]Encodings MP Likelihood: 3.0363158119606264\n",
      "ELBO Loss: 334.0002285733114\n",
      "training:  95%|█████████▍| 946/1000 [02:56<00:10,  5.15it/s]Encodings MP Likelihood: 2.7870834375956246\n",
      "ELBO Loss: 333.61618218844444\n",
      "training:  95%|█████████▍| 947/1000 [02:57<00:09,  5.33it/s]Encodings MP Likelihood: 3.258860173929913\n",
      "ELBO Loss: 334.1966654969574\n",
      "training:  95%|█████████▍| 948/1000 [02:57<00:09,  5.47it/s]Encodings MP Likelihood: 3.086949899565215\n",
      "ELBO Loss: 333.99979363311206\n",
      "training:  95%|█████████▍| 949/1000 [02:57<00:09,  5.57it/s]Encodings MP Likelihood: 2.921342830379401\n",
      "ELBO Loss: 333.7082831134761\n",
      "training:  95%|█████████▌| 950/1000 [02:57<00:08,  5.64it/s]Encodings MP Likelihood: 3.1273905777483497\n",
      "ELBO Loss: 333.57799897327715\n",
      "training:  95%|█████████▌| 951/1000 [02:57<00:08,  5.69it/s]Encodings MP Likelihood: 3.062274065250205\n",
      "ELBO Loss: 333.9058835324277\n",
      "training:  95%|█████████▌| 952/1000 [02:57<00:08,  5.67it/s]Encodings MP Likelihood: 3.123716825633711\n",
      "ELBO Loss: 334.37839919720386\n",
      "training:  95%|█████████▌| 953/1000 [02:58<00:08,  5.38it/s]Encodings MP Likelihood: 2.8228537723008054\n",
      "ELBO Loss: 333.637469386092\n",
      "training:  95%|█████████▌| 954/1000 [02:58<00:08,  5.18it/s]Encodings MP Likelihood: 2.9310708571268727\n",
      "ELBO Loss: 333.8517830595516\n",
      "training:  96%|█████████▌| 955/1000 [02:58<00:08,  5.05it/s]Encodings MP Likelihood: 3.1717464453639197\n",
      "ELBO Loss: 334.0387114884112\n",
      "training:  96%|█████████▌| 956/1000 [02:58<00:08,  4.96it/s]Encodings MP Likelihood: 3.1322262804086813\n",
      "ELBO Loss: 333.3840612002169\n",
      "training:  96%|█████████▌| 957/1000 [02:58<00:08,  4.90it/s]Encodings MP Likelihood: 3.0394948703155964\n",
      "ELBO Loss: 333.7895265984983\n",
      "training:  96%|█████████▌| 958/1000 [02:59<00:08,  5.02it/s]Encodings MP Likelihood: 3.043955217328384\n",
      "ELBO Loss: 333.348613673325\n",
      "training:  96%|█████████▌| 959/1000 [02:59<00:07,  5.22it/s]Encodings MP Likelihood: 2.9349068091704216\n",
      "ELBO Loss: 333.21498798091704\n",
      "training:  96%|█████████▌| 960/1000 [02:59<00:07,  5.37it/s]Encodings MP Likelihood: 3.400374661523826\n",
      "ELBO Loss: 334.3254906578875\n",
      "training:  96%|█████████▌| 961/1000 [02:59<00:07,  5.46it/s]Encodings MP Likelihood: 3.331176271234399\n",
      "ELBO Loss: 333.9613196047552\n",
      "training:  96%|█████████▌| 962/1000 [02:59<00:06,  5.56it/s]Encodings MP Likelihood: 3.2887718825140717\n",
      "ELBO Loss: 333.3978845010397\n",
      "training:  96%|█████████▋| 963/1000 [02:59<00:06,  5.64it/s]Encodings MP Likelihood: 3.0283893657819307\n",
      "ELBO Loss: 333.51494664440077\n",
      "training:  96%|█████████▋| 964/1000 [03:00<00:06,  5.68it/s]Encodings MP Likelihood: 2.9230577897857484\n",
      "ELBO Loss: 332.85189581520575\n",
      "training:  96%|█████████▋| 965/1000 [03:00<00:06,  5.73it/s]Encodings MP Likelihood: 3.208080850284578\n",
      "ELBO Loss: 333.05093432749396\n",
      "training:  97%|█████████▋| 966/1000 [03:00<00:05,  5.77it/s]Encodings MP Likelihood: 3.086321660077836\n",
      "ELBO Loss: 333.8082267365376\n",
      "training:  97%|█████████▋| 967/1000 [03:00<00:05,  5.78it/s]Encodings MP Likelihood: 3.195476603661913\n",
      "ELBO Loss: 333.4373569306473\n",
      "training:  97%|█████████▋| 968/1000 [03:00<00:05,  5.76it/s]Encodings MP Likelihood: 3.235884566580399\n",
      "ELBO Loss: 333.43616788112644\n",
      "training:  97%|█████████▋| 969/1000 [03:01<00:05,  5.71it/s]Encodings MP Likelihood: 3.014242609538617\n",
      "ELBO Loss: 333.44827688847806\n",
      "training:  97%|█████████▋| 970/1000 [03:01<00:05,  5.74it/s]Encodings MP Likelihood: 3.094607428758278\n",
      "ELBO Loss: 333.30916465270315\n",
      "training:  97%|█████████▋| 971/1000 [03:01<00:05,  5.77it/s]Encodings MP Likelihood: 2.949610402225125\n",
      "ELBO Loss: 333.8691172762313\n",
      "training:  97%|█████████▋| 972/1000 [03:01<00:04,  5.79it/s]Encodings MP Likelihood: 2.6233551673514475\n",
      "ELBO Loss: 333.00251377230524\n",
      "training:  97%|█████████▋| 973/1000 [03:01<00:04,  5.81it/s]Encodings MP Likelihood: 3.0188768742001044\n",
      "ELBO Loss: 333.122725009298\n",
      "training:  97%|█████████▋| 974/1000 [03:01<00:04,  5.82it/s]Encodings MP Likelihood: 3.2910203568652636\n",
      "ELBO Loss: 332.93064580963363\n",
      "training:  98%|█████████▊| 975/1000 [03:02<00:04,  5.83it/s]Encodings MP Likelihood: 2.998751471406825\n",
      "ELBO Loss: 333.23184711066017\n",
      "training:  98%|█████████▊| 976/1000 [03:02<00:04,  5.83it/s]Encodings MP Likelihood: 3.159391498473239\n",
      "ELBO Loss: 332.85929528357065\n",
      "training:  98%|█████████▊| 977/1000 [03:02<00:03,  5.82it/s]Encodings MP Likelihood: 2.9932557783539475\n",
      "ELBO Loss: 334.19629214483314\n",
      "training:  98%|█████████▊| 978/1000 [03:02<00:03,  5.80it/s]Encodings MP Likelihood: 2.6969068810892014\n",
      "ELBO Loss: 332.42016818754627\n",
      "training:  98%|█████████▊| 979/1000 [03:02<00:03,  5.81it/s]Encodings MP Likelihood: 2.7775880566828337\n",
      "ELBO Loss: 333.29717123652904\n",
      "training:  98%|█████████▊| 980/1000 [03:02<00:03,  5.76it/s]Encodings MP Likelihood: 2.729081021925933\n",
      "ELBO Loss: 332.9649294374598\n",
      "training:  98%|█████████▊| 981/1000 [03:03<00:03,  5.75it/s]Encodings MP Likelihood: 2.9847070700917615\n",
      "ELBO Loss: 333.3386253601504\n",
      "training:  98%|█████████▊| 982/1000 [03:03<00:03,  5.78it/s]Encodings MP Likelihood: 2.8529945717056924\n",
      "ELBO Loss: 332.363116194799\n",
      "training:  98%|█████████▊| 983/1000 [03:03<00:02,  5.79it/s]Encodings MP Likelihood: 2.99453921383562\n",
      "ELBO Loss: 332.9430232348022\n",
      "training:  98%|█████████▊| 984/1000 [03:03<00:02,  5.74it/s]Encodings MP Likelihood: 3.1676486109584028\n",
      "ELBO Loss: 333.17852216551364\n",
      "training:  98%|█████████▊| 985/1000 [03:03<00:02,  5.76it/s]Encodings MP Likelihood: 2.878331144583663\n",
      "ELBO Loss: 332.93950577544274\n",
      "training:  99%|█████████▊| 986/1000 [03:03<00:02,  5.78it/s]Encodings MP Likelihood: 2.931950724694052\n",
      "ELBO Loss: 332.4081478311903\n",
      "training:  99%|█████████▊| 987/1000 [03:04<00:02,  5.77it/s]Encodings MP Likelihood: 2.8115470315621756\n",
      "ELBO Loss: 332.5562533419635\n",
      "training:  99%|█████████▉| 988/1000 [03:04<00:02,  5.79it/s]Encodings MP Likelihood: 2.9951136636499838\n",
      "ELBO Loss: 332.32566260923056\n",
      "training:  99%|█████████▉| 989/1000 [03:04<00:01,  5.78it/s]Encodings MP Likelihood: 3.049921396591979\n",
      "ELBO Loss: 332.6018380628496\n",
      "training:  99%|█████████▉| 990/1000 [03:04<00:01,  5.79it/s]Encodings MP Likelihood: 3.1318016084725566\n",
      "ELBO Loss: 332.76821921060287\n",
      "training:  99%|█████████▉| 991/1000 [03:04<00:01,  5.81it/s]Encodings MP Likelihood: 3.06542681642917\n",
      "ELBO Loss: 332.3539402712162\n",
      "training:  99%|█████████▉| 992/1000 [03:04<00:01,  5.81it/s]Encodings MP Likelihood: 3.3766425097174007\n",
      "ELBO Loss: 333.0827212629776\n",
      "training:  99%|█████████▉| 993/1000 [03:05<00:01,  5.74it/s]Encodings MP Likelihood: 2.906790581017239\n",
      "ELBO Loss: 332.32411768238006\n",
      "training:  99%|█████████▉| 994/1000 [03:05<00:01,  5.76it/s]Encodings MP Likelihood: 2.917215865390623\n",
      "ELBO Loss: 332.05951069283145\n",
      "training: 100%|█████████▉| 995/1000 [03:05<00:00,  5.78it/s]Encodings MP Likelihood: 3.156519940120766\n",
      "ELBO Loss: 332.2657588992836\n",
      "training: 100%|█████████▉| 996/1000 [03:05<00:00,  5.47it/s]Encodings MP Likelihood: 3.1657020459879646\n",
      "ELBO Loss: 332.43368560964564\n",
      "training: 100%|█████████▉| 997/1000 [03:05<00:00,  5.56it/s]Encodings MP Likelihood: 2.8737775664583376\n",
      "ELBO Loss: 332.1537699830356\n",
      "training: 100%|█████████▉| 998/1000 [03:06<00:00,  5.64it/s]Encodings MP Likelihood: 2.953704578293015\n",
      "ELBO Loss: 331.91987976947746\n",
      "training: 100%|█████████▉| 999/1000 [03:06<00:00,  5.70it/s]Encodings MP Likelihood: 3.0013541893820186\n",
      "ELBO Loss: 332.259208711458\n",
      "training: 100%|██████████| 1000/1000 [03:06<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior and MV imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance is 1.541032366102065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "error = mean_squared_error(full_posterior.get_latent(), leaves_z)\n",
    "print(\"the distance is {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Missing Value imputation By Posterior Predictive sampling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_l = np.mean(np.sum(glm.X, axis=1))\n",
    "\n",
    "# CascVI impitations\n",
    "imputed = {}\n",
    "imputed_z = {}\n",
    "imputed_mcmc_cov = {}\n",
    "imputed_gt = {}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        imputed[n.name], imputed_z[n.name] = full_posterior.imputation_internal(n,\n",
    "                                                            give_mean=False,\n",
    "                                                            library_size=empirical_l\n",
    "                                                           )\n",
    "        _, imputed_mcmc_cov[n.name] = full_posterior.mcmc_estimate(query_node=n,\n",
    "                                                                    n_samples=20\n",
    "                                                                    )\n",
    "        imputed_gt[n.name] = glm.X[n.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_X = [x for x in imputed.values()]\n",
    "imputed_X = np.array(imputed_X).reshape(-1, cas_dataset.X.shape[1])\n",
    "#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 1 (MP Oracle)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CascVI impitations\n",
    "#imputed_cascvi_1 = {}\n",
    "#imputed_cascvi_1_z ={}\n",
    "\n",
    "#for n in tree.traverse('levelorder'):\n",
    "#    if not n.is_leaf():\n",
    "#        _, imputed_cascvi_1_z[n.name] = full_posterior.imputation_internal(n,\n",
    "#                                                                    give_mean=False,\n",
    "#                                                                    library_size=empirical_l,\n",
    "#                                                                    known_latent=leaves_z\n",
    "#        )\n",
    "#        mu_z = np.clip(a=np.exp(glm.W @ imputed_cascvi_1_z[n.name].cpu().numpy() + glm.beta),\n",
    "#                        a_min=0,\n",
    "#                        a_max=1e8\n",
    "#                        )\n",
    "#        samples = np.array([np.random.poisson(mu_z) for i in range(100)])\n",
    "#        imputed_cascvi_1[n.name] = np.clip(a=np.mean(samples, axis=0),\n",
    "#                                           a_min=0,\n",
    "#                                           a_max=1e8\n",
    "#                                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 2 (Reconstruction of Averaged latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed_cascvi_2, imputed_cascvi_2_z = avg_baseline_z(tree=tree,\n",
    "#                                   model=treevae,\n",
    "#                                   posterior=full_posterior,\n",
    "#                                   weighted=False,\n",
    "#                                   n_samples_z=1,\n",
    "#                                   library_size=empirical_l,\n",
    "#                                   gaussian=False,\n",
    "#                                   use_cuda=True\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(326.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "full_posterior.compute_elbo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Unweighted Average of gene expression in Clade\n",
    "\n",
    "The simple idea here is to impute the value of an internal node, with the (un)weighted average of the gene expression values of the leaves, taking the query internal node as the root of the subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = False\n",
    "imputed_avg = avg_weighted_baseline(tree, weighted, glm.X, rounding=True)\n",
    "\n",
    "#get internal nodes\n",
    "avg_X = np.array([x for x in imputed_avg.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_avg_X, _, _ = get_internal(avg_X, glm.mu, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: (Un)weighted Average of decoded latent vectors, with scVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same averaging of the subtrees leaves in **Baseline 1**, only this time, the gene expression data is recovered with scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anndata\n",
    "gene_dataset = GeneExpressionDataset()\n",
    "gene_dataset.populate_from_data(leaves_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs =500\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=128,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=glm.latent,\n",
    "                  ldvae=ldvae\n",
    "              )"
   ]
  },
  {
   "source": [
    "***Set freeze=True to set treeVAE decoder to the true linear decoder used in the simulations***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in vae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 500/500 [00:34<00:00, 14.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "        \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "46.16246484375"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior()\n",
    "\n",
    "scvi_posterior.elbo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 2 (Decoded Average Latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_size = np.mean(np.sum(glm.X, axis=1))\n",
    "scvi_latent = np.array([scvi_posterior.get_latent(give_mean=False)[0] for i in range(10)])\n",
    "\n",
    "imputed_scvi_2, imputed_scvi_2_z = scvi_baseline_z(tree,\n",
    "                                        posterior=scvi_posterior,\n",
    "                                        model=vae,\n",
    "                                        weighted=False,\n",
    "                                        n_samples_z=1,\n",
    "                                        library_size=library_size,\n",
    "                                        use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "#cascvi_latent = full_posterior.get_latent()\n",
    "scvi_latent = scvi_posterior.get_latent()[0]\n",
    "\n",
    "scvi_latent.shape#, cascvi_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of scVI encodings:  -15505.747668543649\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(scvi_latent, cas_dataset.barcodes, scvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), scvi_latent.shape[1], False)\n",
    "mp_lik_scvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of scVI encodings: \", mp_lik_scvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  -518.9174727306946\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(cascvi_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of observations:  -1321.6885433836942\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(leaves_z, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of observations: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood Ratio: tensor(22892.6387, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood ratio\n",
    "lambda_ = (mp_lik_cascvi - mp_lik_scvi)\n",
    "print(\"Likelihood Ratio:\", lambda_)"
   ]
  },
  {
   "source": [
    "# Evaluation Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***Leaves variance cascVI***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz_v_cascvi = full_posterior.empirical_qz_v(n_samples=100,\n",
    "                                    norm=True\n",
    "                                    )"
   ]
  },
  {
   "source": [
    "***Leaves variance scVI***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = []\n",
    "for n in range(100):\n",
    "    latent.append(scvi_posterior.get_latent(give_mean=False)[0])\n",
    "latent = np.array(latent)\n",
    "\n",
    "qz_v_scvi = np.var(latent,\n",
    "                axis=0,\n",
    "                dtype=np.float64)\n",
    "\n",
    "qz_v_scvi = [np.linalg.norm(v) for v in qz_v_scvi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_leaves = np.mean(full_posterior.generate_leaves(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 991)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "reconstructed = scvi_posterior.generate()[0][:, :, 0]\n",
    "reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 991)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "reconstructed.shape"
   ]
  },
  {
   "source": [
    "reconstructed leaves"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "output = []\n",
    "columns = ['node ID', 'depth', 'MSE cascVI', 'MSE scVI', 'avg MSE', 'variance cascVI']\n",
    "idx=0\n",
    "idx_leaf = 0\n",
    "for n in tree.traverse('levelorder'):\n",
    "    depth = n.get_distance(tree)\n",
    "    if not n.is_leaf():\n",
    "        mse_scvi = mse(norm_scvi_X_2[idx], norm_internal_X[idx])\n",
    "        mse_cascvi = mse(norm_imputed_X[idx], norm_internal_X[idx])\n",
    "        mse_avg = mse(norm_avg_X[idx], norm_internal_X[idx])\n",
    "        output.append([n.name, depth, mse_cascvi, mse_scvi, mse_avg, np.linalg.norm(imputed_mcmc_cov[n.name])])\n",
    "        idx += 1\n",
    "    else:\n",
    "        mse_cascvi = mse(reconstructed_leaves[idx_leaf], leaves_X[idx_leaf])\n",
    "        mse_scvi = mse(reconstructed[idx_leaf], leaves_X[idx_leaf])\n",
    "        output.append([n.name, depth, mse_cascvi, mse_scvi, 0, qz_v_cascvi[idx_leaf]])\n",
    "        idx_leaf += 1\n",
    "df_cascvi = pd.DataFrame(data=output, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_cascvi' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e9af820ca5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_cascvi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cascvi' is not defined"
     ]
    }
   ],
   "source": [
    "df_cascvi[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cascvi.to_csv('poisson_stats.txt')"
   ]
  },
  {
   "source": [
    "# 6. Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CPM Normalization (for sample-sample correlation)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get imputations into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_scvi_X_2 = np.array([x for x in imputed_scvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "#internal_cascvi_X = np.array([x for x in imputed_cascvi_1.values()]).reshape(-1, glm.X.shape[1])\n",
    "#internal_cascvi_X_2 = np.array([x for x in imputed_cascvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "\n",
    "#internal_cascvi_X_2.shape, internal_scvi_X_2.shape, imputed_X.shape, internal_avg_X.shape, internal_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(500, 998)"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_internal_X = sc.pp.normalize_total(AnnData(internal_X), target_sum=1e6, inplace=False)['X'] \n",
    "norm_scvi_X_2 = sc.pp.normalize_total(AnnData(internal_scvi_X_2), target_sum=1e6, inplace=False)['X']\n",
    "norm_avg_X = sc.pp.normalize_total(AnnData(internal_avg_X), target_sum=1e6, inplace=False)['X']\n",
    "norm_imputed_X = sc.pp.normalize_total(AnnData(imputed_X), target_sum=1e6, inplace=False)['X']\n",
    "#norm_cascvi_X = sc.pp.normalize_total(AnnData(internal_cascvi_X), target_sum=1e6, inplace=False)['X']\n",
    "#norm_cascvi_X_2 = sc.pp.normalize_total(AnnData(internal_cascvi_X_2), target_sum=1e6, inplace=False)['X']\n",
    "\n",
    "norm_internal_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sample-Sample Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Sample-Sample correlation (Without Normalization)***\n",
    "\n",
    "We will use Scipy to compute a nonparametric rank correlation between the imputed and the groundtruth profiles. The correlation is based on the Spearman Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'groundtruth': internal_X.T, 'cascVI': imputed_X.T, 'scVI': internal_scvi_X_2.T,\n",
    "        'Average': internal_avg_X.T , 'cascVI + Avg': internal_cascvi_X_2.T}\n",
    "        #'MP Oracle': internal_cascvi_X.T\n",
    "        #}\n",
    "df1 = correlations(data, 'None', True)\n",
    "#df1.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Sample-Sample correlation (With ScanPy Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': norm_internal_X.T, 'cascVI': norm_imputed_X.T, 'scVI': norm_scvi_X_2.T, \n",
    "        'Average': norm_avg_X.T , 'cascVI + Avg': norm_cascvi_X_2.T}\n",
    "        #'MP Oracle': norm_cascvi_X.T\n",
    "        #}\n",
    "\n",
    "df2 = correlations(data, 'None', True)\n",
    "#df2.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II. Gene-Gene Correlations"
   ]
  },
  {
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2}\n",
    "        #,\n",
    "        #'MP Oracle': internal_cascvi_X\n",
    "        #}\n",
    "\n",
    "df3 = correlations(data, 'None', True)\n",
    "#df3.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2}\n",
    "        #'MP Oracle': norm_cascvi_X\n",
    "        #}\n",
    "\n",
    "df4 = correlations(data, 'None', True)\n",
    "#df4.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Gene-Gene correlation (With Rank Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "#data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "#        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2,\n",
    "#        'MP Oracle': norm_cascvi_X\n",
    "#        }\n",
    "\n",
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2}\n",
    "        #'MP Oracle': internal_cascvi_X\n",
    "        #}\n",
    "        \n",
    "df5 = correlations(data, 'rank', True)\n",
    "#df5.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Method\", \"Spearman CC\", \"Pearson CC\", \"Kendall Tau\"]\n",
    "data = [df1, df2, df3, df4, df5]\n",
    "#data = [df2, df4]\n",
    "\n",
    "data \n",
    "tables = [[] for i in range(len(data))]\n",
    "\n",
    "#task = [\"Sample-Sample (None)\", \"Sample-Sample (CPM)\", \"Gene-Gene (None)\", \n",
    "           #\"Gene-Gene(CPM)\", \"Gene-Gene (Rank)\" ]\n",
    "\n",
    "for (df, t) in zip(data, tables):\n",
    "    for m in np.unique(df.Method):\n",
    "        sub_df = np.round(df[df['Method'] == m].mean(), decimals=3)\n",
    "        t.append([m, sub_df['Spearman CC'], sub_df['Pearson CC'], sub_df['Kendall Tau']])\n",
    "        \n",
    "# Create and style Data Frames\n",
    "df_table1 = pd.DataFrame(tables[0], columns=columns)\n",
    "df_table2 = pd.DataFrame(tables[1], columns=columns)\n",
    "df_table3 = pd.DataFrame(tables[2], columns=columns)\n",
    "df_table4 = pd.DataFrame(tables[3], columns=columns)\n",
    "df_table5 = pd.DataFrame(tables[4], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " >>> Sample-Sample | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.497       0.814        0.479\n",
       "1        cascVI        0.499       0.845        0.415\n",
       "2  cascVI + Avg        0.493       0.837        0.415\n",
       "3          scVI        0.487       0.830        0.410"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.497</td>\n      <td>0.814</td>\n      <td>0.479</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.499</td>\n      <td>0.845</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.493</td>\n      <td>0.837</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.487</td>\n      <td>0.830</td>\n      <td>0.410</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "print(\" >>> Sample-Sample | No Normalization <<<\")\n",
    "df_table1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Sample-Sample | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.497       0.814        0.479\n",
       "1        cascVI        0.499       0.845        0.415\n",
       "2  cascVI + Avg        0.493       0.837        0.415\n",
       "3          scVI        0.487       0.830        0.410"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.497</td>\n      <td>0.814</td>\n      <td>0.479</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.499</td>\n      <td>0.845</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.493</td>\n      <td>0.837</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.487</td>\n      <td>0.830</td>\n      <td>0.410</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "print(\">>> Sample-Sample | CPM Normalization <<<\")\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.254       0.322        0.238\n",
       "1        cascVI        0.289       0.353        0.235\n",
       "2  cascVI + Avg        0.280       0.340        0.230\n",
       "3          scVI        0.274       0.330        0.226"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.254</td>\n      <td>0.322</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.289</td>\n      <td>0.353</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.280</td>\n      <td>0.340</td>\n      <td>0.230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.274</td>\n      <td>0.330</td>\n      <td>0.226</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | No Normalization <<<\")\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.238       0.268        0.213\n",
       "1        cascVI        0.296       0.379        0.232\n",
       "2  cascVI + Avg        0.288       0.367        0.227\n",
       "3          scVI        0.281       0.353        0.222"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.238</td>\n      <td>0.268</td>\n      <td>0.213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.296</td>\n      <td>0.379</td>\n      <td>0.232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.288</td>\n      <td>0.367</td>\n      <td>0.227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.281</td>\n      <td>0.353</td>\n      <td>0.222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | CPM Normalization <<<\")\n",
    "df_table4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | Rank Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.254       0.254        0.238\n",
       "1        cascVI        0.289       0.289        0.235\n",
       "2  cascVI + Avg        0.280       0.280        0.230\n",
       "3          scVI        0.274       0.274        0.226"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.254</td>\n      <td>0.254</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.289</td>\n      <td>0.289</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.280</td>\n      <td>0.280</td>\n      <td>0.230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.274</td>\n      <td>0.274</td>\n      <td>0.226</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | Rank Normalization <<<\")\n",
    "df_table5.head(10)"
   ]
  },
  {
   "source": [
    "# 8. Latent Space Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### k-NN purity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***LEAVES only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Leaves Only\n",
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.4698933815753866, 0.49297910553113355)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "print(\"Leaves Only\")\n",
    "data = {'groundtruth': leaves_z, 'scVI': scvi_latent,\n",
    "        'cascVI': cascvi_latent\n",
    "        }\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "                    data=data,\n",
    "                    plot=False,\n",
    "                    save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp_purtiy.png'\n",
    "                    )\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])"
   ]
  },
  {
   "source": [
    "*** Internal nodes only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Internal nodes Only\n",
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.44587087695578187, 0.5250488645477123)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "print(\"Internal nodes Only\")\n",
    "\n",
    "full_cascvi_latent = construct_latent(tree, cascvi_latent, imputed_z)\n",
    "full_scvi_latent = construct_latent(tree, scvi_latent, imputed_scvi_2_z)\n",
    "\n",
    "internal_z, internal_idx, internal_mu = get_internal(glm.z, glm.mu, tree)\n",
    "internal_scvi_z, _, _ = get_internal(full_scvi_latent, glm.mu, tree)\n",
    "internal_cascvi_z, _, _ = get_internal(full_cascvi_latent, glm.mu, tree)\n",
    "\n",
    "data = {'groundtruth': internal_z, 'scVI': internal_scvi_z,\n",
    "        'cascVI': internal_cascvi_z\n",
    "        }\n",
    "\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "                    data=data,\n",
    "                    plot=False,\n",
    "                    save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp_purtiy.png'\n",
    "                    )\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])"
   ]
  },
  {
   "source": [
    "***Full tree***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full tree\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.45329867389094985, 0.5212643774644873)"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "print(\"Full tree\")\n",
    "data = {'groundtruth': glm.z, 'scVI': full_scvi_latent,\n",
    "        'cascVI': full_cascvi_latent\n",
    "        }\n",
    "\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "              data=data,\n",
    "              plot=True)\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7  ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}