{
 "cells": [
  {
   "source": [
    "# 0. Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/external\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***import ete3 Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "#tree_name = \"/Users/khalilouardini/Desktop/projects/scVI/scvi/data/Cassiopeia_trees/3726_NT_T1_tree.processed.collapsed.tree\"\n",
    "#tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/Cassiopeia_trees/tree_test.txt\"\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/newick_objects/1000cells/high_fitness/topology8.nwk\"\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "#tree = Tree()\n",
    "#tree.populate(60)\n",
    "\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)\n",
    "    n.name = str(i)\n",
    "\n",
    "k = 1.0\n",
    "branch_length = {}\n",
    "for node in tree.traverse('levelorder'):\n",
    "    if node.name == '0':\n",
    "        branch_length[node.name] = 0.0\n",
    "        continue\n",
    "    branch_length[node.name] = k * node.dist\n",
    "branch_length['prior_root'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.poisson_glm import Poisson_GLM\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from models.vae import VAE\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from inference.inference import UnsupervisedTrainer\n",
    "from scvi.inference import posterior\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.metrics import ks_pvalue, accuracy_imputation, correlations, knn_purity, knn_purity_stratified\n",
    "from external.utils.plots_util import plot_histograms, plot_scatter_mean, plot_ecdf_ks, plot_density\n",
    "from external.utils.plots_util import plot_losses, plot_elbo, plot_common_ancestor, plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import avg_weighted_baseline, scvi_baseline, scvi_baseline_z, cascvi_baseline_z, avg_baseline_z, construct_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f494d92cc50>"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulations (Poisson GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-3a73f08925cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mglm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoisson_GLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = 10\n",
    "g = 1000\n",
    "vis = False\n",
    "leaves_only = False\n",
    "var = 1.0\n",
    "alpha=1.0\n",
    "\n",
    "glm = Poisson_GLM(tree, g, d, vis, leaves_only, branch_length, alpha)\n",
    "\n",
    "glm.simulate_latent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate gene expression count data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2000, 986), (1000, 10), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "glm.simulate_ge(negative_binomial=False)\n",
    "# Quality Control (i.e Gene Filtering)\n",
    "glm.gene_qc()\n",
    "\n",
    "glm.X.shape, glm.W.shape, glm.beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Binomial thinning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts: 0.40562221095334683\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts: {}\".format(np.mean(glm.X == 0)))\n",
    "#glm.binomial_thinning(p=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts after Binomial thinning: 0.8015294117647059\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of dropouts after Binomial thinning: {}\".format(np.mean(glm.X == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get the data and the indexes at the leaves***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 986), (1000, 986), (1000, 986), (1000, 986), (1000, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Latent vectors\n",
    "leaves_z, _, _ = get_leaves(glm.z, glm.mu, tree)\n",
    "\n",
    "#FIXED training set\n",
    "leaves_X, leaves_idx, mu = get_leaves(glm.X, glm.mu, tree)\n",
    "\n",
    "# internal nodes data (for imputation)\n",
    "internal_X, internal_idx, internal_mu = get_internal(glm.X, glm.mu, tree)\n",
    "\n",
    "leaves_X.shape, mu.shape, internal_X.shape, internal_mu.shape, leaves_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Simulated latent space***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_common_ancestor(tree,\n",
    "#                     glm.z,\n",
    "#                     embedding='umap',\n",
    "#                     give_labels=False\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# anndata + gene and celle filtering\n",
    "adata = AnnData(leaves_X)\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]\n",
    "adata.obs_names = [n.name for n in leaves]\n",
    "#sc.pp.filter_genes(adata, min_counts=3)\n",
    "#sc.pp.filter_cells(adata, min_counts=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "tree_bis = copy.deepcopy(tree)\n",
    "scvi_dataset = AnnDatasetFromAnnData(adata, filtering=False)\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', adata.obs_names)\n",
    "cas_dataset = TreeDataset(scvi_dataset, tree=tree_bis, filtering=False)\n",
    "cas_dataset\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = True\n",
    "use_MP = True\n",
    "ldvae = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=use_MP\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (px_decoder): FCLayers(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (px_scale_decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=986, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "freeze = False\n",
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        treevae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in treevae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "treevae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(treevae.decoder.factor_regressor.fc_layers[0][0].weight.numpy().all() == glm.W.T.all())\n",
    "#assert(treevae.decoder.factor_regressor.fc_layers[0][0].bias.numpy().all() == glm.beta.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Are we able to generate the gene expression data by decoding the simulated latent space?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance between the Poisson and the NB means is 3593.3991407286603\n"
     ]
    }
   ],
   "source": [
    "px_scale, px_rate, raw_px_scale = treevae.decoder(treevae.dispersion,\n",
    "                                        torch.from_numpy(leaves_z).float(),\n",
    "                                        torch.from_numpy(np.array([np.log(10000)])).float()\n",
    "                                       )\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if ldvae:\n",
    "    foo = np.clip(a=np.exp(raw_px_scale.detach().cpu().numpy()),\n",
    "            a_min=0,\n",
    "            a_max=1e8\n",
    "    )\n",
    "    mse = mean_squared_error(mu, foo)\n",
    "else:\n",
    "    mse = mean_squared_error(mu, px_rate.detach().numpy())\n",
    "\n",
    "print(\"the distance between the Poisson and the NB means is {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "lambda_ = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_leaves:  [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526], [527], [528], [529], [530], [531], [532], [533], [534], [535], [536], [537], [538], [539], [540], [541], [542], [543], [544], [545], [546], [547], [548], [549], [550], [551], [552], [553], [554], [555], [556], [557], [558], [559], [560], [561], [562], [563], [564], [565], [566], [567], [568], [569], [570], [571], [572], [573], [574], [575], [576], [577], [578], [579], [580], [581], [582], [583], [584], [585], [586], [587], [588], [589], [590], [591], [592], [593], [594], [595], [596], [597], [598], [599], [600], [601], [602], [603], [604], [605], [606], [607], [608], [609], [610], [611], [612], [613], [614], [615], [616], [617], [618], [619], [620], [621], [622], [623], [624], [625], [626], [627], [628], [629], [630], [631], [632], [633], [634], [635], [636], [637], [638], [639], [640], [641], [642], [643], [644], [645], [646], [647], [648], [649], [650], [651], [652], [653], [654], [655], [656], [657], [658], [659], [660], [661], [662], [663], [664], [665], [666], [667], [668], [669], [670], [671], [672], [673], [674], [675], [676], [677], [678], [679], [680], [681], [682], [683], [684], [685], [686], [687], [688], [689], [690], [691], [692], [693], [694], [695], [696], [697], [698], [699], [700], [701], [702], [703], [704], [705], [706], [707], [708], [709], [710], [711], [712], [713], [714], [715], [716], [717], [718], [719], [720], [721], [722], [723], [724], [725], [726], [727], [728], [729], [730], [731], [732], [733], [734], [735], [736], [737], [738], [739], [740], [741], [742], [743], [744], [745], [746], [747], [748], [749], [750], [751], [752], [753], [754], [755], [756], [757], [758], [759], [760], [761], [762], [763], [764], [765], [766], [767], [768], [769], [770], [771], [772], [773], [774], [775], [776], [777], [778], [779], [780], [781], [782], [783], [784], [785], [786], [787], [788], [789], [790], [791], [792], [793], [794], [795], [796], [797], [798], [799], [800], [801], [802], [803], [804], [805], [806], [807], [808], [809], [810], [811], [812], [813], [814], [815], [816], [817], [818], [819], [820], [821], [822], [823], [824], [825], [826], [827], [828], [829], [830], [831], [832], [833], [834], [835], [836], [837], [838], [839], [840], [841], [842], [843], [844], [845], [846], [847], [848], [849], [850], [851], [852], [853], [854], [855], [856], [857], [858], [859], [860], [861], [862], [863], [864], [865], [866], [867], [868], [869], [870], [871], [872], [873], [874], [875], [876], [877], [878], [879], [880], [881], [882], [883], [884], [885], [886], [887], [888], [889], [890], [891], [892], [893], [894], [895], [896], [897], [898], [899], [900], [901], [902], [903], [904], [905], [906], [907], [908], [909], [910], [911], [912], [913], [914], [915], [916], [917], [918], [919], [920], [921], [922], [923], [924], [925], [926], [927], [928], [929], [930], [931], [932], [933], [934], [935], [936], [937], [938], [939], [940], [941], [942], [943], [944], [945], [946], [947], [948], [949], [950], [951], [952], [953], [954], [955], [956], [957], [958], [959], [960], [961], [962], [963], [964], [965], [966], [967], [968], [969], [970], [971], [972], [973], [974], [975], [976], [977], [978], [979], [980], [981], [982], [983], [984], [985], [986], [987], [988], [989], [990], [991], [992], [993], [994], [995], [996], [997], [998], [999]]\ntest_leaves:  []\nvalidation leaves:  []\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "06535318055\n",
      "ELBO Loss: 505.41859936436174\n",
      "training:  85%|████████▌ | 851/1000 [05:44<00:58,  2.56it/s]Encodings MP Likelihood: 1.0544763044411032\n",
      "ELBO Loss: 506.60844305921796\n",
      "training:  85%|████████▌ | 852/1000 [05:45<00:57,  2.59it/s]Encodings MP Likelihood: 0.5582975589701028\n",
      "ELBO Loss: 505.950106439881\n",
      "training:  85%|████████▌ | 853/1000 [05:45<00:56,  2.62it/s]Encodings MP Likelihood: 0.46728642873497944\n",
      "ELBO Loss: 505.9311475998903\n",
      "training:  85%|████████▌ | 854/1000 [05:45<00:55,  2.64it/s]Encodings MP Likelihood: 0.6828820279402438\n",
      "ELBO Loss: 506.24594432187547\n",
      "training:  86%|████████▌ | 855/1000 [05:46<00:55,  2.63it/s]Encodings MP Likelihood: 1.0298745865796273\n",
      "ELBO Loss: 506.2867984632785\n",
      "training:  86%|████████▌ | 856/1000 [05:46<00:54,  2.64it/s]Encodings MP Likelihood: 0.681594781763408\n",
      "ELBO Loss: 505.7864821748761\n",
      "training:  86%|████████▌ | 857/1000 [05:47<00:54,  2.63it/s]Encodings MP Likelihood: 0.6217881578603625\n",
      "ELBO Loss: 506.0820698600068\n",
      "training:  86%|████████▌ | 858/1000 [05:47<00:54,  2.63it/s]Encodings MP Likelihood: 0.5949339861987107\n",
      "ELBO Loss: 505.9886634906549\n",
      "training:  86%|████████▌ | 859/1000 [05:47<00:53,  2.64it/s]Encodings MP Likelihood: 0.4541581423085877\n",
      "ELBO Loss: 506.21350784253474\n",
      "training:  86%|████████▌ | 860/1000 [05:48<00:53,  2.64it/s]Encodings MP Likelihood: 0.580433645328338\n",
      "ELBO Loss: 505.9654356623203\n",
      "training:  86%|████████▌ | 861/1000 [05:48<00:59,  2.32it/s]Encodings MP Likelihood: 0.6752698938775601\n",
      "ELBO Loss: 505.8708398251218\n",
      "training:  86%|████████▌ | 862/1000 [05:49<00:57,  2.41it/s]Encodings MP Likelihood: 0.760633870978443\n",
      "ELBO Loss: 505.69175870381105\n",
      "training:  86%|████████▋ | 863/1000 [05:49<00:55,  2.47it/s]Encodings MP Likelihood: 0.49997144253983933\n",
      "ELBO Loss: 505.90581243083756\n",
      "training:  86%|████████▋ | 864/1000 [05:49<00:53,  2.53it/s]Encodings MP Likelihood: 0.4625447943349193\n",
      "ELBO Loss: 505.73929052301696\n",
      "training:  86%|████████▋ | 865/1000 [05:50<00:51,  2.64it/s]Encodings MP Likelihood: 0.6060837755356915\n",
      "ELBO Loss: 505.40266027691484\n",
      "training:  87%|████████▋ | 866/1000 [05:50<00:51,  2.60it/s]Encodings MP Likelihood: 0.5915155699039222\n",
      "ELBO Loss: 505.93150030238166\n",
      "training:  87%|████████▋ | 867/1000 [05:51<00:51,  2.58it/s]Encodings MP Likelihood: 0.8602899020212679\n",
      "ELBO Loss: 506.260743024436\n",
      "training:  87%|████████▋ | 868/1000 [05:51<00:50,  2.61it/s]Encodings MP Likelihood: 0.6760115444294279\n",
      "ELBO Loss: 505.5935219945593\n",
      "training:  87%|████████▋ | 869/1000 [05:51<00:49,  2.64it/s]Encodings MP Likelihood: 0.9161711561512078\n",
      "ELBO Loss: 505.9290250241582\n",
      "training:  87%|████████▋ | 870/1000 [05:52<00:49,  2.64it/s]Encodings MP Likelihood: 0.707747533669584\n",
      "ELBO Loss: 505.933992946663\n",
      "training:  87%|████████▋ | 871/1000 [05:52<00:48,  2.64it/s]Encodings MP Likelihood: 0.8825465117479628\n",
      "ELBO Loss: 506.37828408627536\n",
      "training:  87%|████████▋ | 872/1000 [05:52<00:49,  2.60it/s]Encodings MP Likelihood: 0.3357081085792421\n",
      "ELBO Loss: 505.73795624195174\n",
      "training:  87%|████████▋ | 873/1000 [05:53<00:48,  2.62it/s]Encodings MP Likelihood: 0.7009932757400871\n",
      "ELBO Loss: 505.5953611027565\n",
      "training:  87%|████████▋ | 874/1000 [05:53<00:47,  2.63it/s]Encodings MP Likelihood: 0.922077362494891\n",
      "ELBO Loss: 505.8593776560045\n",
      "training:  88%|████████▊ | 875/1000 [05:54<00:47,  2.63it/s]Encodings MP Likelihood: 1.208303777463322\n",
      "ELBO Loss: 506.55336317386474\n",
      "training:  88%|████████▊ | 876/1000 [05:54<00:46,  2.64it/s]Encodings MP Likelihood: 0.30799136721173814\n",
      "ELBO Loss: 505.39994676219766\n",
      "training:  88%|████████▊ | 877/1000 [05:54<00:46,  2.65it/s]Encodings MP Likelihood: 0.7198695101693453\n",
      "ELBO Loss: 506.1140637173828\n",
      "training:  88%|████████▊ | 878/1000 [05:55<00:46,  2.63it/s]Encodings MP Likelihood: 0.6346619106150411\n",
      "ELBO Loss: 505.50119257005946\n",
      "training:  88%|████████▊ | 879/1000 [05:55<00:45,  2.65it/s]Encodings MP Likelihood: 0.8161089504745228\n",
      "ELBO Loss: 505.3454308638677\n",
      "training:  88%|████████▊ | 880/1000 [05:55<00:45,  2.66it/s]Encodings MP Likelihood: 0.702004591852552\n",
      "ELBO Loss: 505.770976275882\n",
      "training:  88%|████████▊ | 881/1000 [05:56<00:44,  2.68it/s]Encodings MP Likelihood: 0.6009404335027309\n",
      "ELBO Loss: 504.92672767862274\n",
      "training:  88%|████████▊ | 882/1000 [05:56<00:44,  2.66it/s]Encodings MP Likelihood: 0.8869368886141159\n",
      "ELBO Loss: 505.6925481759949\n",
      "training:  88%|████████▊ | 883/1000 [05:57<00:43,  2.67it/s]Encodings MP Likelihood: 0.476314399338151\n",
      "ELBO Loss: 505.22473012926685\n",
      "training:  88%|████████▊ | 884/1000 [05:57<00:43,  2.65it/s]Encodings MP Likelihood: 0.7123627023530748\n",
      "ELBO Loss: 505.4355007541022\n",
      "training:  88%|████████▊ | 885/1000 [05:57<00:43,  2.66it/s]Encodings MP Likelihood: 1.0096415997683383\n",
      "ELBO Loss: 506.0388954079985\n",
      "training:  89%|████████▊ | 886/1000 [05:58<00:42,  2.67it/s]Encodings MP Likelihood: 0.4807584934402859\n",
      "ELBO Loss: 505.44489868742136\n",
      "training:  89%|████████▊ | 887/1000 [05:58<00:43,  2.57it/s]Encodings MP Likelihood: 0.48369780839012466\n",
      "ELBO Loss: 505.4765364013778\n",
      "training:  89%|████████▉ | 888/1000 [05:59<00:43,  2.57it/s]Encodings MP Likelihood: 0.47350225987230654\n",
      "ELBO Loss: 505.219144005411\n",
      "training:  89%|████████▉ | 889/1000 [05:59<00:44,  2.51it/s]Encodings MP Likelihood: 0.4984476512098215\n",
      "ELBO Loss: 505.11305679750046\n",
      "training:  89%|████████▉ | 890/1000 [05:59<00:43,  2.54it/s]Encodings MP Likelihood: 0.4993338580730215\n",
      "ELBO Loss: 505.33786140138784\n",
      "training:  89%|████████▉ | 891/1000 [06:00<00:42,  2.58it/s]Encodings MP Likelihood: 0.361647408894402\n",
      "ELBO Loss: 505.0962455791179\n",
      "training:  89%|████████▉ | 892/1000 [06:00<00:43,  2.49it/s]Encodings MP Likelihood: 0.792672518770721\n",
      "ELBO Loss: 505.84907020926323\n",
      "training:  89%|████████▉ | 893/1000 [06:01<00:41,  2.55it/s]Encodings MP Likelihood: 0.9994860289916495\n",
      "ELBO Loss: 505.76198814149654\n",
      "training:  89%|████████▉ | 894/1000 [06:01<00:41,  2.57it/s]Encodings MP Likelihood: 0.7150019871554272\n",
      "ELBO Loss: 505.93777708678203\n",
      "training:  90%|████████▉ | 895/1000 [06:01<00:40,  2.57it/s]Encodings MP Likelihood: 0.4485270195842391\n",
      "ELBO Loss: 505.34636715996555\n",
      "training:  90%|████████▉ | 896/1000 [06:02<00:40,  2.59it/s]Encodings MP Likelihood: 0.6774992039478791\n",
      "ELBO Loss: 505.31209542732716\n",
      "training:  90%|████████▉ | 897/1000 [06:02<00:39,  2.61it/s]Encodings MP Likelihood: 0.6913325504346565\n",
      "ELBO Loss: 505.64056812121606\n",
      "training:  90%|████████▉ | 898/1000 [06:02<00:39,  2.58it/s]Encodings MP Likelihood: 0.721237545730057\n",
      "ELBO Loss: 505.0899066717606\n",
      "training:  90%|████████▉ | 899/1000 [06:03<00:42,  2.39it/s]Encodings MP Likelihood: 0.7114100138653362\n",
      "ELBO Loss: 505.0541162658714\n",
      "computing elbo\n",
      "training:  90%|█████████ | 900/1000 [06:04<00:50,  1.97it/s]Encodings MP Likelihood: 0.7674150744077858\n",
      "ELBO Loss: 504.94659948370446\n",
      "training:  90%|█████████ | 901/1000 [06:04<00:50,  1.98it/s]Encodings MP Likelihood: 0.7712096763202214\n",
      "ELBO Loss: 504.9877019694966\n",
      "training:  90%|█████████ | 902/1000 [06:05<00:49,  1.97it/s]Encodings MP Likelihood: 0.559734373949296\n",
      "ELBO Loss: 504.85478256170353\n",
      "training:  90%|█████████ | 903/1000 [06:05<00:49,  1.97it/s]Encodings MP Likelihood: 0.6930121264610353\n",
      "ELBO Loss: 504.6075501498905\n",
      "training:  90%|█████████ | 904/1000 [06:06<00:45,  2.11it/s]Encodings MP Likelihood: 0.3910851138509391\n",
      "ELBO Loss: 504.5167698852052\n",
      "training:  90%|█████████ | 905/1000 [06:06<00:42,  2.24it/s]Encodings MP Likelihood: 0.6152737235986885\n",
      "ELBO Loss: 504.8722987640166\n",
      "training:  91%|█████████ | 906/1000 [06:06<00:41,  2.27it/s]Encodings MP Likelihood: 0.33484754099300756\n",
      "ELBO Loss: 504.95735621928105\n",
      "training:  91%|█████████ | 907/1000 [06:07<00:39,  2.37it/s]Encodings MP Likelihood: 0.6416461190681396\n",
      "ELBO Loss: 505.03145209718133\n",
      "training:  91%|█████████ | 908/1000 [06:07<00:36,  2.51it/s]Encodings MP Likelihood: 0.18740003441709613\n",
      "ELBO Loss: 504.4403575905044\n",
      "training:  91%|█████████ | 909/1000 [06:07<00:35,  2.55it/s]Encodings MP Likelihood: 0.4596457477067058\n",
      "ELBO Loss: 504.8686299694785\n",
      "training:  91%|█████████ | 910/1000 [06:08<00:34,  2.58it/s]Encodings MP Likelihood: 0.3877555309191272\n",
      "ELBO Loss: 504.73038981079435\n",
      "training:  91%|█████████ | 911/1000 [06:08<00:34,  2.60it/s]Encodings MP Likelihood: 0.6725212854236237\n",
      "ELBO Loss: 504.9199499437854\n",
      "training:  91%|█████████ | 912/1000 [06:09<00:33,  2.62it/s]Encodings MP Likelihood: 0.8059872530552019\n",
      "ELBO Loss: 504.87451317083946\n",
      "training:  91%|█████████▏| 913/1000 [06:09<00:33,  2.63it/s]Encodings MP Likelihood: 0.9593218254043522\n",
      "ELBO Loss: 505.3644632813551\n",
      "training:  91%|█████████▏| 914/1000 [06:09<00:32,  2.64it/s]Encodings MP Likelihood: 0.3468790556245706\n",
      "ELBO Loss: 504.7500779944091\n",
      "training:  92%|█████████▏| 915/1000 [06:10<00:32,  2.64it/s]Encodings MP Likelihood: 0.730694676723853\n",
      "ELBO Loss: 504.889362862836\n",
      "training:  92%|█████████▏| 916/1000 [06:10<00:32,  2.61it/s]Encodings MP Likelihood: 0.5810258296049285\n",
      "ELBO Loss: 504.7224876091697\n",
      "training:  92%|█████████▏| 917/1000 [06:11<00:31,  2.61it/s]Encodings MP Likelihood: 0.7355445320118785\n",
      "ELBO Loss: 504.7114656754657\n",
      "training:  92%|█████████▏| 918/1000 [06:11<00:31,  2.62it/s]Encodings MP Likelihood: 0.6552016119414728\n",
      "ELBO Loss: 504.89451831918905\n",
      "training:  92%|█████████▏| 919/1000 [06:11<00:30,  2.62it/s]Encodings MP Likelihood: 0.10233503202505043\n",
      "ELBO Loss: 504.1935025220982\n",
      "training:  92%|█████████▏| 920/1000 [06:12<00:30,  2.62it/s]Encodings MP Likelihood: 0.40071620334957936\n",
      "ELBO Loss: 504.6275222544115\n",
      "training:  92%|█████████▏| 921/1000 [06:12<00:30,  2.59it/s]Encodings MP Likelihood: 0.5529314463222429\n",
      "ELBO Loss: 504.59598891629514\n",
      "training:  92%|█████████▏| 922/1000 [06:12<00:30,  2.59it/s]Encodings MP Likelihood: 0.6060831854295498\n",
      "ELBO Loss: 504.7026563569097\n",
      "training:  92%|█████████▏| 923/1000 [06:13<00:29,  2.60it/s]Encodings MP Likelihood: 0.47950087583822765\n",
      "ELBO Loss: 504.73823101021804\n",
      "training:  92%|█████████▏| 924/1000 [06:13<00:29,  2.61it/s]Encodings MP Likelihood: 0.6760529303298539\n",
      "ELBO Loss: 504.9783902124147\n",
      "training:  92%|█████████▎| 925/1000 [06:14<00:28,  2.61it/s]Encodings MP Likelihood: 0.9289214928123253\n",
      "ELBO Loss: 504.98799965953157\n",
      "training:  93%|█████████▎| 926/1000 [06:14<00:28,  2.62it/s]Encodings MP Likelihood: 0.4549116802242413\n",
      "ELBO Loss: 504.6696376362599\n",
      "training:  93%|█████████▎| 927/1000 [06:14<00:28,  2.60it/s]Encodings MP Likelihood: 0.4412447192995969\n",
      "ELBO Loss: 504.17249473975903\n",
      "training:  93%|█████████▎| 928/1000 [06:15<00:27,  2.60it/s]Encodings MP Likelihood: 0.3001996121487094\n",
      "ELBO Loss: 504.1708008081401\n",
      "training:  93%|█████████▎| 929/1000 [06:15<00:27,  2.60it/s]Encodings MP Likelihood: 0.7580502278013771\n",
      "ELBO Loss: 504.8302788084642\n",
      "training:  93%|█████████▎| 930/1000 [06:15<00:26,  2.65it/s]Encodings MP Likelihood: 0.5387565621386694\n",
      "ELBO Loss: 504.25430532834605\n",
      "training:  93%|█████████▎| 931/1000 [06:16<00:25,  2.71it/s]Encodings MP Likelihood: 0.7197968509291938\n",
      "ELBO Loss: 504.19938555137855\n",
      "training:  93%|█████████▎| 932/1000 [06:16<00:24,  2.76it/s]Encodings MP Likelihood: 0.7674445023239147\n",
      "ELBO Loss: 504.9144593097665\n",
      "training:  93%|█████████▎| 933/1000 [06:17<00:24,  2.77it/s]Encodings MP Likelihood: 0.5553673177261698\n",
      "ELBO Loss: 504.56343587512686\n",
      "training:  93%|█████████▎| 934/1000 [06:17<00:24,  2.72it/s]Encodings MP Likelihood: 0.731489956846182\n",
      "ELBO Loss: 504.6482257268494\n",
      "training:  94%|█████████▎| 935/1000 [06:17<00:24,  2.69it/s]Encodings MP Likelihood: 0.765554899440849\n",
      "ELBO Loss: 504.5666745649825\n",
      "training:  94%|█████████▎| 936/1000 [06:18<00:24,  2.66it/s]Encodings MP Likelihood: 0.5551142067642254\n",
      "ELBO Loss: 504.33703288334624\n",
      "training:  94%|█████████▎| 937/1000 [06:18<00:23,  2.66it/s]Encodings MP Likelihood: 0.47089962895797216\n",
      "ELBO Loss: 504.43642992631\n",
      "training:  94%|█████████▍| 938/1000 [06:18<00:23,  2.65it/s]Encodings MP Likelihood: 0.6262340374837081\n",
      "ELBO Loss: 504.47344327274914\n",
      "training:  94%|█████████▍| 939/1000 [06:19<00:23,  2.63it/s]Encodings MP Likelihood: 0.4066588396241487\n",
      "ELBO Loss: 503.9741360000037\n",
      "training:  94%|█████████▍| 940/1000 [06:19<00:22,  2.62it/s]Encodings MP Likelihood: 1.088399848873348\n",
      "ELBO Loss: 504.888942710652\n",
      "training:  94%|█████████▍| 941/1000 [06:20<00:22,  2.62it/s]Encodings MP Likelihood: 0.6389840548297293\n",
      "ELBO Loss: 504.25162457099265\n",
      "training:  94%|█████████▍| 942/1000 [06:20<00:22,  2.63it/s]Encodings MP Likelihood: 0.4517925722095916\n",
      "ELBO Loss: 504.18711895373036\n",
      "training:  94%|█████████▍| 943/1000 [06:20<00:21,  2.62it/s]Encodings MP Likelihood: 0.3293570040323401\n",
      "ELBO Loss: 504.1004997304484\n",
      "training:  94%|█████████▍| 944/1000 [06:21<00:21,  2.63it/s]Encodings MP Likelihood: 0.99690682471209\n",
      "ELBO Loss: 504.5229696414452\n",
      "training:  94%|█████████▍| 945/1000 [06:21<00:20,  2.64it/s]Encodings MP Likelihood: 0.9360501622392249\n",
      "ELBO Loss: 504.602436742833\n",
      "training:  95%|█████████▍| 946/1000 [06:21<00:20,  2.63it/s]Encodings MP Likelihood: 0.9245422015173029\n",
      "ELBO Loss: 504.6205296739472\n",
      "training:  95%|█████████▍| 947/1000 [06:22<00:20,  2.64it/s]Encodings MP Likelihood: 0.4551805415492408\n",
      "ELBO Loss: 504.3423388574981\n",
      "training:  95%|█████████▍| 948/1000 [06:22<00:19,  2.64it/s]Encodings MP Likelihood: 0.2972142264039399\n",
      "ELBO Loss: 503.77070280260097\n",
      "training:  95%|█████████▍| 949/1000 [06:23<00:19,  2.64it/s]Encodings MP Likelihood: 0.7831957004325045\n",
      "ELBO Loss: 504.46366504095676\n",
      "training:  95%|█████████▌| 950/1000 [06:23<00:18,  2.64it/s]Encodings MP Likelihood: 0.4918773637231664\n",
      "ELBO Loss: 504.0872506176559\n",
      "training:  95%|█████████▌| 951/1000 [06:23<00:18,  2.66it/s]Encodings MP Likelihood: 0.7053454102022778\n",
      "ELBO Loss: 504.0698701195617\n",
      "training:  95%|█████████▌| 952/1000 [06:24<00:18,  2.65it/s]Encodings MP Likelihood: 0.596183492835042\n",
      "ELBO Loss: 504.65838748029665\n",
      "training:  95%|█████████▌| 953/1000 [06:24<00:17,  2.66it/s]Encodings MP Likelihood: 0.559508274355198\n",
      "ELBO Loss: 504.18059057985243\n",
      "training:  95%|█████████▌| 954/1000 [06:25<00:17,  2.66it/s]Encodings MP Likelihood: 0.3755205028436447\n",
      "ELBO Loss: 503.99142956422895\n",
      "training:  96%|█████████▌| 955/1000 [06:25<00:16,  2.66it/s]Encodings MP Likelihood: 0.6132781518734187\n",
      "ELBO Loss: 503.76153018203536\n",
      "training:  96%|█████████▌| 956/1000 [06:25<00:16,  2.64it/s]Encodings MP Likelihood: 0.5891471741113798\n",
      "ELBO Loss: 504.38186862161035\n",
      "training:  96%|█████████▌| 957/1000 [06:26<00:16,  2.65it/s]Encodings MP Likelihood: 0.6556315780235531\n",
      "ELBO Loss: 504.5714360366629\n",
      "training:  96%|█████████▌| 958/1000 [06:26<00:15,  2.65it/s]Encodings MP Likelihood: 0.9012950180200819\n",
      "ELBO Loss: 504.03741421801243\n",
      "training:  96%|█████████▌| 959/1000 [06:26<00:15,  2.67it/s]Encodings MP Likelihood: 0.8783614043755171\n",
      "ELBO Loss: 503.96445144764374\n",
      "training:  96%|█████████▌| 960/1000 [06:27<00:14,  2.67it/s]Encodings MP Likelihood: 0.5394730607902204\n",
      "ELBO Loss: 503.7957424705203\n",
      "training:  96%|█████████▌| 961/1000 [06:27<00:14,  2.66it/s]Encodings MP Likelihood: 0.6854517376892392\n",
      "ELBO Loss: 504.15890333338956\n",
      "training:  96%|█████████▌| 962/1000 [06:28<00:14,  2.64it/s]Encodings MP Likelihood: 0.745846648517573\n",
      "ELBO Loss: 503.97535101726254\n",
      "training:  96%|█████████▋| 963/1000 [06:28<00:13,  2.65it/s]Encodings MP Likelihood: 0.28494403945142877\n",
      "ELBO Loss: 503.9640964102214\n",
      "training:  96%|█████████▋| 964/1000 [06:28<00:13,  2.66it/s]Encodings MP Likelihood: 0.5465358880553238\n",
      "ELBO Loss: 503.8114072175672\n",
      "training:  96%|█████████▋| 965/1000 [06:29<00:13,  2.65it/s]Encodings MP Likelihood: 0.6054213175929852\n",
      "ELBO Loss: 503.7559090812636\n",
      "training:  97%|█████████▋| 966/1000 [06:29<00:12,  2.65it/s]Encodings MP Likelihood: 0.36929063311894395\n",
      "ELBO Loss: 503.8033105386138\n",
      "training:  97%|█████████▋| 967/1000 [06:29<00:12,  2.65it/s]Encodings MP Likelihood: 0.45901247413837637\n",
      "ELBO Loss: 503.66066685825353\n",
      "training:  97%|█████████▋| 968/1000 [06:30<00:12,  2.64it/s]Encodings MP Likelihood: 0.4529997998106565\n",
      "ELBO Loss: 503.98238430923993\n",
      "training:  97%|█████████▋| 969/1000 [06:30<00:11,  2.66it/s]Encodings MP Likelihood: 0.7319120554616872\n",
      "ELBO Loss: 504.3518682960084\n",
      "training:  97%|█████████▋| 970/1000 [06:31<00:11,  2.68it/s]Encodings MP Likelihood: 0.9889458554214219\n",
      "ELBO Loss: 504.145947193672\n",
      "training:  97%|█████████▋| 971/1000 [06:31<00:10,  2.68it/s]Encodings MP Likelihood: 0.3220311757556037\n",
      "ELBO Loss: 503.96524560663613\n",
      "training:  97%|█████████▋| 972/1000 [06:31<00:10,  2.68it/s]Encodings MP Likelihood: 0.3180010285913508\n",
      "ELBO Loss: 503.4729227335736\n",
      "training:  97%|█████████▋| 973/1000 [06:32<00:10,  2.67it/s]Encodings MP Likelihood: 0.8649159142918935\n",
      "ELBO Loss: 503.88136244659546\n",
      "training:  97%|█████████▋| 974/1000 [06:32<00:09,  2.66it/s]Encodings MP Likelihood: 0.5398065078392574\n",
      "ELBO Loss: 504.27387849815653\n",
      "training:  98%|█████████▊| 975/1000 [06:32<00:09,  2.66it/s]Encodings MP Likelihood: 0.7843189038630562\n",
      "ELBO Loss: 503.884837498296\n",
      "training:  98%|█████████▊| 976/1000 [06:33<00:09,  2.66it/s]Encodings MP Likelihood: 0.5274897392619051\n",
      "ELBO Loss: 504.18958206587837\n",
      "training:  98%|█████████▊| 977/1000 [06:33<00:08,  2.65it/s]Encodings MP Likelihood: 0.4551329191126048\n",
      "ELBO Loss: 503.82977928128685\n",
      "training:  98%|█████████▊| 978/1000 [06:34<00:08,  2.66it/s]Encodings MP Likelihood: 0.6522797277447664\n",
      "ELBO Loss: 503.18704434927014\n",
      "training:  98%|█████████▊| 979/1000 [06:34<00:07,  2.65it/s]Encodings MP Likelihood: 0.8267875619857925\n",
      "ELBO Loss: 503.7582610544613\n",
      "training:  98%|█████████▊| 980/1000 [06:34<00:07,  2.64it/s]Encodings MP Likelihood: 0.47419915688775804\n",
      "ELBO Loss: 503.596569786668\n",
      "training:  98%|█████████▊| 981/1000 [06:35<00:07,  2.64it/s]Encodings MP Likelihood: 0.49047978273351645\n",
      "ELBO Loss: 503.7746771223656\n",
      "training:  98%|█████████▊| 982/1000 [06:35<00:06,  2.65it/s]Encodings MP Likelihood: 0.4868627674486559\n",
      "ELBO Loss: 503.559041432722\n",
      "training:  98%|█████████▊| 983/1000 [06:35<00:06,  2.69it/s]Encodings MP Likelihood: 0.411241624908908\n",
      "ELBO Loss: 503.9333734671456\n",
      "training:  98%|█████████▊| 984/1000 [06:36<00:05,  2.78it/s]Encodings MP Likelihood: 0.6216889178615632\n",
      "ELBO Loss: 503.71449868687097\n",
      "training:  98%|█████████▊| 985/1000 [06:36<00:05,  2.84it/s]Encodings MP Likelihood: 0.49223939960512825\n",
      "ELBO Loss: 503.4352412283898\n",
      "training:  99%|█████████▊| 986/1000 [06:36<00:04,  2.87it/s]Encodings MP Likelihood: 0.8218716005240952\n",
      "ELBO Loss: 503.35340175202066\n",
      "training:  99%|█████████▊| 987/1000 [06:37<00:04,  2.92it/s]Encodings MP Likelihood: 1.0192192864692604\n",
      "ELBO Loss: 504.5559080388636\n",
      "training:  99%|█████████▉| 988/1000 [06:37<00:04,  2.94it/s]Encodings MP Likelihood: 0.8242588640135906\n",
      "ELBO Loss: 503.7023490625907\n",
      "training:  99%|█████████▉| 989/1000 [06:37<00:03,  2.97it/s]Encodings MP Likelihood: 0.8070699515682671\n",
      "ELBO Loss: 504.0635086522538\n",
      "training:  99%|█████████▉| 990/1000 [06:38<00:03,  2.98it/s]Encodings MP Likelihood: 0.49901959841051546\n",
      "ELBO Loss: 503.3952002351174\n",
      "training:  99%|█████████▉| 991/1000 [06:38<00:03,  2.99it/s]Encodings MP Likelihood: 0.8216864153649528\n",
      "ELBO Loss: 503.8450055412822\n",
      "training:  99%|█████████▉| 992/1000 [06:38<00:02,  2.98it/s]Encodings MP Likelihood: 0.4555353403406609\n",
      "ELBO Loss: 503.7926950693729\n",
      "training:  99%|█████████▉| 993/1000 [06:39<00:02,  2.95it/s]Encodings MP Likelihood: 0.8209701379067783\n",
      "ELBO Loss: 503.7012311237397\n",
      "training:  99%|█████████▉| 994/1000 [06:39<00:02,  2.97it/s]Encodings MP Likelihood: 0.5342678891111131\n",
      "ELBO Loss: 503.8482324601853\n",
      "training: 100%|█████████▉| 995/1000 [06:39<00:01,  2.99it/s]Encodings MP Likelihood: 0.8417499674083808\n",
      "ELBO Loss: 503.75234684827126\n",
      "training: 100%|█████████▉| 996/1000 [06:40<00:01,  3.00it/s]Encodings MP Likelihood: 0.7173808502221226\n",
      "ELBO Loss: 503.5876275328324\n",
      "training: 100%|█████████▉| 997/1000 [06:40<00:01,  2.99it/s]Encodings MP Likelihood: 0.5160282504834947\n",
      "ELBO Loss: 502.99533983191526\n",
      "training: 100%|█████████▉| 998/1000 [06:40<00:00,  3.00it/s]Encodings MP Likelihood: 0.6763608189203688\n",
      "ELBO Loss: 503.53427443371095\n",
      "training: 100%|█████████▉| 999/1000 [06:41<00:00,  2.97it/s]Encodings MP Likelihood: 0.4132421413070576\n",
      "ELBO Loss: 503.33309241758934\n",
      "computing elbo\n",
      "training: 100%|██████████| 1000/1000 [06:41<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior and MV imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance is 1.5808206827945304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "error = mean_squared_error(full_posterior.get_latent(), leaves_z)\n",
    "print(\"the distance is {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Missing Value imputation By Posterior Predictive sampling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_l = np.mean(np.sum(glm.X, axis=1))\n",
    "\n",
    "# CascVI impitations\n",
    "imputed = {}\n",
    "imputed_z = {}\n",
    "imputed_gt = {}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        imputed[n.name], imputed_z[n.name] = full_posterior.imputation_internal(n,\n",
    "                                                            give_mean=False,\n",
    "                                                            library_size=empirical_l\n",
    "                                                           )\n",
    "        imputed_gt[n.name] = glm.X[n.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_X = [x for x in imputed.values()]\n",
    "imputed_X = np.array(imputed_X).reshape(-1, cas_dataset.X.shape[1])\n",
    "#plot_histograms(imputed_X, \"Histogram of CasscVI imputed gene expression data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 1 (MP Oracle)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CascVI impitations\n",
    "imputed_cascvi_1 = {}\n",
    "imputed_cascvi_1_z ={}\n",
    "\n",
    "for n in tree.traverse('levelorder'):\n",
    "    if not n.is_leaf():\n",
    "        _, imputed_cascvi_1_z[n.name] = full_posterior.imputation_internal(n,\n",
    "                                                                    give_mean=False,\n",
    "                                                                    library_size=empirical_l,\n",
    "                                                                    known_latent=leaves_z\n",
    "        )\n",
    "        mu_z = np.clip(a=np.exp(glm.W @ imputed_cascvi_1_z[n.name].cpu().numpy() + glm.beta),\n",
    "                        a_min=0,\n",
    "                        a_max=1e8\n",
    "                        )\n",
    "        samples = np.array([np.random.poisson(mu_z) for i in range(100)])\n",
    "        imputed_cascvi_1[n.name] = np.clip(a=np.mean(samples, axis=0),\n",
    "                                           a_min=0,\n",
    "                                           a_max=1e8\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CascVI Baseline 2 (Reconstruction of Averaged latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_cascvi_2, imputed_cascvi_2_z = avg_baseline_z(tree=tree,\n",
    "                                   model=treevae,\n",
    "                                   posterior=full_posterior,\n",
    "                                   weighted=False,\n",
    "                                   n_samples_z=1,\n",
    "                                   library_size=empirical_l,\n",
    "                                   gaussian=False,\n",
    "                                   use_cuda=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_posterior.compute_elbo(treevae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Unweighted Average of gene expression in Clade\n",
    "\n",
    "The simple idea here is to impute the value of an internal node, with the (un)weighted average of the gene expression values of the leaves, taking the query internal node as the root of the subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = False\n",
    "imputed_avg = avg_weighted_baseline(tree, weighted, glm.X, rounding=True)\n",
    "\n",
    "#get internal nodes\n",
    "avg_X = np.array([x for x in imputed_avg.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_avg_X, _, _ = get_internal(avg_X, glm.mu, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: (Un)weighted Average of decoded latent vectors, with scVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same averaging of the subtrees leaves in **Baseline 1**, only this time, the gene expression data is recovered with scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anndata\n",
    "gene_dataset = GeneExpressionDataset()\n",
    "gene_dataset.populate_from_data(leaves_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecoderSCVI(\n",
       "  (px_decoder): FCLayers(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (px_scale_decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=986, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       "  (px_r_decoder): Linear(in_features=128, out_features=986, bias=True)\n",
       "  (px_dropout_decoder): Linear(in_features=128, out_features=986, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs =500\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=128,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=glm.latent,\n",
    "                  ldvae=ldvae\n",
    "              )\n",
    "\n",
    "if freeze:\n",
    "    new_weight = torch.from_numpy(glm.W).float()\n",
    "    new_bias = torch.from_numpy(glm.beta).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].weight = torch.nn.Parameter(new_weight)\n",
    "        vae.decoder.factor_regressor.fc_layers[0][0].bias = torch.nn.Parameter(new_bias)\n",
    "        \n",
    "    for param in vae.decoder.factor_regressor.fc_layers[0][0].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance between the Poisson and the NB means is 3595.5991060821543\n"
     ]
    }
   ],
   "source": [
    "px_scale, px_r, px_rate, px_dropout = vae.decoder.forward(vae.dispersion,\n",
    "                                        torch.from_numpy(leaves_z).float(),\n",
    "                                        torch.from_numpy(np.array([np.log(10000)])).float(),\n",
    "                                        None\n",
    "                                        )\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "if ldvae:\n",
    "    foo = np.clip(a=np.exp(px_r.detach().numpy()),\n",
    "            a_min=0,\n",
    "            a_max=5000\n",
    "    )\n",
    "    mse = mean_squared_error(mu, foo)\n",
    "else:\n",
    "    mse = mean_squared_error(mu, px_rate.detach().numpy())\n",
    "\n",
    "print(\"the distance between the Poisson and the NB means is {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 500/500 [00:48<00:00, 10.23it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=None)\n",
    "\n",
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "        \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the distance is 2.110614863810331\n"
     ]
    }
   ],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior(model=vae,\n",
    "                                               gene_dataset=gene_dataset \n",
    "                                                )\n",
    "\n",
    "error = mean_squared_error(scvi_posterior.get_latent()[0], leaves_z)\n",
    "print(\"the distance is {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "494.27008203125"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "scvi_posterior.elbo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***scVI Baseline 2 (Decoded Average Latent space)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_size = np.mean(np.sum(glm.X, axis=1))\n",
    "scvi_latent = np.array([scvi_posterior.get_latent(give_mean=False)[0] for i in range(10)])\n",
    "\n",
    "imputed_scvi_2, imputed_scvi_2_z = scvi_baseline_z(tree,\n",
    "                                        posterior=scvi_posterior,\n",
    "                                        model=vae,\n",
    "                                        weighted=False,\n",
    "                                        n_samples_z=1,\n",
    "                                        library_size=library_size,\n",
    "                                        use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Likelihood Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'full_posterior' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d29f38a47e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcascvi_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscvi_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscvi_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscvi_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcascvi_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_posterior' is not defined"
     ]
    }
   ],
   "source": [
    "cascvi_latent = full_posterior.get_latent()\n",
    "scvi_latent = scvi_posterior.get_latent()[0]\n",
    "\n",
    "scvi_latent.shape, cascvi_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of scVI encodings:  -53355.6203823165\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(scvi_latent, cas_dataset.barcodes, scvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), scvi_latent.shape[1], False)\n",
    "mp_lik_scvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of scVI encodings: \", mp_lik_scvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of cascVI encodings:  2082.99567879663\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(cascvi_latent, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of cascVI encodings: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood of observations:  -241.65942573552684\n"
     ]
    }
   ],
   "source": [
    "treevae.initialize_visit()\n",
    "treevae.initialize_messages(leaves_z, cas_dataset.barcodes, cascvi_latent.shape[1])\n",
    "treevae.perform_message_passing((treevae.tree & treevae.root), cascvi_latent.shape[1], False)\n",
    "mp_lik_cascvi = treevae.aggregate_messages_into_leaves_likelihood(d, add_prior=True)\n",
    "print(\"Likelihood of observations: \", mp_lik_cascvi.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Likelihood Ratio: tensor(53113.9610, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood ratio\n",
    "lambda_ = (mp_lik_cascvi - mp_lik_scvi)\n",
    "print(\"Likelihood Ratio:\", lambda_)"
   ]
  },
  {
   "source": [
    "# 6. Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CPM Normalization (for sample-sample correlation)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get imputations into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000000 into shape (986)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-c958438d5186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minternal_scvi_X_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_scvi_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minternal_cascvi_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_cascvi_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minternal_cascvi_X_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_cascvi_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minternal_cascvi_X_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_scvi_X_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputed_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_avg_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1000000 into shape (986)"
     ]
    }
   ],
   "source": [
    "internal_scvi_X_2 = np.array([x for x in imputed_scvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "#internal_cascvi_X = np.array([x for x in imputed_cascvi_1.values()]).reshape(-1, glm.X.shape[1])\n",
    "internal_cascvi_X_2 = np.array([x for x in imputed_cascvi_2.values()]).reshape(-1, glm.X.shape[1])\n",
    "\n",
    "internal_cascvi_X_2.shape, internal_scvi_X_2.shape, imputed_X.shape, internal_avg_X.shape, internal_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 986)"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_internal_X = sc.pp.normalize_total(AnnData(internal_X), target_sum=1e6, inplace=False)['X'] \n",
    "norm_scvi_X_2 = sc.pp.normalize_total(AnnData(internal_scvi_X_2), target_sum=1e6, inplace=False)['X']\n",
    "norm_avg_X = sc.pp.normalize_total(AnnData(internal_avg_X), target_sum=1e6, inplace=False)['X']\n",
    "norm_imputed_X = sc.pp.normalize_total(AnnData(imputed_X), target_sum=1e6, inplace=False)['X']\n",
    "#norm_cascvi_X = sc.pp.normalize_total(AnnData(internal_cascvi_X), target_sum=1e6, inplace=False)['X']\n",
    "norm_cascvi_X_2 = sc.pp.normalize_total(AnnData(internal_cascvi_X_2), target_sum=1e6, inplace=False)['X']\n",
    "\n",
    "norm_internal_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sample-Sample Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Sample-Sample correlation (Without Normalization)***\n",
    "\n",
    "We will use Scipy to compute a nonparametric rank correlation between the imputed and the groundtruth profiles. The correlation is based on the Spearman Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'groundtruth': internal_X.T, 'cascVI': imputed_X.T, 'scVI': internal_scvi_X_2.T,\n",
    "        'Average': internal_avg_X.T , 'cascVI + Avg': internal_cascvi_X_2.T}\n",
    "        #'MP Oracle': internal_cascvi_X.T\n",
    "        #}\n",
    "df1 = correlations(data, 'None', True)\n",
    "#df1.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Sample-Sample correlation (With ScanPy Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'groundtruth': norm_internal_X.T, 'cascVI': norm_imputed_X.T, 'scVI': norm_scvi_X_2.T, \n",
    "        'Average': norm_avg_X.T , 'cascVI + Avg': norm_cascvi_X_2.T}\n",
    "        #'MP Oracle': norm_cascvi_X.T\n",
    "        #}\n",
    "\n",
    "df2 = correlations(data, 'None', True)\n",
    "#df2.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II. Gene-Gene Correlations"
   ]
  },
  {
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2}\n",
    "        #,\n",
    "        #'MP Oracle': internal_cascvi_X\n",
    "        #}\n",
    "\n",
    "df3 = correlations(data, 'None', True)\n",
    "#df3.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Gene-Gene correlation (With Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2}\n",
    "        #'MP Oracle': norm_cascvi_X\n",
    "        #}\n",
    "\n",
    "df4 = correlations(data, 'None', True)\n",
    "#df4.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Gene-Gene correlation (With Rank Normalization)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:4264: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(SpearmanRConstantInputWarning())\n/home/eecs/khalil.ouardini/miniconda3/envs/scvi-env/lib/python3.7/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "#data = {'groundtruth': norm_internal_X, 'cascVI': norm_imputed_X, 'scVI': norm_scvi_X_2, \n",
    "#        'Average': norm_avg_X , 'cascVI + Avg': norm_cascvi_X_2,\n",
    "#        'MP Oracle': norm_cascvi_X\n",
    "#        }\n",
    "\n",
    "data = {'groundtruth': internal_X, 'cascVI': imputed_X, 'scVI': internal_scvi_X_2,\n",
    "        'Average': internal_avg_X , 'cascVI + Avg': internal_cascvi_X_2}\n",
    "        #'MP Oracle': internal_cascvi_X\n",
    "        #}\n",
    "        \n",
    "df5 = correlations(data, 'rank', True)\n",
    "#df5.head(5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Table Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Method\", \"Spearman CC\", \"Pearson CC\", \"Kendall Tau\"]\n",
    "data = [df1, df2, df3, df4, df5]\n",
    "#data = [df2, df4]\n",
    "\n",
    "data \n",
    "tables = [[] for i in range(len(data))]\n",
    "\n",
    "#task = [\"Sample-Sample (None)\", \"Sample-Sample (CPM)\", \"Gene-Gene (None)\", \n",
    "           #\"Gene-Gene(CPM)\", \"Gene-Gene (Rank)\" ]\n",
    "\n",
    "for (df, t) in zip(data, tables):\n",
    "    for m in np.unique(df.Method):\n",
    "        sub_df = np.round(df[df['Method'] == m].mean(), decimals=3)\n",
    "        t.append([m, sub_df['Spearman CC'], sub_df['Pearson CC'], sub_df['Kendall Tau']])\n",
    "        \n",
    "# Create and style Data Frames\n",
    "df_table1 = pd.DataFrame(tables[0], columns=columns)\n",
    "df_table2 = pd.DataFrame(tables[1], columns=columns)\n",
    "df_table3 = pd.DataFrame(tables[2], columns=columns)\n",
    "df_table4 = pd.DataFrame(tables[3], columns=columns)\n",
    "df_table5 = pd.DataFrame(tables[4], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " >>> Sample-Sample | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.497       0.814        0.479\n",
       "1        cascVI        0.499       0.845        0.415\n",
       "2  cascVI + Avg        0.493       0.837        0.415\n",
       "3          scVI        0.487       0.830        0.410"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.497</td>\n      <td>0.814</td>\n      <td>0.479</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.499</td>\n      <td>0.845</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.493</td>\n      <td>0.837</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.487</td>\n      <td>0.830</td>\n      <td>0.410</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "print(\" >>> Sample-Sample | No Normalization <<<\")\n",
    "df_table1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Sample-Sample | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.497       0.814        0.479\n",
       "1        cascVI        0.499       0.845        0.415\n",
       "2  cascVI + Avg        0.493       0.837        0.415\n",
       "3          scVI        0.487       0.830        0.410"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.497</td>\n      <td>0.814</td>\n      <td>0.479</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.499</td>\n      <td>0.845</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.493</td>\n      <td>0.837</td>\n      <td>0.415</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.487</td>\n      <td>0.830</td>\n      <td>0.410</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "print(\">>> Sample-Sample | CPM Normalization <<<\")\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | No Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.254       0.322        0.238\n",
       "1        cascVI        0.289       0.353        0.235\n",
       "2  cascVI + Avg        0.280       0.340        0.230\n",
       "3          scVI        0.274       0.330        0.226"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.254</td>\n      <td>0.322</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.289</td>\n      <td>0.353</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.280</td>\n      <td>0.340</td>\n      <td>0.230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.274</td>\n      <td>0.330</td>\n      <td>0.226</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | No Normalization <<<\")\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | CPM Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.238       0.268        0.213\n",
       "1        cascVI        0.296       0.379        0.232\n",
       "2  cascVI + Avg        0.288       0.367        0.227\n",
       "3          scVI        0.281       0.353        0.222"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.238</td>\n      <td>0.268</td>\n      <td>0.213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.296</td>\n      <td>0.379</td>\n      <td>0.232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.288</td>\n      <td>0.367</td>\n      <td>0.227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.281</td>\n      <td>0.353</td>\n      <td>0.222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | CPM Normalization <<<\")\n",
    "df_table4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Gene-Gene | Rank Normalization <<<\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Method  Spearman CC  Pearson CC  Kendall Tau\n",
       "0       Average        0.254       0.254        0.238\n",
       "1        cascVI        0.289       0.289        0.235\n",
       "2  cascVI + Avg        0.280       0.280        0.230\n",
       "3          scVI        0.274       0.274        0.226"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Spearman CC</th>\n      <th>Pearson CC</th>\n      <th>Kendall Tau</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Average</td>\n      <td>0.254</td>\n      <td>0.254</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cascVI</td>\n      <td>0.289</td>\n      <td>0.289</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cascVI + Avg</td>\n      <td>0.280</td>\n      <td>0.280</td>\n      <td>0.230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scVI</td>\n      <td>0.274</td>\n      <td>0.274</td>\n      <td>0.226</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "print(\">>> Gene-Gene | Rank Normalization <<<\")\n",
    "df_table5.head(10)"
   ]
  },
  {
   "source": [
    "# 8. Latent Space Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### k-NN purity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***LEAVES only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Leaves Only\n",
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.4698933815753866, 0.49297910553113355)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "print(\"Leaves Only\")\n",
    "data = {'groundtruth': leaves_z, 'scVI': scvi_latent,\n",
    "        'cascVI': cascvi_latent\n",
    "        }\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "                    data=data,\n",
    "                    plot=False,\n",
    "                    save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp_purtiy.png'\n",
    "                    )\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])"
   ]
  },
  {
   "source": [
    "*** Internal nodes only***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Internal nodes Only\n",
      "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.44587087695578187, 0.5250488645477123)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "print(\"Internal nodes Only\")\n",
    "\n",
    "full_cascvi_latent = construct_latent(tree, cascvi_latent, imputed_z)\n",
    "full_scvi_latent = construct_latent(tree, scvi_latent, imputed_scvi_2_z)\n",
    "\n",
    "internal_z, internal_idx, internal_mu = get_internal(glm.z, glm.mu, tree)\n",
    "internal_scvi_z, _, _ = get_internal(full_scvi_latent, glm.mu, tree)\n",
    "internal_cascvi_z, _, _ = get_internal(full_cascvi_latent, glm.mu, tree)\n",
    "\n",
    "data = {'groundtruth': internal_z, 'scVI': internal_scvi_z,\n",
    "        'cascVI': internal_cascvi_z\n",
    "        }\n",
    "\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "                    data=data,\n",
    "                    plot=False,\n",
    "                    save_fig='/home/eecs/khalil.ouardini/Cassiopeia_Transcriptome/scvi/tmp_purtiy.png'\n",
    "                    )\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])"
   ]
  },
  {
   "source": [
    "***Full tree***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full tree\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.45329867389094985, 0.5212643774644873)"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "print(\"Full tree\")\n",
    "data = {'groundtruth': glm.z, 'scVI': full_scvi_latent,\n",
    "        'cascVI': full_cascvi_latent\n",
    "        }\n",
    "\n",
    "scores = knn_purity(max_neighbors=30,\n",
    "              data=data,\n",
    "              plot=True)\n",
    "\n",
    "np.mean(scores['scVI']), np.mean(scores['cascVI'])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7  ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}