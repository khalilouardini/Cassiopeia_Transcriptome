{
 "cells": [
  {
   "source": [
    "# 0. Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('WebAgg')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/eecs\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***import ete3 Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "tree_name = \"/home/eecs/khalil.ouardini/cas_scvi_topologies/newick_objects/500cells/high_fitness/topology8.nwk\"\n",
    "tree = Tree(tree_name, 1)\n",
    "\n",
    "for i, n in enumerate(tree.traverse('levelorder')):\n",
    "    n.add_features(index=i)\n",
    "    n.name = str(i)\n",
    "\n",
    "eps = 1e-3\n",
    "branch_length = {}\n",
    "for node in tree.traverse('levelorder'):\n",
    "    if node.is_root():\n",
    "        branch_length[node.name] = 0.0\n",
    "    else:\n",
    "        branch_length[node.name] = node.dist\n",
    "branch_length['prior_root'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "from anndata import AnnData\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from external.dataset.tree import TreeDataset, GeneExpressionDataset\n",
    "from external.dataset.poisson_glm import Poisson_GLM\n",
    "from external.dataset.anndataset import AnnDatasetFromAnnData\n",
    "\n",
    "# Models\n",
    "from models.vae import VAE\n",
    "import scanpy as sc\n",
    "from external.inference.tree_inference import TreeTrainer\n",
    "from inference.inference import UnsupervisedTrainer\n",
    "from scvi.inference import posterior\n",
    "from external.models.treevae import TreeVAE\n",
    "\n",
    "# Utils\n",
    "from external.utils.data_util import get_leaves, get_internal\n",
    "from external.utils.metrics import ks_pvalue, accuracy_imputation, correlations, knn_purity, knn_purity_stratified\n",
    "from external.utils.plots_util import plot_histograms, plot_scatter_mean, plot_ecdf_ks, plot_density\n",
    "from external.utils.plots_util import plot_losses, plot_elbo, plot_common_ancestor, plot_one_gene, training_dashboard\n",
    "from external.utils.baselines import avg_weighted_baseline, scvi_baseline, scvi_baseline_z, cascvi_baseline_z, avg_baseline_z, construct_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9fdbeeb250>"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulations (Poisson GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "g = 1000\n",
    "vis = False\n",
    "leaves_only = False\n",
    "var = 1.0\n",
    "alpha = 1.0\n",
    "\n",
    "glm = Poisson_GLM(tree, g, d, vis, leaves_only, branch_length, alpha)\n",
    "\n",
    "glm.simulate_latent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate gene expression count data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 1000), (1000, 10), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "glm.simulate_ge(negative_binomial=False)\n",
    "# Quality Control (i.e Gene Filtering)\n",
    "#glm.gene_qc()\n",
    "\n",
    "glm.X.shape, glm.W.shape, glm.beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Binomial thinning***"
   ]
  },
  {
   "source": [
    "print(\"Proportion of dropouts: {}\".format(np.mean(glm.X == 0)))\n",
    "#glm.binomial_thinning(p=0.1)\n",
    "print(\"Proportion of dropouts after Binomial thinning: {}\".format(np.mean(glm.X == 0)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 161,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proportion of dropouts: 0.397263\nProportion of dropouts after Binomial thinning: 0.397263\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get the data and the indexes at the leaves***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((500, 1000), (500, 1000), (500, 1000), (500, 1000), (500, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "# Latent vectors\n",
    "leaves_z, _, _ = get_leaves(glm.z, glm.mu, tree)\n",
    "\n",
    "#FIXED training set\n",
    "leaves_X, leaves_idx, mu = get_leaves(glm.X, glm.mu, tree)\n",
    "\n",
    "# internal nodes data (for imputation)\n",
    "internal_X, internal_idx, internal_mu = get_internal(glm.X, glm.mu, tree)\n",
    "\n",
    "# Additional data for clade samling\n",
    "n_leaves_X = glm.generate_ge(n_samples=50, leaves_idx=leaves_idx)\n",
    "\n",
    "leaves_X.shape, mu.shape, internal_X.shape, internal_mu.shape, leaves_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting CascVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_dataset = GeneExpressionDataset()\n",
    "\n",
    "leaves = [n for n in tree.traverse('levelorder') if n.is_leaf()]\n",
    "cell_names = np.array([[n.name for n in leaves] * len(n_leaves_X)]).flatten()\n",
    "gene_dataset.populate_from_data(X=np.vstack(n_leaves_X),\n",
    "                               gene_names=[str(i) for i in range(glm.X.shape[1])])\n",
    "gene_dataset.initialize_cell_attribute('barcodes', cell_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a TreeDataset object***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GeneExpressionDataset object with n_cells x nb_genes = 25000 x 1000\n",
       "    gene_attribute_names: 'gene_names'\n",
       "    cell_attribute_names: 'local_vars', 'barcodes', 'local_means', 'batch_indices', 'labels'\n",
       "    cell_categorical_attribute_names: 'batch_indices', 'labels'"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "# treeVAE\n",
    "import copy\n",
    "\n",
    "tree_bis = copy.deepcopy(tree)\n",
    "cas_dataset = TreeDataset(gene_dataset, tree=tree_bis, filtering=False)\n",
    "\n",
    "# No batches beacause of the message passing\n",
    "use_cuda = True\n",
    "use_MP = True\n",
    "ldvae = False\n",
    "\n",
    "cas_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=use_MP,\n",
    "              use_clades=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "lambda_ = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***trainer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8951, 20451, 24951, 951, 6451, 18951, 12451, 3451, 11951, 18451, 10951, 9951, 4951, 19951, 23451, 1951, 451, 23951, 22451], [14452, 5952, 5452, 20952, 1452, 13952, 19452, 15952, 11452, 2452, 16952, 17952, 13452, 17452, 9452, 3952, 7452, 22952, 24452, 14952, 7952, 15452, 16452, 8452, 21452, 10452, 21952, 4452, 6952, 12952, 2952, 8952, 20452, 24952, 952, 6452, 18952, 12452, 3452, 11952, 18452, 10952, 9952, 4952, 19952, 23452, 1952, 452, 23952, 22452], [14453, 5953, 5453, 20953, 1453, 13953, 19453, 15953, 11453, 2453, 16953, 17953, 13453, 17453, 9453, 3953, 7453, 22953, 24453, 14953, 7953, 15453, 16453, 8453, 21453, 10453, 21953, 4453, 6953, 12953, 2953, 8953, 20453, 24953, 953, 6453, 18953, 12453, 3453, 11953, 18453, 10953, 9953, 4953, 19953, 23453, 1953, 453, 23953, 22453], [14454, 5954, 5454, 20954, 1454, 13954, 19454, 15954, 11454, 2454, 16954, 17954, 13454, 17454, 9454, 3954, 7454, 22954, 24454, 14954, 7954, 15454, 16454, 8454, 21454, 10454, 21954, 4454, 6954, 12954, 2954, 8954, 20454, 24954, 954, 6454, 18954, 12454, 3454, 11954, 18454, 10954, 9954, 4954, 19954, 23454, 1954, 454, 23954, 22454], [14455, 5955, 5455, 20955, 1455, 13955, 19455, 15955, 11455, 2455, 16955, 17955, 13455, 17455, 9455, 3955, 7455, 22955, 24455, 14955, 7955, 15455, 16455, 8455, 21455, 10455, 21955, 4455, 6955, 12955, 2955, 8955, 20455, 24955, 955, 6455, 18955, 12455, 3455, 11955, 18455, 10955, 9955, 4955, 19955, 23455, 1955, 455, 23955, 22455], [14456, 5956, 5456, 20956, 1456, 13956, 19456, 15956, 11456, 2456, 16956, 17956, 13456, 17456, 9456, 3956, 7456, 22956, 24456, 14956, 7956, 15456, 16456, 8456, 21456, 10456, 21956, 4456, 6956, 12956, 2956, 8956, 20456, 24956, 956, 6456, 18956, 12456, 3456, 11956, 18456, 10956, 9956, 4956, 19956, 23456, 1956, 456, 23956, 22456], [14457, 5957, 5457, 20957, 1457, 13957, 19457, 15957, 11457, 2457, 16957, 17957, 13457, 17457, 9457, 3957, 7457, 22957, 24457, 14957, 7957, 15457, 16457, 8457, 21457, 10457, 21957, 4457, 6957, 12957, 2957, 8957, 20457, 24957, 957, 6457, 18957, 12457, 3457, 11957, 18457, 10957, 9957, 4957, 19957, 23457, 1957, 457, 23957, 22457], [14458, 5958, 5458, 20958, 1458, 13958, 19458, 15958, 11458, 2458, 16958, 17958, 13458, 17458, 9458, 3958, 7458, 22958, 24458, 14958, 7958, 15458, 16458, 8458, 21458, 10458, 21958, 4458, 6958, 12958, 2958, 8958, 20458, 24958, 958, 6458, 18958, 12458, 3458, 11958, 18458, 10958, 9958, 4958, 19958, 23458, 1958, 458, 23958, 22458], [14459, 5959, 5459, 20959, 1459, 13959, 19459, 15959, 11459, 2459, 16959, 17959, 13459, 17459, 9459, 3959, 7459, 22959, 24459, 14959, 7959, 15459, 16459, 8459, 21459, 10459, 21959, 4459, 6959, 12959, 2959, 8959, 20459, 24959, 959, 6459, 18959, 12459, 3459, 11959, 18459, 10959, 9959, 4959, 19959, 23459, 1959, 459, 23959, 22459], [14460, 5960, 5460, 20960, 1460, 13960, 19460, 15960, 11460, 2460, 16960, 17960, 13460, 17460, 9460, 3960, 7460, 22960, 24460, 14960, 7960, 15460, 16460, 8460, 21460, 10460, 21960, 4460, 6960, 12960, 2960, 8960, 20460, 24960, 960, 6460, 18960, 12460, 3460, 11960, 18460, 10960, 9960, 4960, 19960, 23460, 1960, 460, 23960, 22460], [14461, 5961, 5461, 20961, 1461, 13961, 19461, 15961, 11461, 2461, 16961, 17961, 13461, 17461, 9461, 3961, 7461, 22961, 24461, 14961, 7961, 15461, 16461, 8461, 21461, 10461, 21961, 4461, 6961, 12961, 2961, 8961, 20461, 24961, 961, 6461, 18961, 12461, 3461, 11961, 18461, 10961, 9961, 4961, 19961, 23461, 1961, 461, 23961, 22461], [14462, 5962, 5462, 20962, 1462, 13962, 19462, 15962, 11462, 2462, 16962, 17962, 13462, 17462, 9462, 3962, 7462, 22962, 24462, 14962, 7962, 15462, 16462, 8462, 21462, 10462, 21962, 4462, 6962, 12962, 2962, 8962, 20462, 24962, 962, 6462, 18962, 12462, 3462, 11962, 18462, 10962, 9962, 4962, 19962, 23462, 1962, 462, 23962, 22462], [14463, 5963, 5463, 20963, 1463, 13963, 19463, 15963, 11463, 2463, 16963, 17963, 13463, 17463, 9463, 3963, 7463, 22963, 24463, 14963, 7963, 15463, 16463, 8463, 21463, 10463, 21963, 4463, 6963, 12963, 2963, 8963, 20463, 24963, 963, 6463, 18963, 12463, 3463, 11963, 18463, 10963, 9963, 4963, 19963, 23463, 1963, 463, 23963, 22463], [14464, 5964, 5464, 20964, 1464, 13964, 19464, 15964, 11464, 2464, 16964, 17964, 13464, 17464, 9464, 3964, 7464, 22964, 24464, 14964, 7964, 15464, 16464, 8464, 21464, 10464, 21964, 4464, 6964, 12964, 2964, 8964, 20464, 24964, 964, 6464, 18964, 12464, 3464, 11964, 18464, 10964, 9964, 4964, 19964, 23464, 1964, 464, 23964, 22464], [14465, 5965, 5465, 20965, 1465, 13965, 19465, 15965, 11465, 2465, 16965, 17965, 13465, 17465, 9465, 3965, 7465, 22965, 24465, 14965, 7965, 15465, 16465, 8465, 21465, 10465, 21965, 4465, 6965, 12965, 2965, 8965, 20465, 24965, 965, 6465, 18965, 12465, 3465, 11965, 18465, 10965, 9965, 4965, 19965, 23465, 1965, 465, 23965, 22465], [14466, 5966, 5466, 20966, 1466, 13966, 19466, 15966, 11466, 2466, 16966, 17966, 13466, 17466, 9466, 3966, 7466, 22966, 24466, 14966, 7966, 15466, 16466, 8466, 21466, 10466, 21966, 4466, 6966, 12966, 2966, 8966, 20466, 24966, 966, 6466, 18966, 12466, 3466, 11966, 18466, 10966, 9966, 4966, 19966, 23466, 1966, 466, 23966, 22466], [14467, 5967, 5467, 20967, 1467, 13967, 19467, 15967, 11467, 2467, 16967, 17967, 13467, 17467, 9467, 3967, 7467, 22967, 24467, 14967, 7967, 15467, 16467, 8467, 21467, 10467, 21967, 4467, 6967, 12967, 2967, 8967, 20467, 24967, 967, 6467, 18967, 12467, 3467, 11967, 18467, 10967, 9967, 4967, 19967, 23467, 1967, 467, 23967, 22467], [14468, 5968, 5468, 20968, 1468, 13968, 19468, 15968, 11468, 2468, 16968, 17968, 13468, 17468, 9468, 3968, 7468, 22968, 24468, 14968, 7968, 15468, 16468, 8468, 21468, 10468, 21968, 4468, 6968, 12968, 2968, 8968, 20468, 24968, 968, 6468, 18968, 12468, 3468, 11968, 18468, 10968, 9968, 4968, 19968, 23468, 1968, 468, 23968, 22468], [14469, 5969, 5469, 20969, 1469, 13969, 19469, 15969, 11469, 2469, 16969, 17969, 13469, 17469, 9469, 3969, 7469, 22969, 24469, 14969, 7969, 15469, 16469, 8469, 21469, 10469, 21969, 4469, 6969, 12969, 2969, 8969, 20469, 24969, 969, 6469, 18969, 12469, 3469, 11969, 18469, 10969, 9969, 4969, 19969, 23469, 1969, 469, 23969, 22469], [14470, 5970, 5470, 20970, 1470, 13970, 19470, 15970, 11470, 2470, 16970, 17970, 13470, 17470, 9470, 3970, 7470, 22970, 24470, 14970, 7970, 15470, 16470, 8470, 21470, 10470, 21970, 4470, 6970, 12970, 2970, 8970, 20470, 24970, 970, 6470, 18970, 12470, 3470, 11970, 18470, 10970, 9970, 4970, 19970, 23470, 1970, 470, 23970, 22470], [14471, 5971, 5471, 20971, 1471, 13971, 19471, 15971, 11471, 2471, 16971, 17971, 13471, 17471, 9471, 3971, 7471, 22971, 24471, 14971, 7971, 15471, 16471, 8471, 21471, 10471, 21971, 4471, 6971, 12971, 2971, 8971, 20471, 24971, 971, 6471, 18971, 12471, 3471, 11971, 18471, 10971, 9971, 4971, 19971, 23471, 1971, 471, 23971, 22471], [14472, 5972, 5472, 20972, 1472, 13972, 19472, 15972, 11472, 2472, 16972, 17972, 13472, 17472, 9472, 3972, 7472, 22972, 24472, 14972, 7972, 15472, 16472, 8472, 21472, 10472, 21972, 4472, 6972, 12972, 2972, 8972, 20472, 24972, 972, 6472, 18972, 12472, 3472, 11972, 18472, 10972, 9972, 4972, 19972, 23472, 1972, 472, 23972, 22472], [14473, 5973, 5473, 20973, 1473, 13973, 19473, 15973, 11473, 2473, 16973, 17973, 13473, 17473, 9473, 3973, 7473, 22973, 24473, 14973, 7973, 15473, 16473, 8473, 21473, 10473, 21973, 4473, 6973, 12973, 2973, 8973, 20473, 24973, 973, 6473, 18973, 12473, 3473, 11973, 18473, 10973, 9973, 4973, 19973, 23473, 1973, 473, 23973, 22473], [14474, 5974, 5474, 20974, 1474, 13974, 19474, 15974, 11474, 2474, 16974, 17974, 13474, 17474, 9474, 3974, 7474, 22974, 24474, 14974, 7974, 15474, 16474, 8474, 21474, 10474, 21974, 4474, 6974, 12974, 2974, 8974, 20474, 24974, 974, 6474, 18974, 12474, 3474, 11974, 18474, 10974, 9974, 4974, 19974, 23474, 1974, 474, 23974, 22474], [14475, 5975, 5475, 20975, 1475, 13975, 19475, 15975, 11475, 2475, 16975, 17975, 13475, 17475, 9475, 3975, 7475, 22975, 24475, 14975, 7975, 15475, 16475, 8475, 21475, 10475, 21975, 4475, 6975, 12975, 2975, 8975, 20475, 24975, 975, 6475, 18975, 12475, 3475, 11975, 18475, 10975, 9975, 4975, 19975, 23475, 1975, 475, 23975, 22475], [14476, 5976, 5476, 20976, 1476, 13976, 19476, 15976, 11476, 2476, 16976, 17976, 13476, 17476, 9476, 3976, 7476, 22976, 24476, 14976, 7976, 15476, 16476, 8476, 21476, 10476, 21976, 4476, 6976, 12976, 2976, 8976, 20476, 24976, 976, 6476, 18976, 12476, 3476, 11976, 18476, 10976, 9976, 4976, 19976, 23476, 1976, 476, 23976, 22476], [14477, 5977, 5477, 20977, 1477, 13977, 19477, 15977, 11477, 2477, 16977, 17977, 13477, 17477, 9477, 3977, 7477, 22977, 24477, 14977, 7977, 15477, 16477, 8477, 21477, 10477, 21977, 4477, 6977, 12977, 2977, 8977, 20477, 24977, 977, 6477, 18977, 12477, 3477, 11977, 18477, 10977, 9977, 4977, 19977, 23477, 1977, 477, 23977, 22477], [14478, 5978, 5478, 20978, 1478, 13978, 19478, 15978, 11478, 2478, 16978, 17978, 13478, 17478, 9478, 3978, 7478, 22978, 24478, 14978, 7978, 15478, 16478, 8478, 21478, 10478, 21978, 4478, 6978, 12978, 2978, 8978, 20478, 24978, 978, 6478, 18978, 12478, 3478, 11978, 18478, 10978, 9978, 4978, 19978, 23478, 1978, 478, 23978, 22478], [14479, 5979, 5479, 20979, 1479, 13979, 19479, 15979, 11479, 2479, 16979, 17979, 13479, 17479, 9479, 3979, 7479, 22979, 24479, 14979, 7979, 15479, 16479, 8479, 21479, 10479, 21979, 4479, 6979, 12979, 2979, 8979, 20479, 24979, 979, 6479, 18979, 12479, 3479, 11979, 18479, 10979, 9979, 4979, 19979, 23479, 1979, 479, 23979, 22479], [14480, 5980, 5480, 20980, 1480, 13980, 19480, 15980, 11480, 2480, 16980, 17980, 13480, 17480, 9480, 3980, 7480, 22980, 24480, 14980, 7980, 15480, 16480, 8480, 21480, 10480, 21980, 4480, 6980, 12980, 2980, 8980, 20480, 24980, 980, 6480, 18980, 12480, 3480, 11980, 18480, 10980, 9980, 4980, 19980, 23480, 1980, 480, 23980, 22480], [14481, 5981, 5481, 20981, 1481, 13981, 19481, 15981, 11481, 2481, 16981, 17981, 13481, 17481, 9481, 3981, 7481, 22981, 24481, 14981, 7981, 15481, 16481, 8481, 21481, 10481, 21981, 4481, 6981, 12981, 2981, 8981, 20481, 24981, 981, 6481, 18981, 12481, 3481, 11981, 18481, 10981, 9981, 4981, 19981, 23481, 1981, 481, 23981, 22481], [14482, 5982, 5482, 20982, 1482, 13982, 19482, 15982, 11482, 2482, 16982, 17982, 13482, 17482, 9482, 3982, 7482, 22982, 24482, 14982, 7982, 15482, 16482, 8482, 21482, 10482, 21982, 4482, 6982, 12982, 2982, 8982, 20482, 24982, 982, 6482, 18982, 12482, 3482, 11982, 18482, 10982, 9982, 4982, 19982, 23482, 1982, 482, 23982, 22482], [14483, 5983, 5483, 20983, 1483, 13983, 19483, 15983, 11483, 2483, 16983, 17983, 13483, 17483, 9483, 3983, 7483, 22983, 24483, 14983, 7983, 15483, 16483, 8483, 21483, 10483, 21983, 4483, 6983, 12983, 2983, 8983, 20483, 24983, 983, 6483, 18983, 12483, 3483, 11983, 18483, 10983, 9983, 4983, 19983, 23483, 1983, 483, 23983, 22483], [14484, 5984, 5484, 20984, 1484, 13984, 19484, 15984, 11484, 2484, 16984, 17984, 13484, 17484, 9484, 3984, 7484, 22984, 24484, 14984, 7984, 15484, 16484, 8484, 21484, 10484, 21984, 4484, 6984, 12984, 2984, 8984, 20484, 24984, 984, 6484, 18984, 12484, 3484, 11984, 18484, 10984, 9984, 4984, 19984, 23484, 1984, 484, 23984, 22484], [14485, 5985, 5485, 20985, 1485, 13985, 19485, 15985, 11485, 2485, 16985, 17985, 13485, 17485, 9485, 3985, 7485, 22985, 24485, 14985, 7985, 15485, 16485, 8485, 21485, 10485, 21985, 4485, 6985, 12985, 2985, 8985, 20485, 24985, 985, 6485, 18985, 12485, 3485, 11985, 18485, 10985, 9985, 4985, 19985, 23485, 1985, 485, 23985, 22485], [14486, 5986, 5486, 20986, 1486, 13986, 19486, 15986, 11486, 2486, 16986, 17986, 13486, 17486, 9486, 3986, 7486, 22986, 24486, 14986, 7986, 15486, 16486, 8486, 21486, 10486, 21986, 4486, 6986, 12986, 2986, 8986, 20486, 24986, 986, 6486, 18986, 12486, 3486, 11986, 18486, 10986, 9986, 4986, 19986, 23486, 1986, 486, 23986, 22486], [14487, 5987, 5487, 20987, 1487, 13987, 19487, 15987, 11487, 2487, 16987, 17987, 13487, 17487, 9487, 3987, 7487, 22987, 24487, 14987, 7987, 15487, 16487, 8487, 21487, 10487, 21987, 4487, 6987, 12987, 2987, 8987, 20487, 24987, 987, 6487, 18987, 12487, 3487, 11987, 18487, 10987, 9987, 4987, 19987, 23487, 1987, 487, 23987, 22487], [14488, 5988, 5488, 20988, 1488, 13988, 19488, 15988, 11488, 2488, 16988, 17988, 13488, 17488, 9488, 3988, 7488, 22988, 24488, 14988, 7988, 15488, 16488, 8488, 21488, 10488, 21988, 4488, 6988, 12988, 2988, 8988, 20488, 24988, 988, 6488, 18988, 12488, 3488, 11988, 18488, 10988, 9988, 4988, 19988, 23488, 1988, 488, 23988, 22488], [14489, 5989, 5489, 20989, 1489, 13989, 19489, 15989, 11489, 2489, 16989, 17989, 13489, 17489, 9489, 3989, 7489, 22989, 24489, 14989, 7989, 15489, 16489, 8489, 21489, 10489, 21989, 4489, 6989, 12989, 2989, 8989, 20489, 24989, 989, 6489, 18989, 12489, 3489, 11989, 18489, 10989, 9989, 4989, 19989, 23489, 1989, 489, 23989, 22489], [14490, 5990, 5490, 20990, 1490, 13990, 19490, 15990, 11490, 2490, 16990, 17990, 13490, 17490, 9490, 3990, 7490, 22990, 24490, 14990, 7990, 15490, 16490, 8490, 21490, 10490, 21990, 4490, 6990, 12990, 2990, 8990, 20490, 24990, 990, 6490, 18990, 12490, 3490, 11990, 18490, 10990, 9990, 4990, 19990, 23490, 1990, 490, 23990, 22490], [14491, 5991, 5491, 20991, 1491, 13991, 19491, 15991, 11491, 2491, 16991, 17991, 13491, 17491, 9491, 3991, 7491, 22991, 24491, 14991, 7991, 15491, 16491, 8491, 21491, 10491, 21991, 4491, 6991, 12991, 2991, 8991, 20491, 24991, 991, 6491, 18991, 12491, 3491, 11991, 18491, 10991, 9991, 4991, 19991, 23491, 1991, 491, 23991, 22491], [14492, 5992, 5492, 20992, 1492, 13992, 19492, 15992, 11492, 2492, 16992, 17992, 13492, 17492, 9492, 3992, 7492, 22992, 24492, 14992, 7992, 15492, 16492, 8492, 21492, 10492, 21992, 4492, 6992, 12992, 2992, 8992, 20492, 24992, 992, 6492, 18992, 12492, 3492, 11992, 18492, 10992, 9992, 4992, 19992, 23492, 1992, 492, 23992, 22492], [14493, 5993, 5493, 20993, 1493, 13993, 19493, 15993, 11493, 2493, 16993, 17993, 13493, 17493, 9493, 3993, 7493, 22993, 24493, 14993, 7993, 15493, 16493, 8493, 21493, 10493, 21993, 4493, 6993, 12993, 2993, 8993, 20493, 24993, 993, 6493, 18993, 12493, 3493, 11993, 18493, 10993, 9993, 4993, 19993, 23493, 1993, 493, 23993, 22493], [14494, 5994, 5494, 20994, 1494, 13994, 19494, 15994, 11494, 2494, 16994, 17994, 13494, 17494, 9494, 3994, 7494, 22994, 24494, 14994, 7994, 15494, 16494, 8494, 21494, 10494, 21994, 4494, 6994, 12994, 2994, 8994, 20494, 24994, 994, 6494, 18994, 12494, 3494, 11994, 18494, 10994, 9994, 4994, 19994, 23494, 1994, 494, 23994, 22494], [14495, 5995, 5495, 20995, 1495, 13995, 19495, 15995, 11495, 2495, 16995, 17995, 13495, 17495, 9495, 3995, 7495, 22995, 24495, 14995, 7995, 15495, 16495, 8495, 21495, 10495, 21995, 4495, 6995, 12995, 2995, 8995, 20495, 24995, 995, 6495, 18995, 12495, 3495, 11995, 18495, 10995, 9995, 4995, 19995, 23495, 1995, 495, 23995, 22495], [14496, 5996, 5496, 20996, 1496, 13996, 19496, 15996, 11496, 2496, 16996, 17996, 13496, 17496, 9496, 3996, 7496, 22996, 24496, 14996, 7996, 15496, 16496, 8496, 21496, 10496, 21996, 4496, 6996, 12996, 2996, 8996, 20496, 24996, 996, 6496, 18996, 12496, 3496, 11996, 18496, 10996, 9996, 4996, 19996, 23496, 1996, 496, 23996, 22496], [14497, 5997, 5497, 20997, 1497, 13997, 19497, 15997, 11497, 2497, 16997, 17997, 13497, 17497, 9497, 3997, 7497, 22997, 24497, 14997, 7997, 15497, 16497, 8497, 21497, 10497, 21997, 4497, 6997, 12997, 2997, 8997, 20497, 24997, 997, 6497, 18997, 12497, 3497, 11997, 18497, 10997, 9997, 4997, 19997, 23497, 1997, 497, 23997, 22497], [14498, 5998, 5498, 20998, 1498, 13998, 19498, 15998, 11498, 2498, 16998, 17998, 13498, 17498, 9498, 3998, 7498, 22998, 24498, 14998, 7998, 15498, 16498, 8498, 21498, 10498, 21998, 4498, 6998, 12998, 2998, 8998, 20498, 24998, 998, 6498, 18998, 12498, 3498, 11998, 18498, 10998, 9998, 4998, 19998, 23498, 1998, 498, 23998, 22498], [14499, 5999, 5499, 20999, 1499, 13999, 19499, 15999, 11499, 2499, 16999, 17999, 13499, 17499, 9499, 3999, 7499, 22999, 24499, 14999, 7999, 15499, 16499, 8499, 21499, 10499, 21999, 4499, 6999, 12999, 2999, 8999, 20499, 24999, 999, 6499, 18999, 12499, 3499, 11999, 18499, 10999, 9999, 4999, 19999, 23499, 1999, 499, 23999, 22499]]\ntest_leaves:  [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\nvalidation leaves:  [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer = TreeTrainer(\n",
    "    model = treevae,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "619392161788492\n",
      "ELBO Loss: 547.8903127655192\n",
      "training:  85%|████████▌ | 850/1000 [03:06<00:32,  4.58it/s]Encodings MP Likelihood: 3.1238883796369796\n",
      "ELBO Loss: 548.6046658091453\n",
      "training:  85%|████████▌ | 851/1000 [03:06<00:32,  4.55it/s]Encodings MP Likelihood: 3.0160012275014925\n",
      "ELBO Loss: 547.821594901664\n",
      "training:  85%|████████▌ | 852/1000 [03:07<00:32,  4.54it/s]Encodings MP Likelihood: 3.159468114825604\n",
      "ELBO Loss: 547.5561929569711\n",
      "training:  85%|████████▌ | 853/1000 [03:07<00:32,  4.56it/s]Encodings MP Likelihood: 2.9598585675118576\n",
      "ELBO Loss: 548.5187769716201\n",
      "training:  85%|████████▌ | 854/1000 [03:07<00:32,  4.53it/s]Encodings MP Likelihood: 2.846599525361267\n",
      "ELBO Loss: 546.6036250994849\n",
      "training:  86%|████████▌ | 855/1000 [03:07<00:31,  4.54it/s]Encodings MP Likelihood: 3.190666396941343\n",
      "ELBO Loss: 546.0382478930685\n",
      "training:  86%|████████▌ | 856/1000 [03:07<00:31,  4.50it/s]Encodings MP Likelihood: 3.466221648148704\n",
      "ELBO Loss: 548.1130543513677\n",
      "training:  86%|████████▌ | 857/1000 [03:08<00:32,  4.44it/s]Encodings MP Likelihood: 2.840215606346179\n",
      "ELBO Loss: 548.6227743307535\n",
      "training:  86%|████████▌ | 858/1000 [03:08<00:32,  4.33it/s]Encodings MP Likelihood: 3.135019782646614\n",
      "ELBO Loss: 548.2989180044091\n",
      "training:  86%|████████▌ | 859/1000 [03:08<00:32,  4.37it/s]Encodings MP Likelihood: 3.3083639936723217\n",
      "ELBO Loss: 547.1458440343266\n",
      "training:  86%|████████▌ | 860/1000 [03:08<00:31,  4.43it/s]Encodings MP Likelihood: 2.669535547259471\n",
      "ELBO Loss: 546.8371029311198\n",
      "training:  86%|████████▌ | 861/1000 [03:09<00:31,  4.44it/s]Encodings MP Likelihood: 2.6710361489409573\n",
      "ELBO Loss: 546.8519886229591\n",
      "training:  86%|████████▌ | 862/1000 [03:09<00:30,  4.48it/s]Encodings MP Likelihood: 2.8360517749427254\n",
      "ELBO Loss: 548.37819799974\n",
      "training:  86%|████████▋ | 863/1000 [03:09<00:30,  4.52it/s]Encodings MP Likelihood: 3.1881476312726527\n",
      "ELBO Loss: 549.511622974826\n",
      "training:  86%|████████▋ | 864/1000 [03:09<00:29,  4.54it/s]Encodings MP Likelihood: 2.8146255538620553\n",
      "ELBO Loss: 546.1442012103014\n",
      "training:  86%|████████▋ | 865/1000 [03:09<00:29,  4.55it/s]Encodings MP Likelihood: 3.187843397337317\n",
      "ELBO Loss: 547.8637455291265\n",
      "training:  87%|████████▋ | 866/1000 [03:10<00:29,  4.56it/s]Encodings MP Likelihood: 3.2292019288382523\n",
      "ELBO Loss: 548.0286855499495\n",
      "training:  87%|████████▋ | 867/1000 [03:10<00:29,  4.57it/s]Encodings MP Likelihood: 2.988868681847117\n",
      "ELBO Loss: 549.724937196308\n",
      "training:  87%|████████▋ | 868/1000 [03:10<00:28,  4.58it/s]Encodings MP Likelihood: 3.336042845815382\n",
      "ELBO Loss: 548.0538719394775\n",
      "training:  87%|████████▋ | 869/1000 [03:10<00:28,  4.58it/s]Encodings MP Likelihood: 3.115443307094335\n",
      "ELBO Loss: 548.8803968063613\n",
      "training:  87%|████████▋ | 870/1000 [03:11<00:28,  4.57it/s]Encodings MP Likelihood: 2.9541572631583084\n",
      "ELBO Loss: 547.8247489387904\n",
      "training:  87%|████████▋ | 871/1000 [03:11<00:28,  4.53it/s]Encodings MP Likelihood: 3.4472796983635696\n",
      "ELBO Loss: 547.6740445254488\n",
      "training:  87%|████████▋ | 872/1000 [03:11<00:28,  4.52it/s]Encodings MP Likelihood: 2.8274034364395644\n",
      "ELBO Loss: 548.5286842500915\n",
      "training:  87%|████████▋ | 873/1000 [03:11<00:28,  4.51it/s]Encodings MP Likelihood: 3.1374164956289676\n",
      "ELBO Loss: 549.6759701311723\n",
      "training:  87%|████████▋ | 874/1000 [03:11<00:28,  4.48it/s]Encodings MP Likelihood: 3.1306868705884754\n",
      "ELBO Loss: 548.6650669196298\n",
      "training:  88%|████████▊ | 875/1000 [03:12<00:27,  4.51it/s]Encodings MP Likelihood: 2.9577615385328095\n",
      "ELBO Loss: 546.5352686495313\n",
      "training:  88%|████████▊ | 876/1000 [03:12<00:27,  4.54it/s]Encodings MP Likelihood: 3.4322312922301803\n",
      "ELBO Loss: 548.8051095241054\n",
      "training:  88%|████████▊ | 877/1000 [03:12<00:26,  4.56it/s]Encodings MP Likelihood: 3.3922112798579174\n",
      "ELBO Loss: 549.4599043212492\n",
      "training:  88%|████████▊ | 878/1000 [03:12<00:26,  4.58it/s]Encodings MP Likelihood: 3.118641716799517\n",
      "ELBO Loss: 548.8819727181067\n",
      "training:  88%|████████▊ | 879/1000 [03:13<00:26,  4.57it/s]Encodings MP Likelihood: 3.1908707043807985\n",
      "ELBO Loss: 548.2009108826445\n",
      "training:  88%|████████▊ | 880/1000 [03:13<00:26,  4.57it/s]Encodings MP Likelihood: 2.82355500006542\n",
      "ELBO Loss: 547.4988605043285\n",
      "training:  88%|████████▊ | 881/1000 [03:13<00:26,  4.54it/s]Encodings MP Likelihood: 2.9749496502090556\n",
      "ELBO Loss: 547.8806867198822\n",
      "training:  88%|████████▊ | 882/1000 [03:13<00:26,  4.52it/s]Encodings MP Likelihood: 3.146444379840888\n",
      "ELBO Loss: 549.4571054583554\n",
      "training:  88%|████████▊ | 883/1000 [03:13<00:25,  4.52it/s]Encodings MP Likelihood: 2.7282308478013273\n",
      "ELBO Loss: 547.8741811773309\n",
      "training:  88%|████████▊ | 884/1000 [03:14<00:25,  4.53it/s]Encodings MP Likelihood: 2.816103676350824\n",
      "ELBO Loss: 547.0448871451429\n",
      "training:  88%|████████▊ | 885/1000 [03:14<00:25,  4.55it/s]Encodings MP Likelihood: 3.1016556022954265\n",
      "ELBO Loss: 548.3461868619283\n",
      "training:  89%|████████▊ | 886/1000 [03:14<00:25,  4.56it/s]Encodings MP Likelihood: 3.2113238321498048\n",
      "ELBO Loss: 549.6478104274381\n",
      "training:  89%|████████▊ | 887/1000 [03:14<00:24,  4.55it/s]Encodings MP Likelihood: 2.6665264520679712\n",
      "ELBO Loss: 546.6412246676758\n",
      "training:  89%|████████▉ | 888/1000 [03:15<00:24,  4.50it/s]Encodings MP Likelihood: 2.96563913294551\n",
      "ELBO Loss: 548.0673503673762\n",
      "training:  89%|████████▉ | 889/1000 [03:15<00:24,  4.48it/s]Encodings MP Likelihood: 2.967174513014959\n",
      "ELBO Loss: 548.0109546110016\n",
      "training:  89%|████████▉ | 890/1000 [03:15<00:24,  4.52it/s]Encodings MP Likelihood: 2.8784391652815504\n",
      "ELBO Loss: 547.5818037843674\n",
      "training:  89%|████████▉ | 891/1000 [03:15<00:24,  4.50it/s]Encodings MP Likelihood: 3.2590789829062827\n",
      "ELBO Loss: 548.8295368227898\n",
      "training:  89%|████████▉ | 892/1000 [03:15<00:23,  4.53it/s]Encodings MP Likelihood: 3.181488070833322\n",
      "ELBO Loss: 547.7420048581693\n",
      "training:  89%|████████▉ | 893/1000 [03:16<00:23,  4.54it/s]Encodings MP Likelihood: 2.9404398093716644\n",
      "ELBO Loss: 547.1630521170954\n",
      "training:  89%|████████▉ | 894/1000 [03:16<00:23,  4.51it/s]Encodings MP Likelihood: 3.358118072567361\n",
      "ELBO Loss: 547.9532652957507\n",
      "training:  90%|████████▉ | 895/1000 [03:16<00:23,  4.53it/s]Encodings MP Likelihood: 3.104941471610383\n",
      "ELBO Loss: 549.5834974994648\n",
      "training:  90%|████████▉ | 896/1000 [03:16<00:22,  4.53it/s]Encodings MP Likelihood: 3.301171295403511\n",
      "ELBO Loss: 548.4497706064448\n",
      "training:  90%|████████▉ | 897/1000 [03:17<00:23,  4.36it/s]Encodings MP Likelihood: 2.836481859736899\n",
      "ELBO Loss: 548.3719755715442\n",
      "training:  90%|████████▉ | 898/1000 [03:17<00:23,  4.30it/s]Encodings MP Likelihood: 2.890765435430163\n",
      "ELBO Loss: 546.4453765757708\n",
      "training:  90%|████████▉ | 899/1000 [03:17<00:23,  4.37it/s]Encodings MP Likelihood: 3.0019849375881216\n",
      "ELBO Loss: 549.0090843576534\n",
      "computing elbo\n",
      "training:  90%|█████████ | 900/1000 [03:17<00:25,  3.90it/s]Encodings MP Likelihood: 2.90654484172727\n",
      "ELBO Loss: 547.8760408503612\n",
      "training:  90%|█████████ | 901/1000 [03:18<00:24,  4.05it/s]Encodings MP Likelihood: 3.3025670705270276\n",
      "ELBO Loss: 547.6344290798884\n",
      "training:  90%|█████████ | 902/1000 [03:18<00:23,  4.19it/s]Encodings MP Likelihood: 3.017243977981487\n",
      "ELBO Loss: 548.0425059735859\n",
      "training:  90%|█████████ | 903/1000 [03:18<00:23,  4.10it/s]Encodings MP Likelihood: 3.2495953524667827\n",
      "ELBO Loss: 547.9910770376453\n",
      "training:  90%|█████████ | 904/1000 [03:18<00:23,  4.11it/s]Encodings MP Likelihood: 2.9542448756348283\n",
      "ELBO Loss: 548.0402021383966\n",
      "training:  90%|█████████ | 905/1000 [03:19<00:22,  4.22it/s]Encodings MP Likelihood: 2.6183709218509774\n",
      "ELBO Loss: 546.7905229555013\n",
      "training:  91%|█████████ | 906/1000 [03:19<00:21,  4.31it/s]Encodings MP Likelihood: 3.0059613802579017\n",
      "ELBO Loss: 547.8262053413379\n",
      "training:  91%|█████████ | 907/1000 [03:19<00:21,  4.36it/s]Encodings MP Likelihood: 3.128422373181507\n",
      "ELBO Loss: 547.5214402596888\n",
      "training:  91%|█████████ | 908/1000 [03:19<00:20,  4.41it/s]Encodings MP Likelihood: 3.1640938853946157\n",
      "ELBO Loss: 546.7765935571654\n",
      "training:  91%|█████████ | 909/1000 [03:19<00:20,  4.44it/s]Encodings MP Likelihood: 2.8863452493737247\n",
      "ELBO Loss: 547.6935451785914\n",
      "training:  91%|█████████ | 910/1000 [03:20<00:20,  4.44it/s]Encodings MP Likelihood: 3.258706172863663\n",
      "ELBO Loss: 548.832533311267\n",
      "training:  91%|█████████ | 911/1000 [03:20<00:19,  4.48it/s]Encodings MP Likelihood: 3.170157655265824\n",
      "ELBO Loss: 546.3946677769405\n",
      "training:  91%|█████████ | 912/1000 [03:20<00:19,  4.51it/s]Encodings MP Likelihood: 2.888847218125903\n",
      "ELBO Loss: 547.8468395850549\n",
      "training:  91%|█████████▏| 913/1000 [03:20<00:19,  4.52it/s]Encodings MP Likelihood: 2.6792853419604943\n",
      "ELBO Loss: 546.7507888041952\n",
      "training:  91%|█████████▏| 914/1000 [03:21<00:19,  4.52it/s]Encodings MP Likelihood: 2.9777613740302056\n",
      "ELBO Loss: 546.7474999117841\n",
      "training:  92%|█████████▏| 915/1000 [03:21<00:18,  4.54it/s]Encodings MP Likelihood: 2.9598073047842646\n",
      "ELBO Loss: 547.3905605502653\n",
      "training:  92%|█████████▏| 916/1000 [03:21<00:18,  4.55it/s]Encodings MP Likelihood: 3.0107080974609532\n",
      "ELBO Loss: 549.2856032874071\n",
      "training:  92%|█████████▏| 917/1000 [03:21<00:18,  4.57it/s]Encodings MP Likelihood: 2.9493507790958513\n",
      "ELBO Loss: 546.4073129434478\n",
      "training:  92%|█████████▏| 918/1000 [03:21<00:18,  4.44it/s]Encodings MP Likelihood: 3.2304763805537764\n",
      "ELBO Loss: 548.292271079581\n",
      "training:  92%|█████████▏| 919/1000 [03:22<00:18,  4.37it/s]Encodings MP Likelihood: 2.6831305905137475\n",
      "ELBO Loss: 548.4698835267309\n",
      "training:  92%|█████████▏| 920/1000 [03:22<00:18,  4.40it/s]Encodings MP Likelihood: 2.728341155206726\n",
      "ELBO Loss: 545.7069675044902\n",
      "training:  92%|█████████▏| 921/1000 [03:22<00:17,  4.43it/s]Encodings MP Likelihood: 3.074078663173292\n",
      "ELBO Loss: 547.4958475684036\n",
      "training:  92%|█████████▏| 922/1000 [03:22<00:17,  4.46it/s]Encodings MP Likelihood: 2.752646007812243\n",
      "ELBO Loss: 549.8869084547226\n",
      "training:  92%|█████████▏| 923/1000 [03:23<00:17,  4.49it/s]Encodings MP Likelihood: 3.225256872383241\n",
      "ELBO Loss: 547.9377500477907\n",
      "training:  92%|█████████▏| 924/1000 [03:23<00:16,  4.52it/s]Encodings MP Likelihood: 2.905371189888302\n",
      "ELBO Loss: 547.5438599896182\n",
      "training:  92%|█████████▎| 925/1000 [03:23<00:16,  4.49it/s]Encodings MP Likelihood: 3.070415977611875\n",
      "ELBO Loss: 548.4569420728354\n",
      "training:  93%|█████████▎| 926/1000 [03:23<00:16,  4.40it/s]Encodings MP Likelihood: 3.2634658309893463\n",
      "ELBO Loss: 547.7450427091499\n",
      "training:  93%|█████████▎| 927/1000 [03:23<00:16,  4.45it/s]Encodings MP Likelihood: 2.907539215822651\n",
      "ELBO Loss: 546.5668561863621\n",
      "training:  93%|█████████▎| 928/1000 [03:24<00:16,  4.48it/s]Encodings MP Likelihood: 3.320935619484767\n",
      "ELBO Loss: 549.6996535461209\n",
      "training:  93%|█████████▎| 929/1000 [03:24<00:15,  4.50it/s]Encodings MP Likelihood: 3.067526528336013\n",
      "ELBO Loss: 546.0023670272496\n",
      "training:  93%|█████████▎| 930/1000 [03:24<00:15,  4.50it/s]Encodings MP Likelihood: 3.280546662007815\n",
      "ELBO Loss: 546.9482134582071\n",
      "training:  93%|█████████▎| 931/1000 [03:24<00:15,  4.52it/s]Encodings MP Likelihood: 2.9847919189784875\n",
      "ELBO Loss: 547.2864599167946\n",
      "training:  93%|█████████▎| 932/1000 [03:25<00:14,  4.53it/s]Encodings MP Likelihood: 2.7450609717929644\n",
      "ELBO Loss: 547.0338308404678\n",
      "training:  93%|█████████▎| 933/1000 [03:25<00:14,  4.55it/s]Encodings MP Likelihood: 2.9685850891852885\n",
      "ELBO Loss: 549.1347930229696\n",
      "training:  93%|█████████▎| 934/1000 [03:25<00:14,  4.57it/s]Encodings MP Likelihood: 3.1812545491010504\n",
      "ELBO Loss: 546.5661273003735\n",
      "training:  94%|█████████▎| 935/1000 [03:25<00:14,  4.56it/s]Encodings MP Likelihood: 3.0034549341959633\n",
      "ELBO Loss: 545.5396903585264\n",
      "training:  94%|█████████▎| 936/1000 [03:25<00:14,  4.51it/s]Encodings MP Likelihood: 2.8586558224523295\n",
      "ELBO Loss: 549.1462979291916\n",
      "training:  94%|█████████▎| 937/1000 [03:26<00:13,  4.51it/s]Encodings MP Likelihood: 3.459024009485013\n",
      "ELBO Loss: 548.1919452753801\n",
      "training:  94%|█████████▍| 938/1000 [03:26<00:13,  4.54it/s]Encodings MP Likelihood: 3.30103276421553\n",
      "ELBO Loss: 548.7775395843798\n",
      "training:  94%|█████████▍| 939/1000 [03:26<00:13,  4.53it/s]Encodings MP Likelihood: 2.9995129154870326\n",
      "ELBO Loss: 548.3574065236688\n",
      "training:  94%|█████████▍| 940/1000 [03:26<00:13,  4.51it/s]Encodings MP Likelihood: 3.1552098179271697\n",
      "ELBO Loss: 548.3782023258955\n",
      "training:  94%|█████████▍| 941/1000 [03:27<00:13,  4.53it/s]Encodings MP Likelihood: 2.9981678916975345\n",
      "ELBO Loss: 546.6773650513418\n",
      "training:  94%|█████████▍| 942/1000 [03:27<00:12,  4.53it/s]Encodings MP Likelihood: 3.0505055899419777\n",
      "ELBO Loss: 547.6455917452444\n",
      "training:  94%|█████████▍| 943/1000 [03:27<00:12,  4.55it/s]Encodings MP Likelihood: 3.166713243101068\n",
      "ELBO Loss: 547.712706141223\n",
      "training:  94%|█████████▍| 944/1000 [03:27<00:12,  4.56it/s]Encodings MP Likelihood: 3.4251632203833773\n",
      "ELBO Loss: 548.8350829578013\n",
      "training:  94%|█████████▍| 945/1000 [03:27<00:12,  4.56it/s]Encodings MP Likelihood: 3.102106515544808\n",
      "ELBO Loss: 547.3845364941147\n",
      "training:  95%|█████████▍| 946/1000 [03:28<00:11,  4.53it/s]Encodings MP Likelihood: 3.0609826013612573\n",
      "ELBO Loss: 547.2725226141904\n",
      "training:  95%|█████████▍| 947/1000 [03:28<00:11,  4.52it/s]Encodings MP Likelihood: 2.953233152808794\n",
      "ELBO Loss: 547.429950676077\n",
      "training:  95%|█████████▍| 948/1000 [03:28<00:11,  4.53it/s]Encodings MP Likelihood: 3.023813084491046\n",
      "ELBO Loss: 546.8180718312376\n",
      "training:  95%|█████████▍| 949/1000 [03:28<00:11,  4.50it/s]Encodings MP Likelihood: 3.155312936090845\n",
      "ELBO Loss: 547.9595699204616\n",
      "training:  95%|█████████▌| 950/1000 [03:29<00:11,  4.35it/s]Encodings MP Likelihood: 2.8580755317289626\n",
      "ELBO Loss: 548.0206528539193\n",
      "training:  95%|█████████▌| 951/1000 [03:29<00:11,  4.41it/s]Encodings MP Likelihood: 2.9083580275319476\n",
      "ELBO Loss: 547.2747902160932\n",
      "training:  95%|█████████▌| 952/1000 [03:29<00:10,  4.46it/s]Encodings MP Likelihood: 2.6481638125060716\n",
      "ELBO Loss: 546.9552559425415\n",
      "training:  95%|█████████▌| 953/1000 [03:29<00:10,  4.29it/s]Encodings MP Likelihood: 2.6800361370385453\n",
      "ELBO Loss: 547.0296524727389\n",
      "training:  95%|█████████▌| 954/1000 [03:29<00:10,  4.31it/s]Encodings MP Likelihood: 3.2215721981477348\n",
      "ELBO Loss: 549.1779547737772\n",
      "training:  96%|█████████▌| 955/1000 [03:30<00:10,  4.39it/s]Encodings MP Likelihood: 3.113752810518118\n",
      "ELBO Loss: 546.0397282771773\n",
      "training:  96%|█████████▌| 956/1000 [03:30<00:09,  4.42it/s]Encodings MP Likelihood: 3.1354362207057123\n",
      "ELBO Loss: 547.8363855244035\n",
      "training:  96%|█████████▌| 957/1000 [03:30<00:10,  4.25it/s]Encodings MP Likelihood: 2.868571103650801\n",
      "ELBO Loss: 547.7889640096867\n",
      "training:  96%|█████████▌| 958/1000 [03:30<00:09,  4.33it/s]Encodings MP Likelihood: 2.994872390829325\n",
      "ELBO Loss: 547.8637485105019\n",
      "training:  96%|█████████▌| 959/1000 [03:31<00:09,  4.37it/s]Encodings MP Likelihood: 2.7303799104135122\n",
      "ELBO Loss: 548.5526753385461\n",
      "training:  96%|█████████▌| 960/1000 [03:31<00:09,  4.43it/s]Encodings MP Likelihood: 3.133203284010778\n",
      "ELBO Loss: 547.3090181810779\n",
      "training:  96%|█████████▌| 961/1000 [03:31<00:08,  4.48it/s]Encodings MP Likelihood: 2.922258653480943\n",
      "ELBO Loss: 547.8178485622549\n",
      "training:  96%|█████████▌| 962/1000 [03:31<00:08,  4.51it/s]Encodings MP Likelihood: 3.394724750465905\n",
      "ELBO Loss: 546.693997633005\n",
      "training:  96%|█████████▋| 963/1000 [03:31<00:08,  4.49it/s]Encodings MP Likelihood: 3.1516872307213695\n",
      "ELBO Loss: 545.3527875027959\n",
      "training:  96%|█████████▋| 964/1000 [03:32<00:07,  4.52it/s]Encodings MP Likelihood: 2.9553740436095315\n",
      "ELBO Loss: 546.3988574490126\n",
      "training:  96%|█████████▋| 965/1000 [03:32<00:07,  4.54it/s]Encodings MP Likelihood: 3.1207064294996867\n",
      "ELBO Loss: 546.5125398917102\n",
      "training:  97%|█████████▋| 966/1000 [03:32<00:07,  4.56it/s]Encodings MP Likelihood: 3.2477024465383195\n",
      "ELBO Loss: 547.7530407399856\n",
      "training:  97%|█████████▋| 967/1000 [03:32<00:07,  4.57it/s]Encodings MP Likelihood: 2.8096408015806884\n",
      "ELBO Loss: 546.5601288051638\n",
      "training:  97%|█████████▋| 968/1000 [03:33<00:07,  4.54it/s]Encodings MP Likelihood: 3.4974950766531756\n",
      "ELBO Loss: 548.3272991091633\n",
      "training:  97%|█████████▋| 969/1000 [03:33<00:06,  4.52it/s]Encodings MP Likelihood: 3.085742175217086\n",
      "ELBO Loss: 547.063989086699\n",
      "training:  97%|█████████▋| 970/1000 [03:33<00:06,  4.52it/s]Encodings MP Likelihood: 2.874012553999928\n",
      "ELBO Loss: 548.1090521118401\n",
      "training:  97%|█████████▋| 971/1000 [03:33<00:06,  4.53it/s]Encodings MP Likelihood: 2.9030224719584274\n",
      "ELBO Loss: 547.6070993632029\n",
      "training:  97%|█████████▋| 972/1000 [03:33<00:06,  4.55it/s]Encodings MP Likelihood: 3.0054926444913623\n",
      "ELBO Loss: 547.1358567689729\n",
      "training:  97%|█████████▋| 973/1000 [03:34<00:05,  4.56it/s]Encodings MP Likelihood: 2.889142700829273\n",
      "ELBO Loss: 546.6870336018611\n",
      "training:  97%|█████████▋| 974/1000 [03:34<00:05,  4.57it/s]Encodings MP Likelihood: 2.940120777083965\n",
      "ELBO Loss: 547.4545152095375\n",
      "training:  98%|█████████▊| 975/1000 [03:34<00:05,  4.57it/s]Encodings MP Likelihood: 2.8368686755309147\n",
      "ELBO Loss: 546.2769267649844\n",
      "training:  98%|█████████▊| 976/1000 [03:34<00:05,  4.58it/s]Encodings MP Likelihood: 2.8515212071596783\n",
      "ELBO Loss: 546.0131748025684\n",
      "training:  98%|█████████▊| 977/1000 [03:35<00:05,  4.56it/s]Encodings MP Likelihood: 2.939455209124553\n",
      "ELBO Loss: 547.7138054233255\n",
      "training:  98%|█████████▊| 978/1000 [03:35<00:04,  4.54it/s]Encodings MP Likelihood: 3.537248914177133\n",
      "ELBO Loss: 547.9761240728068\n",
      "training:  98%|█████████▊| 979/1000 [03:35<00:04,  4.49it/s]Encodings MP Likelihood: 3.075205524910022\n",
      "ELBO Loss: 547.1979589901027\n",
      "training:  98%|█████████▊| 980/1000 [03:35<00:04,  4.38it/s]Encodings MP Likelihood: 3.1170544015328123\n",
      "ELBO Loss: 548.189296497824\n",
      "training:  98%|█████████▊| 981/1000 [03:35<00:04,  4.33it/s]Encodings MP Likelihood: 2.9971692670059604\n",
      "ELBO Loss: 546.1372019026505\n",
      "training:  98%|█████████▊| 982/1000 [03:36<00:04,  4.40it/s]Encodings MP Likelihood: 2.952481916372637\n",
      "ELBO Loss: 549.3510556661832\n",
      "training:  98%|█████████▊| 983/1000 [03:36<00:03,  4.46it/s]Encodings MP Likelihood: 2.954262954905052\n",
      "ELBO Loss: 546.7947010152325\n",
      "training:  98%|█████████▊| 984/1000 [03:36<00:03,  4.40it/s]Encodings MP Likelihood: 2.9427044863358542\n",
      "ELBO Loss: 546.8536085038318\n",
      "training:  98%|█████████▊| 985/1000 [03:36<00:03,  4.36it/s]Encodings MP Likelihood: 3.153201539556028\n",
      "ELBO Loss: 548.2997954745528\n",
      "training:  99%|█████████▊| 986/1000 [03:37<00:03,  4.43it/s]Encodings MP Likelihood: 3.2839069078474976\n",
      "ELBO Loss: 548.7417207309297\n",
      "training:  99%|█████████▊| 987/1000 [03:37<00:02,  4.48it/s]Encodings MP Likelihood: 2.947397792448695\n",
      "ELBO Loss: 545.9178206635387\n",
      "training:  99%|█████████▉| 988/1000 [03:37<00:02,  4.50it/s]Encodings MP Likelihood: 2.937235876134123\n",
      "ELBO Loss: 548.3141418533877\n",
      "training:  99%|█████████▉| 989/1000 [03:37<00:02,  4.49it/s]Encodings MP Likelihood: 3.059162430509576\n",
      "ELBO Loss: 547.4808752818186\n",
      "training:  99%|█████████▉| 990/1000 [03:37<00:02,  4.52it/s]Encodings MP Likelihood: 2.8452628803197295\n",
      "ELBO Loss: 546.2030760550614\n",
      "training:  99%|█████████▉| 991/1000 [03:38<00:01,  4.53it/s]Encodings MP Likelihood: 2.500095988353151\n",
      "ELBO Loss: 548.9496500115819\n",
      "training:  99%|█████████▉| 992/1000 [03:38<00:01,  4.55it/s]Encodings MP Likelihood: 3.3828040598603653\n",
      "ELBO Loss: 547.5434686712679\n",
      "training:  99%|█████████▉| 993/1000 [03:38<00:01,  4.56it/s]Encodings MP Likelihood: 2.775925585530649\n",
      "ELBO Loss: 545.1979694424973\n",
      "training:  99%|█████████▉| 994/1000 [03:38<00:01,  4.53it/s]Encodings MP Likelihood: 3.012632558897986\n",
      "ELBO Loss: 547.2553983299601\n",
      "training: 100%|█████████▉| 995/1000 [03:39<00:01,  4.54it/s]Encodings MP Likelihood: 3.1441388720485186\n",
      "ELBO Loss: 549.0434661340494\n",
      "training: 100%|█████████▉| 996/1000 [03:39<00:00,  4.56it/s]Encodings MP Likelihood: 3.0309777615557936\n",
      "ELBO Loss: 547.7677506249971\n",
      "training: 100%|█████████▉| 997/1000 [03:39<00:00,  4.57it/s]Encodings MP Likelihood: 2.593080652524333\n",
      "ELBO Loss: 544.9909730035064\n",
      "training: 100%|█████████▉| 998/1000 [03:39<00:00,  4.56it/s]Encodings MP Likelihood: 3.006868002505653\n",
      "ELBO Loss: 548.6123946441598\n",
      "training: 100%|█████████▉| 999/1000 [03:39<00:00,  4.52it/s]Encodings MP Likelihood: 3.129953655171519\n",
      "ELBO Loss: 548.5788904543829\n",
      "computing elbo\n",
      "training: 100%|██████████| 1000/1000 [03:40<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dashboard(trainer, treevae.encoder_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "computing elbo\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(541.6196, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "full_posterior = trainer.create_posterior(trainer.model, cas_dataset, trainer.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "\n",
    "full_posterior.compute_elbo(treevae)"
   ]
  },
  {
   "source": [
    "***Elbo on held-out log-likelihood***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2384.3051, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "elbo_cascvi = 0\n",
    "with torch.no_grad():\n",
    "    input = torch.from_numpy(leaves_X).float().to(device='cuda:0')\n",
    "    reconst_loss, qz, mp_lik = treevae.forward(input)\n",
    "    elbo_cascvi += torch.sum(reconst_loss)\n",
    "    elbo_cascvi += lambda_ * torch.sum(qz)\n",
    "    elbo_cascvi -= lambda_ * mp_lik\n",
    "\n",
    "n_samples = leaves_X.shape[0]\n",
    "elbo_cascvi /= n_samples\n",
    "\n",
    "elbo_cascvi"
   ]
  },
  {
   "source": [
    "# 3. Fitting scVI - full batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "treevae_full = TreeVAE(cas_dataset.nb_genes,\n",
    "              tree = cas_dataset.tree,\n",
    "              n_latent=glm.latent,\n",
    "              n_hidden=128,\n",
    "              n_layers=1,\n",
    "              reconstruction_loss='poisson',\n",
    "              prior_t = branch_length,\n",
    "              ldvae = ldvae,\n",
    "              use_MP=False,\n",
    "              use_clades=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8951, 20451, 24951, 951, 6451, 18951, 12451, 3451, 11951, 18451, 10951, 9951, 4951, 19951, 23451, 1951, 451, 23951, 22451], [14452, 5952, 5452, 20952, 1452, 13952, 19452, 15952, 11452, 2452, 16952, 17952, 13452, 17452, 9452, 3952, 7452, 22952, 24452, 14952, 7952, 15452, 16452, 8452, 21452, 10452, 21952, 4452, 6952, 12952, 2952, 8952, 20452, 24952, 952, 6452, 18952, 12452, 3452, 11952, 18452, 10952, 9952, 4952, 19952, 23452, 1952, 452, 23952, 22452], [14453, 5953, 5453, 20953, 1453, 13953, 19453, 15953, 11453, 2453, 16953, 17953, 13453, 17453, 9453, 3953, 7453, 22953, 24453, 14953, 7953, 15453, 16453, 8453, 21453, 10453, 21953, 4453, 6953, 12953, 2953, 8953, 20453, 24953, 953, 6453, 18953, 12453, 3453, 11953, 18453, 10953, 9953, 4953, 19953, 23453, 1953, 453, 23953, 22453], [14454, 5954, 5454, 20954, 1454, 13954, 19454, 15954, 11454, 2454, 16954, 17954, 13454, 17454, 9454, 3954, 7454, 22954, 24454, 14954, 7954, 15454, 16454, 8454, 21454, 10454, 21954, 4454, 6954, 12954, 2954, 8954, 20454, 24954, 954, 6454, 18954, 12454, 3454, 11954, 18454, 10954, 9954, 4954, 19954, 23454, 1954, 454, 23954, 22454], [14455, 5955, 5455, 20955, 1455, 13955, 19455, 15955, 11455, 2455, 16955, 17955, 13455, 17455, 9455, 3955, 7455, 22955, 24455, 14955, 7955, 15455, 16455, 8455, 21455, 10455, 21955, 4455, 6955, 12955, 2955, 8955, 20455, 24955, 955, 6455, 18955, 12455, 3455, 11955, 18455, 10955, 9955, 4955, 19955, 23455, 1955, 455, 23955, 22455], [14456, 5956, 5456, 20956, 1456, 13956, 19456, 15956, 11456, 2456, 16956, 17956, 13456, 17456, 9456, 3956, 7456, 22956, 24456, 14956, 7956, 15456, 16456, 8456, 21456, 10456, 21956, 4456, 6956, 12956, 2956, 8956, 20456, 24956, 956, 6456, 18956, 12456, 3456, 11956, 18456, 10956, 9956, 4956, 19956, 23456, 1956, 456, 23956, 22456], [14457, 5957, 5457, 20957, 1457, 13957, 19457, 15957, 11457, 2457, 16957, 17957, 13457, 17457, 9457, 3957, 7457, 22957, 24457, 14957, 7957, 15457, 16457, 8457, 21457, 10457, 21957, 4457, 6957, 12957, 2957, 8957, 20457, 24957, 957, 6457, 18957, 12457, 3457, 11957, 18457, 10957, 9957, 4957, 19957, 23457, 1957, 457, 23957, 22457], [14458, 5958, 5458, 20958, 1458, 13958, 19458, 15958, 11458, 2458, 16958, 17958, 13458, 17458, 9458, 3958, 7458, 22958, 24458, 14958, 7958, 15458, 16458, 8458, 21458, 10458, 21958, 4458, 6958, 12958, 2958, 8958, 20458, 24958, 958, 6458, 18958, 12458, 3458, 11958, 18458, 10958, 9958, 4958, 19958, 23458, 1958, 458, 23958, 22458], [14459, 5959, 5459, 20959, 1459, 13959, 19459, 15959, 11459, 2459, 16959, 17959, 13459, 17459, 9459, 3959, 7459, 22959, 24459, 14959, 7959, 15459, 16459, 8459, 21459, 10459, 21959, 4459, 6959, 12959, 2959, 8959, 20459, 24959, 959, 6459, 18959, 12459, 3459, 11959, 18459, 10959, 9959, 4959, 19959, 23459, 1959, 459, 23959, 22459], [14460, 5960, 5460, 20960, 1460, 13960, 19460, 15960, 11460, 2460, 16960, 17960, 13460, 17460, 9460, 3960, 7460, 22960, 24460, 14960, 7960, 15460, 16460, 8460, 21460, 10460, 21960, 4460, 6960, 12960, 2960, 8960, 20460, 24960, 960, 6460, 18960, 12460, 3460, 11960, 18460, 10960, 9960, 4960, 19960, 23460, 1960, 460, 23960, 22460], [14461, 5961, 5461, 20961, 1461, 13961, 19461, 15961, 11461, 2461, 16961, 17961, 13461, 17461, 9461, 3961, 7461, 22961, 24461, 14961, 7961, 15461, 16461, 8461, 21461, 10461, 21961, 4461, 6961, 12961, 2961, 8961, 20461, 24961, 961, 6461, 18961, 12461, 3461, 11961, 18461, 10961, 9961, 4961, 19961, 23461, 1961, 461, 23961, 22461], [14462, 5962, 5462, 20962, 1462, 13962, 19462, 15962, 11462, 2462, 16962, 17962, 13462, 17462, 9462, 3962, 7462, 22962, 24462, 14962, 7962, 15462, 16462, 8462, 21462, 10462, 21962, 4462, 6962, 12962, 2962, 8962, 20462, 24962, 962, 6462, 18962, 12462, 3462, 11962, 18462, 10962, 9962, 4962, 19962, 23462, 1962, 462, 23962, 22462], [14463, 5963, 5463, 20963, 1463, 13963, 19463, 15963, 11463, 2463, 16963, 17963, 13463, 17463, 9463, 3963, 7463, 22963, 24463, 14963, 7963, 15463, 16463, 8463, 21463, 10463, 21963, 4463, 6963, 12963, 2963, 8963, 20463, 24963, 963, 6463, 18963, 12463, 3463, 11963, 18463, 10963, 9963, 4963, 19963, 23463, 1963, 463, 23963, 22463], [14464, 5964, 5464, 20964, 1464, 13964, 19464, 15964, 11464, 2464, 16964, 17964, 13464, 17464, 9464, 3964, 7464, 22964, 24464, 14964, 7964, 15464, 16464, 8464, 21464, 10464, 21964, 4464, 6964, 12964, 2964, 8964, 20464, 24964, 964, 6464, 18964, 12464, 3464, 11964, 18464, 10964, 9964, 4964, 19964, 23464, 1964, 464, 23964, 22464], [14465, 5965, 5465, 20965, 1465, 13965, 19465, 15965, 11465, 2465, 16965, 17965, 13465, 17465, 9465, 3965, 7465, 22965, 24465, 14965, 7965, 15465, 16465, 8465, 21465, 10465, 21965, 4465, 6965, 12965, 2965, 8965, 20465, 24965, 965, 6465, 18965, 12465, 3465, 11965, 18465, 10965, 9965, 4965, 19965, 23465, 1965, 465, 23965, 22465], [14466, 5966, 5466, 20966, 1466, 13966, 19466, 15966, 11466, 2466, 16966, 17966, 13466, 17466, 9466, 3966, 7466, 22966, 24466, 14966, 7966, 15466, 16466, 8466, 21466, 10466, 21966, 4466, 6966, 12966, 2966, 8966, 20466, 24966, 966, 6466, 18966, 12466, 3466, 11966, 18466, 10966, 9966, 4966, 19966, 23466, 1966, 466, 23966, 22466], [14467, 5967, 5467, 20967, 1467, 13967, 19467, 15967, 11467, 2467, 16967, 17967, 13467, 17467, 9467, 3967, 7467, 22967, 24467, 14967, 7967, 15467, 16467, 8467, 21467, 10467, 21967, 4467, 6967, 12967, 2967, 8967, 20467, 24967, 967, 6467, 18967, 12467, 3467, 11967, 18467, 10967, 9967, 4967, 19967, 23467, 1967, 467, 23967, 22467], [14468, 5968, 5468, 20968, 1468, 13968, 19468, 15968, 11468, 2468, 16968, 17968, 13468, 17468, 9468, 3968, 7468, 22968, 24468, 14968, 7968, 15468, 16468, 8468, 21468, 10468, 21968, 4468, 6968, 12968, 2968, 8968, 20468, 24968, 968, 6468, 18968, 12468, 3468, 11968, 18468, 10968, 9968, 4968, 19968, 23468, 1968, 468, 23968, 22468], [14469, 5969, 5469, 20969, 1469, 13969, 19469, 15969, 11469, 2469, 16969, 17969, 13469, 17469, 9469, 3969, 7469, 22969, 24469, 14969, 7969, 15469, 16469, 8469, 21469, 10469, 21969, 4469, 6969, 12969, 2969, 8969, 20469, 24969, 969, 6469, 18969, 12469, 3469, 11969, 18469, 10969, 9969, 4969, 19969, 23469, 1969, 469, 23969, 22469], [14470, 5970, 5470, 20970, 1470, 13970, 19470, 15970, 11470, 2470, 16970, 17970, 13470, 17470, 9470, 3970, 7470, 22970, 24470, 14970, 7970, 15470, 16470, 8470, 21470, 10470, 21970, 4470, 6970, 12970, 2970, 8970, 20470, 24970, 970, 6470, 18970, 12470, 3470, 11970, 18470, 10970, 9970, 4970, 19970, 23470, 1970, 470, 23970, 22470], [14471, 5971, 5471, 20971, 1471, 13971, 19471, 15971, 11471, 2471, 16971, 17971, 13471, 17471, 9471, 3971, 7471, 22971, 24471, 14971, 7971, 15471, 16471, 8471, 21471, 10471, 21971, 4471, 6971, 12971, 2971, 8971, 20471, 24971, 971, 6471, 18971, 12471, 3471, 11971, 18471, 10971, 9971, 4971, 19971, 23471, 1971, 471, 23971, 22471], [14472, 5972, 5472, 20972, 1472, 13972, 19472, 15972, 11472, 2472, 16972, 17972, 13472, 17472, 9472, 3972, 7472, 22972, 24472, 14972, 7972, 15472, 16472, 8472, 21472, 10472, 21972, 4472, 6972, 12972, 2972, 8972, 20472, 24972, 972, 6472, 18972, 12472, 3472, 11972, 18472, 10972, 9972, 4972, 19972, 23472, 1972, 472, 23972, 22472], [14473, 5973, 5473, 20973, 1473, 13973, 19473, 15973, 11473, 2473, 16973, 17973, 13473, 17473, 9473, 3973, 7473, 22973, 24473, 14973, 7973, 15473, 16473, 8473, 21473, 10473, 21973, 4473, 6973, 12973, 2973, 8973, 20473, 24973, 973, 6473, 18973, 12473, 3473, 11973, 18473, 10973, 9973, 4973, 19973, 23473, 1973, 473, 23973, 22473], [14474, 5974, 5474, 20974, 1474, 13974, 19474, 15974, 11474, 2474, 16974, 17974, 13474, 17474, 9474, 3974, 7474, 22974, 24474, 14974, 7974, 15474, 16474, 8474, 21474, 10474, 21974, 4474, 6974, 12974, 2974, 8974, 20474, 24974, 974, 6474, 18974, 12474, 3474, 11974, 18474, 10974, 9974, 4974, 19974, 23474, 1974, 474, 23974, 22474], [14475, 5975, 5475, 20975, 1475, 13975, 19475, 15975, 11475, 2475, 16975, 17975, 13475, 17475, 9475, 3975, 7475, 22975, 24475, 14975, 7975, 15475, 16475, 8475, 21475, 10475, 21975, 4475, 6975, 12975, 2975, 8975, 20475, 24975, 975, 6475, 18975, 12475, 3475, 11975, 18475, 10975, 9975, 4975, 19975, 23475, 1975, 475, 23975, 22475], [14476, 5976, 5476, 20976, 1476, 13976, 19476, 15976, 11476, 2476, 16976, 17976, 13476, 17476, 9476, 3976, 7476, 22976, 24476, 14976, 7976, 15476, 16476, 8476, 21476, 10476, 21976, 4476, 6976, 12976, 2976, 8976, 20476, 24976, 976, 6476, 18976, 12476, 3476, 11976, 18476, 10976, 9976, 4976, 19976, 23476, 1976, 476, 23976, 22476], [14477, 5977, 5477, 20977, 1477, 13977, 19477, 15977, 11477, 2477, 16977, 17977, 13477, 17477, 9477, 3977, 7477, 22977, 24477, 14977, 7977, 15477, 16477, 8477, 21477, 10477, 21977, 4477, 6977, 12977, 2977, 8977, 20477, 24977, 977, 6477, 18977, 12477, 3477, 11977, 18477, 10977, 9977, 4977, 19977, 23477, 1977, 477, 23977, 22477], [14478, 5978, 5478, 20978, 1478, 13978, 19478, 15978, 11478, 2478, 16978, 17978, 13478, 17478, 9478, 3978, 7478, 22978, 24478, 14978, 7978, 15478, 16478, 8478, 21478, 10478, 21978, 4478, 6978, 12978, 2978, 8978, 20478, 24978, 978, 6478, 18978, 12478, 3478, 11978, 18478, 10978, 9978, 4978, 19978, 23478, 1978, 478, 23978, 22478], [14479, 5979, 5479, 20979, 1479, 13979, 19479, 15979, 11479, 2479, 16979, 17979, 13479, 17479, 9479, 3979, 7479, 22979, 24479, 14979, 7979, 15479, 16479, 8479, 21479, 10479, 21979, 4479, 6979, 12979, 2979, 8979, 20479, 24979, 979, 6479, 18979, 12479, 3479, 11979, 18479, 10979, 9979, 4979, 19979, 23479, 1979, 479, 23979, 22479], [14480, 5980, 5480, 20980, 1480, 13980, 19480, 15980, 11480, 2480, 16980, 17980, 13480, 17480, 9480, 3980, 7480, 22980, 24480, 14980, 7980, 15480, 16480, 8480, 21480, 10480, 21980, 4480, 6980, 12980, 2980, 8980, 20480, 24980, 980, 6480, 18980, 12480, 3480, 11980, 18480, 10980, 9980, 4980, 19980, 23480, 1980, 480, 23980, 22480], [14481, 5981, 5481, 20981, 1481, 13981, 19481, 15981, 11481, 2481, 16981, 17981, 13481, 17481, 9481, 3981, 7481, 22981, 24481, 14981, 7981, 15481, 16481, 8481, 21481, 10481, 21981, 4481, 6981, 12981, 2981, 8981, 20481, 24981, 981, 6481, 18981, 12481, 3481, 11981, 18481, 10981, 9981, 4981, 19981, 23481, 1981, 481, 23981, 22481], [14482, 5982, 5482, 20982, 1482, 13982, 19482, 15982, 11482, 2482, 16982, 17982, 13482, 17482, 9482, 3982, 7482, 22982, 24482, 14982, 7982, 15482, 16482, 8482, 21482, 10482, 21982, 4482, 6982, 12982, 2982, 8982, 20482, 24982, 982, 6482, 18982, 12482, 3482, 11982, 18482, 10982, 9982, 4982, 19982, 23482, 1982, 482, 23982, 22482], [14483, 5983, 5483, 20983, 1483, 13983, 19483, 15983, 11483, 2483, 16983, 17983, 13483, 17483, 9483, 3983, 7483, 22983, 24483, 14983, 7983, 15483, 16483, 8483, 21483, 10483, 21983, 4483, 6983, 12983, 2983, 8983, 20483, 24983, 983, 6483, 18983, 12483, 3483, 11983, 18483, 10983, 9983, 4983, 19983, 23483, 1983, 483, 23983, 22483], [14484, 5984, 5484, 20984, 1484, 13984, 19484, 15984, 11484, 2484, 16984, 17984, 13484, 17484, 9484, 3984, 7484, 22984, 24484, 14984, 7984, 15484, 16484, 8484, 21484, 10484, 21984, 4484, 6984, 12984, 2984, 8984, 20484, 24984, 984, 6484, 18984, 12484, 3484, 11984, 18484, 10984, 9984, 4984, 19984, 23484, 1984, 484, 23984, 22484], [14485, 5985, 5485, 20985, 1485, 13985, 19485, 15985, 11485, 2485, 16985, 17985, 13485, 17485, 9485, 3985, 7485, 22985, 24485, 14985, 7985, 15485, 16485, 8485, 21485, 10485, 21985, 4485, 6985, 12985, 2985, 8985, 20485, 24985, 985, 6485, 18985, 12485, 3485, 11985, 18485, 10985, 9985, 4985, 19985, 23485, 1985, 485, 23985, 22485], [14486, 5986, 5486, 20986, 1486, 13986, 19486, 15986, 11486, 2486, 16986, 17986, 13486, 17486, 9486, 3986, 7486, 22986, 24486, 14986, 7986, 15486, 16486, 8486, 21486, 10486, 21986, 4486, 6986, 12986, 2986, 8986, 20486, 24986, 986, 6486, 18986, 12486, 3486, 11986, 18486, 10986, 9986, 4986, 19986, 23486, 1986, 486, 23986, 22486], [14487, 5987, 5487, 20987, 1487, 13987, 19487, 15987, 11487, 2487, 16987, 17987, 13487, 17487, 9487, 3987, 7487, 22987, 24487, 14987, 7987, 15487, 16487, 8487, 21487, 10487, 21987, 4487, 6987, 12987, 2987, 8987, 20487, 24987, 987, 6487, 18987, 12487, 3487, 11987, 18487, 10987, 9987, 4987, 19987, 23487, 1987, 487, 23987, 22487], [14488, 5988, 5488, 20988, 1488, 13988, 19488, 15988, 11488, 2488, 16988, 17988, 13488, 17488, 9488, 3988, 7488, 22988, 24488, 14988, 7988, 15488, 16488, 8488, 21488, 10488, 21988, 4488, 6988, 12988, 2988, 8988, 20488, 24988, 988, 6488, 18988, 12488, 3488, 11988, 18488, 10988, 9988, 4988, 19988, 23488, 1988, 488, 23988, 22488], [14489, 5989, 5489, 20989, 1489, 13989, 19489, 15989, 11489, 2489, 16989, 17989, 13489, 17489, 9489, 3989, 7489, 22989, 24489, 14989, 7989, 15489, 16489, 8489, 21489, 10489, 21989, 4489, 6989, 12989, 2989, 8989, 20489, 24989, 989, 6489, 18989, 12489, 3489, 11989, 18489, 10989, 9989, 4989, 19989, 23489, 1989, 489, 23989, 22489], [14490, 5990, 5490, 20990, 1490, 13990, 19490, 15990, 11490, 2490, 16990, 17990, 13490, 17490, 9490, 3990, 7490, 22990, 24490, 14990, 7990, 15490, 16490, 8490, 21490, 10490, 21990, 4490, 6990, 12990, 2990, 8990, 20490, 24990, 990, 6490, 18990, 12490, 3490, 11990, 18490, 10990, 9990, 4990, 19990, 23490, 1990, 490, 23990, 22490], [14491, 5991, 5491, 20991, 1491, 13991, 19491, 15991, 11491, 2491, 16991, 17991, 13491, 17491, 9491, 3991, 7491, 22991, 24491, 14991, 7991, 15491, 16491, 8491, 21491, 10491, 21991, 4491, 6991, 12991, 2991, 8991, 20491, 24991, 991, 6491, 18991, 12491, 3491, 11991, 18491, 10991, 9991, 4991, 19991, 23491, 1991, 491, 23991, 22491], [14492, 5992, 5492, 20992, 1492, 13992, 19492, 15992, 11492, 2492, 16992, 17992, 13492, 17492, 9492, 3992, 7492, 22992, 24492, 14992, 7992, 15492, 16492, 8492, 21492, 10492, 21992, 4492, 6992, 12992, 2992, 8992, 20492, 24992, 992, 6492, 18992, 12492, 3492, 11992, 18492, 10992, 9992, 4992, 19992, 23492, 1992, 492, 23992, 22492], [14493, 5993, 5493, 20993, 1493, 13993, 19493, 15993, 11493, 2493, 16993, 17993, 13493, 17493, 9493, 3993, 7493, 22993, 24493, 14993, 7993, 15493, 16493, 8493, 21493, 10493, 21993, 4493, 6993, 12993, 2993, 8993, 20493, 24993, 993, 6493, 18993, 12493, 3493, 11993, 18493, 10993, 9993, 4993, 19993, 23493, 1993, 493, 23993, 22493], [14494, 5994, 5494, 20994, 1494, 13994, 19494, 15994, 11494, 2494, 16994, 17994, 13494, 17494, 9494, 3994, 7494, 22994, 24494, 14994, 7994, 15494, 16494, 8494, 21494, 10494, 21994, 4494, 6994, 12994, 2994, 8994, 20494, 24994, 994, 6494, 18994, 12494, 3494, 11994, 18494, 10994, 9994, 4994, 19994, 23494, 1994, 494, 23994, 22494], [14495, 5995, 5495, 20995, 1495, 13995, 19495, 15995, 11495, 2495, 16995, 17995, 13495, 17495, 9495, 3995, 7495, 22995, 24495, 14995, 7995, 15495, 16495, 8495, 21495, 10495, 21995, 4495, 6995, 12995, 2995, 8995, 20495, 24995, 995, 6495, 18995, 12495, 3495, 11995, 18495, 10995, 9995, 4995, 19995, 23495, 1995, 495, 23995, 22495], [14496, 5996, 5496, 20996, 1496, 13996, 19496, 15996, 11496, 2496, 16996, 17996, 13496, 17496, 9496, 3996, 7496, 22996, 24496, 14996, 7996, 15496, 16496, 8496, 21496, 10496, 21996, 4496, 6996, 12996, 2996, 8996, 20496, 24996, 996, 6496, 18996, 12496, 3496, 11996, 18496, 10996, 9996, 4996, 19996, 23496, 1996, 496, 23996, 22496], [14497, 5997, 5497, 20997, 1497, 13997, 19497, 15997, 11497, 2497, 16997, 17997, 13497, 17497, 9497, 3997, 7497, 22997, 24497, 14997, 7997, 15497, 16497, 8497, 21497, 10497, 21997, 4497, 6997, 12997, 2997, 8997, 20497, 24997, 997, 6497, 18997, 12497, 3497, 11997, 18497, 10997, 9997, 4997, 19997, 23497, 1997, 497, 23997, 22497], [14498, 5998, 5498, 20998, 1498, 13998, 19498, 15998, 11498, 2498, 16998, 17998, 13498, 17498, 9498, 3998, 7498, 22998, 24498, 14998, 7998, 15498, 16498, 8498, 21498, 10498, 21998, 4498, 6998, 12998, 2998, 8998, 20498, 24998, 998, 6498, 18998, 12498, 3498, 11998, 18498, 10998, 9998, 4998, 19998, 23498, 1998, 498, 23998, 22498], [14499, 5999, 5499, 20999, 1499, 13999, 19499, 15999, 11499, 2499, 16999, 17999, 13499, 17499, 9499, 3999, 7499, 22999, 24499, 14999, 7999, 15499, 16499, 8499, 21499, 10499, 21999, 4499, 6999, 12999, 2999, 8999, 20499, 24999, 999, 6499, 18999, 12499, 3499, 11999, 18499, 10999, 9999, 4999, 19999, 23499, 1999, 499, 23999, 22499]]\ntest_leaves:  [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\nvalidation leaves:  [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "trainer_full = TreeTrainer(\n",
    "    model = treevae_full,\n",
    "    gene_dataset = cas_dataset,\n",
    "    lambda_ = lambda_,\n",
    "    train_size=1.0,\n",
    "    test_size=0,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=freq,\n",
    "    n_epochs_kl_warmup=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elihood: 0.0\n",
      "ELBO Loss: 551.904085081295\n",
      "training:  71%|███████   | 708/1000 [00:14<00:05, 49.74it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.8278068298582\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9224001873963\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.670422462832\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3407046941792\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.529149061548\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7435128710008\n",
      "training:  71%|███████▏  | 714/1000 [00:14<00:05, 50.47it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7305169586552\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0041177024121\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2963111416195\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.1573420084993\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6719670772698\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9197260178236\n",
      "training:  72%|███████▏  | 720/1000 [00:14<00:05, 51.97it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.7957194130665\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1844486384688\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.0733291907007\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.7674330471987\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9546825565358\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.1797981223652\n",
      "training:  73%|███████▎  | 726/1000 [00:14<00:05, 51.69it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.4419353840558\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.1617102231434\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5062787404504\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1388815288158\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9323072665841\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3111025656802\n",
      "training:  73%|███████▎  | 732/1000 [00:14<00:05, 47.24it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4207322334537\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.7565083436755\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.8518303278936\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.5318906084558\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 548.2545179623803\n",
      "training:  74%|███████▎  | 737/1000 [00:14<00:06, 43.81it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.7947686251047\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9356747339926\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.3752747347011\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.3205551808484\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9875412836491\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3634022244142\n",
      "training:  74%|███████▍  | 743/1000 [00:14<00:05, 46.15it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.078687847674\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.587010047507\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5821308229282\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.1266136206178\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2210522320396\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.5809035776676\n",
      "training:  75%|███████▍  | 749/1000 [00:14<00:05, 49.18it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.660586550428\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1088905913781\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9897024277757\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9270395829969\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.528366523108\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1668413760107\n",
      "training:  76%|███████▌  | 755/1000 [00:15<00:04, 50.66it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2037336293419\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.7866429435311\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6300596623184\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.297454649444\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4442805058995\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.0623624238798\n",
      "training:  76%|███████▌  | 761/1000 [00:15<00:04, 51.08it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.3873324029229\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.7567071556412\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7819526597278\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.2904446045642\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.8576897549193\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4112087146834\n",
      "training:  77%|███████▋  | 767/1000 [00:15<00:04, 52.40it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4742913970657\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1184075300516\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1801561917015\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7710503367214\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.382849161961\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7568584884914\n",
      "training:  77%|███████▋  | 773/1000 [00:15<00:04, 52.94it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2681737179386\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2885430714852\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.4769499584479\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.3948980571366\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.3525092737082\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4377253330889\n",
      "training:  78%|███████▊  | 779/1000 [00:15<00:04, 52.58it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6578456772062\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7912417273285\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2325464738198\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.1836962938022\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0007233460677\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4701733784411\n",
      "training:  78%|███████▊  | 785/1000 [00:15<00:04, 53.56it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.3019191568955\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0247074433611\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.2878641424236\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0299675899513\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.3127483153731\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.3326707365601\n",
      "training:  79%|███████▉  | 791/1000 [00:15<00:04, 52.11it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5232932968369\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6845826366214\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.624897297491\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.5608559810521\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3425539933953\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2748958584428\n",
      "training:  80%|███████▉  | 797/1000 [00:15<00:03, 52.45it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.398732642342\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5974372867904\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0880390922355\n",
      "computing elbo\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.6098026486123\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6416245928984\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.693735929241\n",
      "training:  80%|████████  | 803/1000 [00:15<00:03, 51.16it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.5346613704332\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9936337930748\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.4359743301002\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7254040815094\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.1200793882794\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6210031049443\n",
      "training:  81%|████████  | 809/1000 [00:16<00:03, 47.97it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.8945297136739\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6967963379037\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2518631872587\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.5638011460243\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.932070306643\n",
      "training:  81%|████████▏ | 814/1000 [00:16<00:03, 48.26it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6224199825567\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2536328344748\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5295730947444\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9017555871103\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.6078237259588\n",
      "training:  82%|████████▏ | 819/1000 [00:16<00:03, 47.53it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9073203387376\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7476018458093\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3456477387061\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1413262891042\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.5440389285704\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5511336249208\n",
      "training:  82%|████████▎ | 825/1000 [00:16<00:03, 49.13it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3902554356739\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.1678746101418\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4073128104059\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2590935684966\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.2339790096255\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1288851034617\n",
      "training:  83%|████████▎ | 831/1000 [00:16<00:03, 50.53it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6347378519573\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1794926203421\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.4352268115293\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0172064679546\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.0107036731378\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.958673898408\n",
      "training:  84%|████████▎ | 837/1000 [00:16<00:03, 50.79it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6462958677798\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.1866109520943\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6743065417035\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2087013438668\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.8607319506839\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.8140778055774\n",
      "training:  84%|████████▍ | 843/1000 [00:16<00:03, 47.75it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.8099583490737\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.320155656915\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.6739096279043\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2403564190747\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0218388310673\n",
      "training:  85%|████████▍ | 848/1000 [00:16<00:03, 45.57it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2052832655841\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0551378844945\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.0300078180223\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6287831592487\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3674273775744\n",
      "training:  85%|████████▌ | 853/1000 [00:17<00:03, 45.84it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9199964268249\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2325190641176\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6193090638585\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 554.5381451324047\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 553.3183890707261\n",
      "training:  86%|████████▌ | 858/1000 [00:17<00:03, 45.20it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3230399522439\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.3275353550883\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0173188286828\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1029467701941\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1095348304059\n",
      "training:  86%|████████▋ | 863/1000 [00:17<00:03, 43.96it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4247793729326\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5022590942815\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1676712603114\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1849284830247\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5539876852413\n",
      "training:  87%|████████▋ | 868/1000 [00:17<00:02, 44.35it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3289916980448\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.5852891538512\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6565619501055\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6438484900065\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.7714600462743\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6599473861296\n",
      "training:  87%|████████▋ | 874/1000 [00:17<00:02, 46.09it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0258306005186\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.9345417008898\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2335569548782\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3692500929785\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0743233377615\n",
      "training:  88%|████████▊ | 879/1000 [00:17<00:02, 46.45it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5121023585077\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3178480126264\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.3024595464208\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1151351062281\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9719080597964\n",
      "training:  88%|████████▊ | 884/1000 [00:17<00:02, 45.82it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3592842337877\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9659540789482\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0054392922208\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.4960043704506\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.960141529316\n",
      "training:  89%|████████▉ | 889/1000 [00:17<00:02, 44.81it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5332313873896\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.828455951475\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.952795168603\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.1502355260288\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9325523829211\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3165638637007\n",
      "training:  90%|████████▉ | 895/1000 [00:17<00:02, 47.20it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0914075746491\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5257034358631\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9969355501318\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.8023371000861\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0778276313107\n",
      "computing elbo\n",
      "training:  90%|█████████ | 900/1000 [00:18<00:02, 44.62it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.1232637988445\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4029375430864\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.8818265147939\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.5165779069159\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.6349707726529\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5818731324998\n",
      "training:  91%|█████████ | 906/1000 [00:18<00:01, 47.24it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0874820120276\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2584970171063\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.7385760110716\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6717567195722\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.300785662659\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.5534660900389\n",
      "training:  91%|█████████ | 912/1000 [00:18<00:01, 49.04it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5588383885561\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9001851973006\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9173403758971\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.778587882653\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.34563276229\n",
      "training:  92%|█████████▏| 917/1000 [00:18<00:01, 47.87it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9167230095576\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.2334994852307\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2975633443176\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0259714194125\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9751803000082\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.210677792228\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3858850297848\n",
      "training:  92%|█████████▏| 924/1000 [00:18<00:01, 52.25it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.9760883370701\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0982563743719\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.8803844893738\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9328995575365\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2414713205625\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3336916859218\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.7055510610564\n",
      "training:  93%|█████████▎| 931/1000 [00:18<00:01, 55.25it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7901851766901\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3418415800342\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1855468839035\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0426057673398\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6110653855918\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7203683813719\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.709647987366\n",
      "training:  94%|█████████▍| 938/1000 [00:18<00:01, 57.32it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4189830722668\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2177850195794\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2749287606531\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.8327562648758\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2422719956334\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.0642905016736\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7486160319772\n",
      "training:  94%|█████████▍| 945/1000 [00:18<00:00, 58.72it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7830347733548\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1910604618015\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.3576715303408\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4790764991126\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7473325348974\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 548.8845041616887\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.8236008619899\n",
      "training:  95%|█████████▌| 952/1000 [00:18<00:00, 58.84it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.804916794231\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9711392992889\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3401473729429\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3650272651328\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3349411426377\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2045167102325\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5548834916871\n",
      "training:  96%|█████████▌| 959/1000 [00:19<00:00, 59.38it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.9402691131746\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7933415908708\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3661385172063\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.9967566755859\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6170827153883\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.1366220163164\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2190129571136\n",
      "training:  97%|█████████▋| 966/1000 [00:19<00:00, 60.08it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4416618104802\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0050087488349\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2584286031719\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.8877930375368\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.8340147391852\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3772633069215\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0750876795662\n",
      "training:  97%|█████████▋| 973/1000 [00:19<00:00, 60.39it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.7373893288452\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.5830431253447\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.4960851921081\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.9423601936523\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6375597820833\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6261734401801\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.2324515179688\n",
      "training:  98%|█████████▊| 980/1000 [00:19<00:00, 60.00it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.7432529237387\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.0379316805174\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.1726577643367\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1166641861637\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.0864958756371\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.6632016752584\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6964009667915\n",
      "training:  99%|█████████▊| 987/1000 [00:19<00:00, 60.62it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.2327354535422\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.4027975656148\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.0177318355406\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.6340562163422\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.8263501094162\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.5921994662303\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.3582177898379\n",
      "training:  99%|█████████▉| 994/1000 [00:19<00:00, 60.26it/s]Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 551.6445311886991\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.2519738202789\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 552.1994727892458\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 548.6294429749818\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 550.6749202239359\n",
      "Encodings MP Likelihood: 0.0\n",
      "ELBO Loss: 549.9569541488522\n",
      "computing elbo\n",
      "training: 100%|██████████| 1000/1000 [00:19<00:00, 50.52it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer_full.train(n_epochs=n_epochs,\n",
    "              lr=lr\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "computing elbo\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(547.3132, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "full_batch_posterior = trainer.create_posterior(trainer_full.model, cas_dataset, trainer_full.clades,\n",
    "                                indices=np.arange(len(cas_dataset))\n",
    "                                         )\n",
    "\n",
    "full_batch_posterior.compute_elbo(treevae_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2213.0098, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "elbo_full = 0\n",
    "with torch.no_grad():\n",
    "    input = torch.from_numpy(leaves_X).float().to(device='cuda:0')\n",
    "    reconst_loss, qz, _ = treevae_full.forward(input)\n",
    "    elbo_full += torch.sum(reconst_loss)\n",
    "    elbo_full += lambda_ * torch.sum(qz)\n",
    "\n",
    "n_samples = leaves_X.shape[0]\n",
    "elbo_full /= n_samples\n",
    "\n",
    "elbo_full"
   ]
  },
  {
   "source": [
    "# 4. Fitting scVI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: (Un)weighted Average of decoded latent vectors, with scVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same averaging of the subtrees leaves in **Baseline 1**, only this time, the gene expression data is recovered with scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GeneExpressionDataset object with n_cells x nb_genes = 25000 x 1000\n",
       "    gene_attribute_names: 'gene_names'\n",
       "    cell_attribute_names: 'local_vars', 'barcodes', 'local_means', 'batch_indices', 'labels'\n",
       "    cell_categorical_attribute_names: 'batch_indices', 'labels'"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "# anndata\n",
    "scvi_dataset = GeneExpressionDataset()\n",
    "\n",
    "scvi_dataset.populate_from_data(X=np.vstack(n_leaves_X),\n",
    "                               gene_names=[str(i) for i in range(glm.X.shape[1])])\n",
    "scvi_dataset.initialize_cell_attribute('barcodes', cell_names)\n",
    "\n",
    "scvi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_epochs =600\n",
    "use_batches = False\n",
    "\n",
    "vae = VAE(gene_dataset.nb_genes,\n",
    "                  n_batch=cas_dataset.n_batches * use_batches,\n",
    "                  n_hidden=128,\n",
    "                  n_layers=1,\n",
    "                  reconstruction_loss='poisson',\n",
    "                  n_latent=glm.latent,\n",
    "                  ldvae=ldvae\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training: 100%|██████████| 600/600 [28:16<00:00,  2.83s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train history scVI')"
      ]
     },
     "metadata": {},
     "execution_count": 206
    }
   ],
   "source": [
    "trainer_scvi = UnsupervisedTrainer(model=vae,\n",
    "                              gene_dataset=gene_dataset,\n",
    "                              train_size=1.0,\n",
    "                              use_cuda=use_cuda,\n",
    "                              frequency=10,\n",
    "                              n_epochs_kl_warmup=200\n",
    "                              )\n",
    "\n",
    "# train scVI\n",
    "trainer_scvi.train(n_epochs=n_epochs, lr=1e-3) \n",
    "                                        \n",
    "elbo_train_scvi = trainer_scvi.history[\"elbo_train_set\"]\n",
    "x = np.linspace(0, 100, (len(elbo_train_scvi)))\n",
    "plt.plot(np.log(elbo_train_scvi), \n",
    "         label=\"train\", color='blue',\n",
    "         linestyle=':',\n",
    "         linewidth=3\n",
    "        )\n",
    "        \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.title(\"Train history scVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi_posterior = trainer_scvi.create_posterior(model=vae,\n",
    "                                               gene_dataset=scvi_dataset \n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "545.93428234375"
      ]
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "scvi_posterior.elbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensors in scvi_posterior:\n",
    "    x, local_l_mean, local_l_var, _, _ = tensors\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "61485.956"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "l1 = local_l_mean[0][0].item() * torch.ones((input.shape[0])).to('cuda:0')\n",
    "l2 = local_l_var[0][0].item() * torch.ones((input.shape[0])).to('cuda:0')\n",
    "\n",
    "input = torch.from_numpy(leaves_X).float().to(device='cuda:0')\n",
    "elbo = 0\n",
    "with torch.no_grad():\n",
    "    reconst_loss, kl_divergence, kl_divergence_global = vae.forward(input, l1, l2)\n",
    "    elbo += torch.sum(reconst_loss + kl_divergence).item()\n",
    "n_samples = leaves_X.shape[0]\n",
    "elbo /= n_samples\n",
    "\n",
    "elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "source": [
    "kl_divergence_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd08038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864",
   "display_name": "Python 3.7  ('scvi-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8038a79804d646dd36b3762b0d60c87c86d89e40c61f6758cc1d2f18aca59864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}